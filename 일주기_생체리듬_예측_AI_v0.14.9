{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtQYoCjuAEM6mKqhy8ksTg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqL5FJoAMfBd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "생체리듬 예측 AI - 데이터 생성 안정화 (v0.14.9 - 최종)\n",
        "\n",
        "목적: 수치해석 불안정성 문제 해결\n",
        "방법: '위상 모델링' 접근법을 채택하고, 외부 자극의 물리적 단위를 위상 변화 속도로\n",
        "변환하는 '스케일링 계수(κ)'를 도입하여 정량적 타당성 확보\n",
        "\n",
        "[v0.14.9 주요 개선 사항]:\n",
        "1.  **스케일링 계수(κ) 도입**:\n",
        "    - 외부 자극의 힘(단위: state/min)을 위상 변화 속도(단위: rad/min)로 변환하는\n",
        "      핵심 파라미터 `kappa`를 도입\n",
        "    - `d(phase)/dt = ω_내재 + κ * Z(phase) * F` 공식을 사용하여 시뮬레이션\n",
        "\n",
        "2.  **자동 보정(Calibration) 기능 추가**:\n",
        "    - `calibrate_kappa` 헬퍼 함수를 추가하여, `kappa` 값을 임의로 정하는 대신\n",
        "      실제 임상 데이터에 기반하여 동적으로 계산\n",
        "    - \"수면 자극만 있을 때 하루 최대 0.27시간의 위상 변화가 일어난다\"는\n",
        "      실험 결과를 정확히 재현하는 `kappa` 값 계산\n",
        "\n",
        "3.  **최종 안정성 및 생물학적 타당성 확보**:\n",
        "    - 이 접근법은 불안정성의 근원인 x, xc의 직접 계산을 배제하고, 안정성이 보장된\n",
        "      '위상' 변수만을 다룸\n",
        "    - 동시에, 외부 자극의 효과를 실제 실험 데이터로 검증된 스케일로 변환하여\n",
        "      반영하므로, 안정성과 생물학적 현실성을 모두 만족시킴\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 0. 라이브러리 임포트 및 파이프라인 설정\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Concatenate, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --- 파이프라인 제어 설정 ---\n",
        "OUTPUT_DIR = \"final_biometric_pipeline_output_v0.14.9\"\n",
        "\n",
        "# --- 데이터 및 모델 설정 (PC 환경 최적화) ---\n",
        "DATA_DURATION_DAYS = 7\n",
        "INPUT_SEQUENCE_LENGTH = 4 * 24 * 60\n",
        "PREDICTION_HORIZON = 60\n",
        "PHASE_CORRECTION_LOOKBACK = 3 * 24 * 60\n",
        "DAY_MINUTES = 24 * 60\n",
        "ANALYSIS_DAYS = 3\n",
        "NUM_MARKERS_TO_KEEP = 3\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VALIDATION_RATIO = 0.1\n",
        "\n",
        "# --- 모델 하이퍼파라미터 (성능 튜닝) ---\n",
        "D_MODEL = 192\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 8\n",
        "DFF = 384\n",
        "DROPOUT_RATE = 0.05\n",
        "NUM_FOURIER_HARMONICS = 5\n",
        "\n",
        "# --- 학습 하이퍼파라미터 (성능 튜닝) ---\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0005\n",
        "LAMBDA_REG = 0.1\n",
        "LAMBDA_CONT = 1.0\n",
        "LAMBDA_ANCHOR = 0.5\n",
        "\n",
        "# --- 하이퍼파라미터 검증 ---\n",
        "assert D_MODEL % NUM_HEADS == 0, f\"D_MODEL({D_MODEL})은 NUM_HEADS({NUM_HEADS})로 나누어떨어져야 합니다.\"\n",
        "\n",
        "# --- 물리 모델 파라미터 ---\n",
        "PARAMS = {\n",
        "    'alpha0': 0.1, 'beta': 0.007, 'G': 37, 'p': 0.5, 'I0': 9500,\n",
        "    'mu': 0.13, 'q': 1/3, 'k': 0.55, 'rho': 0.032, 'tau_x': 24.2,\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 생체리듬 물리 모델 (SCN 코어 엔진) 및 헬퍼 함수\n",
        "# =============================================================================\n",
        "def get_external_forces(t, y, params, day_timestamps, lux_values, sleep_values):\n",
        "    \"\"\"\n",
        "    주어진 상태(y)와 환경(빛, 수면)에서 외부 자극 B와 Ns만 계산하여 반환합니다.\n",
        "    \"\"\"\n",
        "    x, xc, n = y\n",
        "\n",
        "    current_idx = int(np.clip(np.round(t), 0, len(lux_values) - 1))\n",
        "    I = lux_values[current_idx]\n",
        "    sigma = sleep_values[current_idx]\n",
        "\n",
        "    I = max(I, 0)\n",
        "    alpha = params['alpha0'] * ((I / params['I0']) ** params['p']) * (I / (I + 100))\n",
        "    B_hat = params['G'] * (1 - n) * alpha\n",
        "    B = B_hat * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "\n",
        "    Ns_hat = params['rho'] * (1/3 - sigma)\n",
        "    Ns = Ns_hat * (1 - np.tanh(10 * x))\n",
        "\n",
        "    dn_dt = 60 * (alpha * (1 - n) - params['beta'] * n)\n",
        "\n",
        "    return B, Ns, dn_dt\n",
        "\n",
        "\n",
        "def find_hr_nadir(heart_rate_data, is_sleeping_data, day_minutes=1440):\n",
        "    num_days = len(heart_rate_data) // day_minutes\n",
        "    daily_nadirs = []\n",
        "    for day in range(num_days):\n",
        "        day_start = day * day_minutes\n",
        "        day_end = (day + 1) * day_minutes\n",
        "        day_hr = heart_rate_data[day_start:day_end]\n",
        "        day_sleep = is_sleeping_data[day_start:day_end]\n",
        "        sleep_hr = day_hr[day_sleep == 1]\n",
        "        if len(sleep_hr) > 0:\n",
        "            nadir_in_sleep = np.argmin(sleep_hr)\n",
        "            original_indices = np.where(day_sleep == 1)[0]\n",
        "            nadir_in_day = original_indices[nadir_in_sleep]\n",
        "            daily_nadirs.append(nadir_in_day)\n",
        "        else:\n",
        "            daily_nadirs.append(np.argmin(day_hr))\n",
        "    return np.mean(daily_nadirs) if daily_nadirs else day_minutes / 2\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 딥러닝 모델 정의\n",
        "# =============================================================================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class FourierTrajectoryLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_harmonics, **kwargs):\n",
        "        super(FourierTrajectoryLayer, self).__init__(**kwargs)\n",
        "        self.num_harmonics = num_harmonics\n",
        "        self.output_dim = DAY_MINUTES\n",
        "        self.t = tf.linspace(0.0, 2 * np.pi, self.output_dim)\n",
        "        self.t = tf.cast(self.t, dtype=tf.float32)\n",
        "\n",
        "    def call(self, coeffs):\n",
        "        coeffs_x = coeffs[:, :(1 + 2 * self.num_harmonics)]\n",
        "        coeffs_xc = coeffs[:, (1 + 2 * self.num_harmonics):]\n",
        "        traj_x = self._build_trajectory(coeffs_x)\n",
        "        traj_xc = self._build_trajectory(coeffs_xc)\n",
        "        return tf.stack([traj_x, traj_xc], axis=-1)\n",
        "\n",
        "    def _build_trajectory(self, coeffs):\n",
        "        a0 = coeffs[:, 0:1]\n",
        "        a_n = coeffs[:, 1:self.num_harmonics + 1]\n",
        "        b_n = coeffs[:, self.num_harmonics + 1:]\n",
        "        trajectory = a0\n",
        "        for n in range(1, self.num_harmonics + 1):\n",
        "            trajectory += a_n[:, n-1:n] * tf.cos(n * self.t)\n",
        "            trajectory += b_n[:, n-1:n] * tf.sin(n * self.t)\n",
        "        return trajectory\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(FourierTrajectoryLayer, self).get_config()\n",
        "        config.update({\"num_harmonics\": self.num_harmonics})\n",
        "        return config\n",
        "\n",
        "def build_phase_correction_model(lookback_period, num_harmonics):\n",
        "    num_coeffs_per_traj = 1 + 2 * num_harmonics\n",
        "    output_size = num_coeffs_per_traj * 2\n",
        "    input_body1 = Input(shape=(lookback_period, 3), name='corr_input_body1')\n",
        "    input_body2 = Input(shape=(lookback_period, 1), name='corr_input_body2')\n",
        "    input_zeit1 = Input(shape=(lookback_period, 1), name='corr_input_zeit1')\n",
        "    input_zeit2 = Input(shape=(lookback_period, 1), name='corr_input_zeit2')\n",
        "    input_zeit3 = Input(shape=(lookback_period, 1), name='corr_input_zeit3')\n",
        "    input_sleep = Input(shape=(lookback_period, 1), name='corr_input_sleep')\n",
        "\n",
        "    def create_feat_extractor(inp, name):\n",
        "        x = Conv1D(16, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn1')(inp)\n",
        "        x = Conv1D(8, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn2')(x)\n",
        "        return GlobalAveragePooling1D(name=f'corr_{name}_pool')(x)\n",
        "\n",
        "    features = [\n",
        "        create_feat_extractor(input_body1, 'body1'), create_feat_extractor(input_body2, 'body2'),\n",
        "        create_feat_extractor(input_zeit1, 'zeit1'), create_feat_extractor(input_zeit2, 'zeit2'),\n",
        "        create_feat_extractor(input_zeit3, 'zeit3'), create_feat_extractor(input_sleep, 'sleep'),\n",
        "    ]\n",
        "    combined = Concatenate()(features)\n",
        "    x = Dense(64, activation='relu')(combined)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    fourier_coeffs = Dense(output_size, activation='linear', name='fourier_coeffs')(x)\n",
        "    model_inputs = [input_body1, input_body2, input_zeit1, input_zeit2, input_zeit3, input_sleep]\n",
        "    model = Model(inputs=model_inputs, outputs=fourier_coeffs, name='PhaseCorrectionModel')\n",
        "    return model\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\"position\": self.position, \"d_model\": self.d_model})\n",
        "        return config\n",
        "\n",
        "def build_feature_extractor(time_steps=INPUT_SEQUENCE_LENGTH, d_model=D_MODEL):\n",
        "    input_lco = Input(shape=(time_steps, 2), name='input_lco')\n",
        "    input_lux = Input(shape=(time_steps, 1), name='input_lux')\n",
        "    input_sleep = Input(shape=(time_steps, 1), name='input_sleep')\n",
        "    input_body1 = Input(shape=(time_steps, 3), name='input_body1')\n",
        "    input_body2 = Input(shape=(time_steps, 1), name='input_body2')\n",
        "    input_zeit1 = Input(shape=(time_steps, 1), name='input_zeit1')\n",
        "    input_zeit2 = Input(shape=(time_steps, 1), name='input_zeit2')\n",
        "    input_zeit3 = Input(shape=(time_steps, 1), name='input_zeit3')\n",
        "    def create_cnn_block(n_features, name_prefix):\n",
        "        return tf.keras.Sequential([\n",
        "            Conv1D(filters=32, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn1\"),\n",
        "            Conv1D(filters=n_features, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn2\")\n",
        "        ])\n",
        "\n",
        "    feature_proportions = {\n",
        "        'lco': 0.25, 'lux': 0.125, 'sleep': 0.125, 'body1': 0.25,\n",
        "        'body2': 0.0625, 'zeit1': 0.0625, 'zeit2': 0.0625, 'zeit3': 0.0625,\n",
        "    }\n",
        "    cnn_block_map = {\n",
        "        name: max(2, int(d_model * prop) // 2 * 2)\n",
        "        for name, prop in feature_proportions.items()\n",
        "    }\n",
        "    current_sum = sum(cnn_block_map.values())\n",
        "    if current_sum != d_model:\n",
        "        diff = d_model - current_sum\n",
        "        cnn_block_map['lco'] += diff\n",
        "    assert sum(cnn_block_map.values()) == d_model, f\"Sum of feature dimensions {sum(cnn_block_map.values())} must equal d_model {d_model}\"\n",
        "\n",
        "    features_lco = create_cnn_block(cnn_block_map['lco'], 'lco')(input_lco)\n",
        "    features_lux = create_cnn_block(cnn_block_map['lux'], 'lux')(input_lux)\n",
        "    features_sleep = create_cnn_block(cnn_block_map['sleep'], 'sleep')(input_sleep)\n",
        "    features_body1 = create_cnn_block(cnn_block_map['body1'], 'body1')(input_body1)\n",
        "    features_body2 = create_cnn_block(cnn_block_map['body2'], 'body2')(input_body2)\n",
        "    features_zeit1 = create_cnn_block(cnn_block_map['zeit1'], 'zeit1')(input_zeit1)\n",
        "    features_zeit2 = create_cnn_block(cnn_block_map['zeit2'], 'zeit2')(input_zeit2)\n",
        "    features_zeit3 = create_cnn_block(cnn_block_map['zeit3'], 'zeit3')(input_zeit3)\n",
        "\n",
        "    combined_features = Concatenate(axis=-1, name='combined_features')([\n",
        "        features_lco, features_lux, features_sleep, features_body1,\n",
        "        features_body2, features_zeit1, features_zeit2, features_zeit3\n",
        "    ])\n",
        "    pos_encoding_layer = PositionalEncoding(time_steps, d_model)\n",
        "    encoded_features = pos_encoding_layer(combined_features)\n",
        "    model_inputs = [input_lco, input_lux, input_sleep, input_body1, input_body2, input_zeit1, input_zeit2, input_zeit3]\n",
        "    model = Model(inputs=model_inputs, outputs=encoded_features, name='FeatureExtractor')\n",
        "    return model\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ContextualTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(ContextualTransformerBlock, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.rate = rate\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "    def call(self, inputs, training=False):\n",
        "        past_info, current_info = inputs\n",
        "        attn_output_current = self.mha1(query=current_info, key=current_info, value=current_info, training=training)\n",
        "        current_info_sa = self.layernorm1(current_info + self.dropout1(attn_output_current, training=training))\n",
        "        attn_output_cross = self.mha2(query=current_info_sa, key=past_info, value=past_info, training=training)\n",
        "        current_info_contextualized = self.layernorm2(current_info_sa + self.dropout2(attn_output_cross, training=training))\n",
        "        ffn_output = self.ffn(current_info_contextualized)\n",
        "        final_output = self.layernorm3(current_info_contextualized + self.dropout3(ffn_output, training=training))\n",
        "        return final_output\n",
        "    def get_config(self):\n",
        "        config = super(ContextualTransformerBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(SelfAttentionBlock, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.rate = rate\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "    def call(self, x, training=False):\n",
        "        attn_output = self.mha(query=x, key=x, value=x, training=training)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "    def get_config(self):\n",
        "        config = super(SelfAttentionBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class MainPredictor(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, prediction_horizon, rate=0.1, **kwargs):\n",
        "        super(MainPredictor, self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.prediction_horizon = prediction_horizon\n",
        "        self.rate = rate\n",
        "        self.first_block = ContextualTransformerBlock(d_model, num_heads, dff, rate)\n",
        "        self.other_blocks = [SelfAttentionBlock(d_model, num_heads, dff, rate) for _ in range(num_layers - 1)]\n",
        "        self.prediction_head = tf.keras.Sequential([\n",
        "            Dense(128, activation='relu', name=\"pred_head_dense1\"),\n",
        "            Dropout(rate),\n",
        "            Dense(prediction_horizon, name=\"final_prediction\")\n",
        "        ], name=\"PREDICTION_HEAD\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoded_features, past_info, last_lco = inputs\n",
        "\n",
        "        current_info_length = 2 * DAY_MINUTES\n",
        "        current_info = encoded_features[:, -current_info_length:, :]\n",
        "        x = self.first_block((past_info, current_info), training=training)\n",
        "        for block in self.other_blocks:\n",
        "            x = block(x, training=training)\n",
        "        pooled_vector = tf.reduce_mean(x, axis=1)\n",
        "        last_vector = x[:, -1, :]\n",
        "        combined_final_vector = tf.concat([pooled_vector, last_vector, last_lco], axis=-1)\n",
        "        predictions = self.prediction_head(combined_final_vector)\n",
        "        return predictions\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"num_layers\": self.num_layers, \"d_model\": self.d_model,\n",
        "                \"num_heads\": self.num_heads, \"dff\": self.dff,\n",
        "                \"prediction_horizon\": self.prediction_horizon, \"rate\": self.rate}\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class IntegratedModel(Model):\n",
        "    def __init__(self, phase_correction_model, main_predictor, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.phase_correction_model = phase_correction_model\n",
        "        self.main_predictor = main_predictor\n",
        "        self.config = config\n",
        "        self.fourier_layer = FourierTrajectoryLayer(config['num_harmonics'])\n",
        "        self.lambda_reg = self.config['lambda_reg']\n",
        "        self.lambda_cont = self.config['lambda_cont']\n",
        "        self.lambda_anchor = self.config['lambda_anchor']\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        full_inputs = inputs['full_inputs']\n",
        "        baseline_inputs = inputs['baseline_inputs']\n",
        "        past_info = inputs['past_info']\n",
        "        encoded_features = inputs['encoded_features']\n",
        "\n",
        "        batch_size = tf.shape(full_inputs)[0]\n",
        "        num_days_in_sequence = self.config['input_seq_len'] // DAY_MINUTES\n",
        "\n",
        "        required_len_for_patches = (num_days_in_sequence - 1) * DAY_MINUTES + self.config['correction_lookback']\n",
        "        patches_input = full_inputs[:, :required_len_for_patches, :]\n",
        "        patches_input_4d = patches_input[:, :, tf.newaxis, :]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=patches_input_4d,\n",
        "            sizes=[1, self.config['correction_lookback'], 1, 1],\n",
        "            strides=[1, DAY_MINUTES, 1, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding='VALID'\n",
        "        )\n",
        "        num_features = tf.shape(full_inputs)[-1]\n",
        "        all_lookback_data = tf.reshape(\n",
        "            patches,\n",
        "            [batch_size, num_days_in_sequence, self.config['correction_lookback'], num_features]\n",
        "        )\n",
        "        reshaped_lookback_data = tf.reshape(\n",
        "            all_lookback_data,\n",
        "            [batch_size * num_days_in_sequence, self.config['correction_lookback'], num_features]\n",
        "        )\n",
        "\n",
        "        predictor_inputs = [\n",
        "            reshaped_lookback_data[:, :, 2:5],\n",
        "            reshaped_lookback_data[:, :, 5:6],\n",
        "            reshaped_lookback_data[:, :, 7:8],\n",
        "            reshaped_lookback_data[:, :, 8:9],\n",
        "            reshaped_lookback_data[:, :, 6:7],\n",
        "            reshaped_lookback_data[:, :, 1:2],\n",
        "        ]\n",
        "        fourier_coeffs = self.phase_correction_model(predictor_inputs, training=training)\n",
        "\n",
        "        delta_trajectory_flat = self.fourier_layer(fourier_coeffs)\n",
        "        all_delta_trajs = tf.reshape(\n",
        "            delta_trajectory_flat,\n",
        "            [batch_size, num_days_in_sequence, DAY_MINUTES, 2]\n",
        "        )\n",
        "\n",
        "        baseline_reshaped = tf.reshape(\n",
        "            baseline_inputs[:, self.config['correction_lookback'] : self.config['correction_lookback'] + self.config['input_seq_len'], :],\n",
        "            [batch_size, num_days_in_sequence, DAY_MINUTES, 2]\n",
        "        )\n",
        "        all_corrected_trajs = baseline_reshaped + all_delta_trajs\n",
        "\n",
        "        final_lco_input = tf.reshape(all_corrected_trajs, [batch_size, -1, 2])\n",
        "        last_lco_corrected = final_lco_input[:, -1, :]\n",
        "\n",
        "        y_pred = self.main_predictor((encoded_features, past_info, last_lco_corrected), training=training)\n",
        "\n",
        "        if training:\n",
        "            reg_loss = tf.reduce_mean(tf.square(all_delta_trajs)) * self.lambda_reg\n",
        "            self.add_loss(reg_loss)\n",
        "            continuity_gaps = all_corrected_trajs[:, :-1, -1, :] - all_corrected_trajs[:, 1:, 0, :]\n",
        "            cont_loss = tf.reduce_mean(tf.square(continuity_gaps)) * self.lambda_cont\n",
        "            self.add_loss(cont_loss)\n",
        "            last_day_corrected_x = all_corrected_trajs[:, -1, :, 0]\n",
        "            predicted_cbt_nadir_idx = tf.cast(tf.argmin(last_day_corrected_x, axis=1), tf.float32)\n",
        "\n",
        "            last_day_data = full_inputs[:, -DAY_MINUTES:, :]\n",
        "            hr_series = last_day_data[:, :, 2]\n",
        "            sleep_series = last_day_data[:, :, 1]\n",
        "            sleep_mask = tf.cast(sleep_series > 0.5, dtype=tf.float32)\n",
        "            has_sleep = tf.reduce_any(tf.cast(sleep_mask, tf.bool), axis=1)\n",
        "            hr_in_sleep = hr_series * sleep_mask + (1.0 - sleep_mask) * 1e9\n",
        "            nadir_in_sleep_indices = tf.cast(tf.argmin(hr_in_sleep, axis=1), tf.float32)\n",
        "            nadir_overall_indices = tf.cast(tf.argmin(hr_series, axis=1), tf.float32)\n",
        "            actual_hr_nadir_idx = tf.where(has_sleep, nadir_in_sleep_indices, nadir_overall_indices)\n",
        "            anchor_loss_val = tf.reduce_mean(tf.square(predicted_cbt_nadir_idx - actual_hr_nadir_idx)) / (DAY_MINUTES**2)\n",
        "            anchor_loss = anchor_loss_val * self.lambda_anchor\n",
        "            self.add_loss(anchor_loss)\n",
        "        return y_pred\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"config\": self.config}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config_data, custom_objects=None):\n",
        "        return cls(**config_data)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. TFRecord 생성 및 파싱\n",
        "# =============================================================================\n",
        "def moving_average_np(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'same') / w\n",
        "\n",
        "def generate_past_info_np(main_inputs_unscaled, encoded_features, d_model):\n",
        "    day_minutes = DAY_MINUTES\n",
        "    analysis_days = ANALYSIS_DAYS\n",
        "    num_markers_to_keep = NUM_MARKERS_TO_KEEP\n",
        "    analysis_window_size = analysis_days * day_minutes\n",
        "\n",
        "    analysis_inputs = main_inputs_unscaled[:analysis_window_size, :]\n",
        "    analysis_features = encoded_features[:analysis_window_size, :]\n",
        "\n",
        "    skin_temp = analysis_inputs[:, 5]\n",
        "    ambient_temp = analysis_inputs[:, 6]\n",
        "    corrected_skin_temp = skin_temp - ambient_temp\n",
        "    analysis_inputs = np.c_[analysis_inputs, corrected_skin_temp]\n",
        "\n",
        "    daily_inputs = analysis_inputs.reshape((analysis_days, day_minutes, -1))\n",
        "    daily_features = analysis_features.reshape((analysis_days, day_minutes, d_model))\n",
        "\n",
        "    is_sleeping = daily_inputs[..., 1]\n",
        "    heart_rate = daily_inputs[..., 2]\n",
        "    meal_event = daily_inputs[..., 7]\n",
        "    exercise_event = daily_inputs[..., 8]\n",
        "\n",
        "    event_mask = np.logical_or(meal_event, exercise_event)\n",
        "    hr_no_event = np.where(event_mask, np.inf, heart_rate)\n",
        "\n",
        "    sleep_mask = is_sleeping > 0.5\n",
        "    hr_in_sleep = np.where(sleep_mask, hr_no_event, np.inf)\n",
        "\n",
        "    nadir_indices = np.zeros(analysis_days, dtype=int)\n",
        "    for day in range(analysis_days):\n",
        "        if np.any(sleep_mask[day]):\n",
        "            nadir_indices[day] = np.argmin(hr_in_sleep[day])\n",
        "        else:\n",
        "            nadir_indices[day] = np.argmin(hr_no_event[day])\n",
        "\n",
        "    temp_series = daily_inputs[..., -1]\n",
        "\n",
        "    onset_indices = np.zeros(analysis_days, dtype=int)\n",
        "    offset_indices = np.zeros(analysis_days, dtype=int)\n",
        "\n",
        "    for day in range(analysis_days):\n",
        "        temp_smoothed = moving_average_np(temp_series[day], 30)\n",
        "        temp_deriv = np.gradient(temp_smoothed)\n",
        "        temp_deriv_smoothed = moving_average_np(temp_deriv, 10)\n",
        "\n",
        "        post_nadir_mask = np.arange(day_minutes) > nadir_indices[day]\n",
        "        onset_candidate_mask = np.logical_and(post_nadir_mask, temp_deriv_smoothed > 0.001)\n",
        "        onset_candidates = np.where(onset_candidate_mask)[0]\n",
        "        onset_indices[day] = onset_candidates[0] if len(onset_candidates) > 0 else 0\n",
        "\n",
        "        offset_candidate_mask = temp_deriv_smoothed < -0.001\n",
        "        offset_candidates = np.where(offset_candidate_mask)[0]\n",
        "        offset_indices[day] = offset_candidates[-1] if len(offset_candidates) > 0 else 0\n",
        "\n",
        "    def gather_vectors(features, indices):\n",
        "        day_indices = np.arange(analysis_days)\n",
        "        return features[day_indices, indices, :]\n",
        "\n",
        "    onset_vectors = gather_vectors(daily_features, onset_indices)\n",
        "    nadir_vectors = gather_vectors(daily_features, nadir_indices)\n",
        "    offset_vectors = gather_vectors(daily_features, offset_indices)\n",
        "\n",
        "    onset_vectors_last = onset_vectors[-num_markers_to_keep:]\n",
        "    nadir_vectors_last = nadir_vectors[-num_markers_to_keep:]\n",
        "    offset_vectors_last = offset_vectors[-num_markers_to_keep:]\n",
        "\n",
        "    past_info = np.concatenate([onset_vectors_last, nadir_vectors_last, offset_vectors_last], axis=0)\n",
        "    return past_info\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def serialize_example(full_inputs, baseline_inputs, past_info, encoded_features, label):\n",
        "    feature = {\n",
        "        'full_inputs': _bytes_feature(tf.io.serialize_tensor(full_inputs).numpy()),\n",
        "        'baseline_inputs': _bytes_feature(tf.io.serialize_tensor(baseline_inputs).numpy()),\n",
        "        'past_info': _bytes_feature(tf.io.serialize_tensor(past_info).numpy()),\n",
        "        'encoded_features': _bytes_feature(tf.io.serialize_tensor(encoded_features).numpy()),\n",
        "        'label': _bytes_feature(tf.io.serialize_tensor(label).numpy()),\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "def create_tfrecords(df_unscaled, df_scaled, indices, config, feature_extractor, file_path):\n",
        "    print(f\"--- TFRecord 파일 생성 시작: {file_path} ---\")\n",
        "\n",
        "    max_len = config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon']\n",
        "    hr_col_idx = df_unscaled.columns.get_loc('heart_rate')\n",
        "    baseline_cols = ['x_base', 'xc_base']\n",
        "    baseline_indices = [df_unscaled.columns.get_loc(c) for c in baseline_cols]\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        feature_extractor_cpu = build_feature_extractor(config['input_seq_len'], config['d_model'])\n",
        "        feature_extractor_cpu.set_weights(feature_extractor.get_weights())\n",
        "\n",
        "    with tf.io.TFRecordWriter(file_path) as writer:\n",
        "        for start_idx in tqdm(indices, desc=f\"{os.path.basename(file_path)} 생성 중\"):\n",
        "            x_slice_unscaled = df_unscaled.iloc[start_idx : start_idx + max_len].values\n",
        "            x_slice_scaled = df_scaled.iloc[start_idx : start_idx + max_len].values\n",
        "\n",
        "            label = x_slice_unscaled[max_len - config['pred_horizon'] : max_len, hr_col_idx]\n",
        "            full_inputs = x_slice_scaled\n",
        "            baseline_inputs = x_slice_scaled[:, baseline_indices]\n",
        "\n",
        "            main_input_start = config['correction_lookback']\n",
        "            main_input_end = main_input_start + config['input_seq_len']\n",
        "            main_input_unscaled = x_slice_unscaled[main_input_start:main_input_end]\n",
        "            main_input_scaled = x_slice_scaled[main_input_start:main_input_end]\n",
        "\n",
        "            cnn_inputs_np = [\n",
        "                main_input_scaled[:, 9:11], main_input_scaled[:, 0:1],\n",
        "                main_input_scaled[:, 1:2],   main_input_scaled[:, 2:5],\n",
        "                main_input_scaled[:, 5:6],   main_input_scaled[:, 7:8],\n",
        "                main_input_scaled[:, 8:9],   main_input_scaled[:, 6:7],\n",
        "            ]\n",
        "            cnn_inputs_tf = [tf.convert_to_tensor(inp[np.newaxis, ...], dtype=tf.float32) for inp in cnn_inputs_np]\n",
        "\n",
        "            encoded_features = feature_extractor_cpu(cnn_inputs_tf, training=False).numpy().squeeze(0)\n",
        "\n",
        "            past_info = generate_past_info_np(main_input_unscaled, encoded_features, config['d_model'])\n",
        "\n",
        "            example = serialize_example(\n",
        "                tf.constant(full_inputs, dtype=tf.float32),\n",
        "                tf.constant(baseline_inputs, dtype=tf.float32),\n",
        "                tf.constant(past_info, dtype=tf.float32),\n",
        "                tf.constant(encoded_features, dtype=tf.float32),\n",
        "                tf.constant(label, dtype=tf.float32)\n",
        "            )\n",
        "            writer.write(example)\n",
        "    print(f\"--- TFRecord 파일 생성 완료: {file_path} ---\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 데이터 생성 함수\n",
        "# =============================================================================\n",
        "def generate_initial_dataframe(output_dir, duration_days):\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "    total_minutes = duration_days * 24 * 60\n",
        "    timestamps = pd.to_datetime('2025-07-01') + pd.to_timedelta(np.arange(total_minutes), 'm')\n",
        "    df = pd.DataFrame(index=timestamps)\n",
        "    minute_of_day = df.index.hour * 60 + df.index.minute\n",
        "\n",
        "    # 1. 기본 생체 신호 생성\n",
        "    base_hr = pd.Series(\n",
        "        70 - 15 * (((df.index.hour >= 23) | (df.index.hour < 7)).astype(int)) - 10 * np.cos(2 * np.pi * (minute_of_day - 12*60) / (24*60)),\n",
        "        index=df.index\n",
        "    )\n",
        "    base_hrv = pd.Series(\n",
        "        50 + 20 * (((df.index.hour >= 23) | (df.index.hour < 7)).astype(int)) + 15 * np.cos(2 * np.pi * (minute_of_day - 4*60) / (24*60)),\n",
        "        index=df.index\n",
        "    )\n",
        "    base_resp_rate = pd.Series(\n",
        "        16 - 4 * (((df.index.hour >= 23) | (df.index.hour < 7)).astype(int)),\n",
        "        index=df.index\n",
        "    )\n",
        "\n",
        "    df['heart_rate'] = base_hr\n",
        "    df['hrv'] = base_hrv\n",
        "    df['respiration_rate'] = base_resp_rate\n",
        "\n",
        "    # 2. 이벤트 효과 계산 및 적용\n",
        "    df['meal_event'] = 0\n",
        "    df.loc[df.index.hour.isin([8, 13, 19]), 'meal_event'] = 1\n",
        "    df['exercise_event'] = 0\n",
        "    df.loc[df.index.hour.isin([17, 18]), 'exercise_event'] = 1\n",
        "\n",
        "    exercise_indices = df[df['exercise_event'] == 1].index\n",
        "    for idx in exercise_indices:\n",
        "        time_window = pd.date_range(start=idx - pd.Timedelta(minutes=15), end=idx + pd.Timedelta(minutes=45), freq='min')\n",
        "        valid_window = df.index.intersection(time_window)\n",
        "\n",
        "        time_diff_seconds = np.abs((valid_window - idx).total_seconds())\n",
        "        effect = (1 - time_diff_seconds / (45 * 60))\n",
        "\n",
        "        df.loc[valid_window, 'heart_rate'] = base_hr.loc[valid_window] + 30 * effect\n",
        "        df.loc[valid_window, 'hrv'] = base_hrv.loc[valid_window] - 20 * effect\n",
        "        df.loc[valid_window, 'respiration_rate'] = base_resp_rate.loc[valid_window] + 5 * effect\n",
        "\n",
        "    # 3. 나머지 데이터 생성 및 최종 노이즈 추가\n",
        "    df['lux'] = np.select(\n",
        "        [(df.index.hour < 7), (df.index.hour >= 7) & (df.index.hour < 9), (df.index.hour >= 9) & (df.index.hour < 17), (df.index.hour >= 17) & (df.index.hour < 23), (df.index.hour >= 23)],\n",
        "        [0, 150, 400, 150, 0], default=0\n",
        "    )\n",
        "    df['is_sleeping'] = ((df.index.hour >= 23) | (df.index.hour < 7)).astype(int)\n",
        "    df['skin_temp'] = 34 + 1.5 * np.sin(2 * np.pi * (minute_of_day - 18*60) / (24*60))\n",
        "    df['ambient_temp'] = 22 + 3 * np.sin(2 * np.pi * (minute_of_day - 15*60) / (24*60))\n",
        "\n",
        "    df['heart_rate'] += np.random.normal(0, 2, total_minutes)\n",
        "    df['hrv'] += np.random.normal(0, 5, total_minutes)\n",
        "    df['respiration_rate'] += np.random.normal(0, 1, total_minutes)\n",
        "    df['skin_temp'] += np.random.normal(0, 0.2, total_minutes)\n",
        "\n",
        "    df.index.name = 'timestamp'\n",
        "    canonical_columns = [\n",
        "        'lux', 'is_sleeping', 'heart_rate', 'hrv', 'respiration_rate',\n",
        "        'skin_temp', 'ambient_temp', 'meal_event', 'exercise_event'\n",
        "    ]\n",
        "    return df[canonical_columns]\n",
        "\n",
        "# =============================================================================\n",
        "# 4-1. 안정적인 기준 궤도 생성 함수\n",
        "# =============================================================================\n",
        "def calibrate_kappa(params):\n",
        "    \"\"\"\n",
        "    실험 데이터(수면만으로 하루 0.27시간 위상 변화)를 바탕으로\n",
        "    스케일링 계수 kappa를 보정(calibrate)합니다.\n",
        "    \"\"\"\n",
        "    print(\"--- 스케일링 계수 kappa 보정 시작 ---\")\n",
        "    target_phase_shift_hours = 0.27\n",
        "    target_phase_shift_rad = -target_phase_shift_hours * (2 * np.pi / 24.0)\n",
        "\n",
        "    lux_cal = np.zeros(DAY_MINUTES)\n",
        "    sleep_cal = np.zeros(DAY_MINUTES)\n",
        "    sleep_cal[0:480] = 1 # 8시간 수면\n",
        "\n",
        "    def get_phase_sensitivity(phase_angle):\n",
        "        return -np.sin(phase_angle)\n",
        "\n",
        "    def get_shift_for_kappa(kappa_trial):\n",
        "        omega_intrinsic = 2 * np.pi / (params['tau_x'] * 60)\n",
        "        phase_angle = 0.0\n",
        "        n = 0.5\n",
        "        dt = 1.0\n",
        "\n",
        "        for t in range(DAY_MINUTES):\n",
        "            x, xc = np.cos(phase_angle), np.sin(phase_angle)\n",
        "            y = np.array([x, xc, n])\n",
        "\n",
        "            _, Ns, dn_dt = get_external_forces(t, y, params, None, lux_cal, sleep_cal)\n",
        "\n",
        "            F = Ns\n",
        "            Z = get_phase_sensitivity(phase_angle)\n",
        "\n",
        "            d_phase_dt = omega_intrinsic + kappa_trial * Z * F\n",
        "            phase_angle += d_phase_dt * dt\n",
        "            n += dn_dt * dt\n",
        "\n",
        "        total_shift = (phase_angle - (omega_intrinsic * DAY_MINUTES))\n",
        "        return total_shift\n",
        "\n",
        "    low_kappa, high_kappa = 0, 1.0\n",
        "    for _ in range(15):\n",
        "        mid_kappa = (low_kappa + high_kappa) / 2\n",
        "        shift = get_shift_for_kappa(mid_kappa)\n",
        "        if shift < target_phase_shift_rad:\n",
        "            high_kappa = mid_kappa\n",
        "        else:\n",
        "            low_kappa = mid_kappa\n",
        "\n",
        "    kappa_final = (low_kappa + high_kappa) / 2\n",
        "    print(f\"보정된 kappa 값: {kappa_final:.6f}\")\n",
        "    return kappa_final\n",
        "\n",
        "def generate_baseline_trajectory(df, params):\n",
        "    \"\"\"\n",
        "    '위상 모델링'과 '스케일 보정'을 사용하여 안정적인 궤도를 생성합니다.\n",
        "    \"\"\"\n",
        "    print(\"--- [v0.14.9] 기준 궤도 생성 시작 (위상 모델링 + 스케일 보정) ---\")\n",
        "\n",
        "    # 1. 스케일링 계수(kappa)를 실험 데이터 기반으로 보정\n",
        "    kappa = calibrate_kappa(params)\n",
        "\n",
        "    # 2. 시뮬레이션에 필요한 상수 및 변수 초기화\n",
        "    total_minutes = len(df)\n",
        "    full_trajectory = np.zeros((total_minutes, 3))\n",
        "\n",
        "    omega_intrinsic = 2 * np.pi / (params['tau_x'] * 60) # 내재 각속도\n",
        "\n",
        "    def get_phase_sensitivity(phase_angle):\n",
        "        # x에 대한 힘(F)이 위상에 미치는 영향은 -sin(theta)에 비례\n",
        "        # xc에 대한 힘은 cos(theta)에 비례하지만, 모델에서 F는 dx_dt에만 직접 더해짐\n",
        "        return -np.sin(phase_angle)\n",
        "\n",
        "    # 사용자의 평균 위상을 기준으로 초기 위상각 결정\n",
        "    avg_hr_nadir_minute = int(find_hr_nadir(df['heart_rate'].values, df['is_sleeping'].values))\n",
        "    phase_angle_at_cbt_nadir = -np.pi\n",
        "    rad_per_minute = 2 * np.pi / DAY_MINUTES\n",
        "    initial_phase_angle = phase_angle_at_cbt_nadir - (avg_hr_nadir_minute - 120) * rad_per_minute\n",
        "\n",
        "    current_phase_angle = initial_phase_angle\n",
        "    current_n = 0.5\n",
        "\n",
        "    dt = 1.0  # 1분 단위로 시뮬레이션\n",
        "\n",
        "    # 3. '위상 전용' 메인 시뮬레이션 루프\n",
        "    print(\"보정된 kappa를 사용하여 최종 위상 시뮬레이션 시작...\")\n",
        "    for t in tqdm(range(total_minutes), desc=\"위상 모델링 시뮬레이션 중\"):\n",
        "        # A. 현재 (x, xc) 상태 계산\n",
        "        x = np.cos(current_phase_angle)\n",
        "        xc = np.sin(current_phase_angle)\n",
        "        y_current = np.array([x, xc, current_n])\n",
        "\n",
        "        # B. 현재 상태를 궤도에 기록\n",
        "        full_trajectory[t, :] = y_current\n",
        "\n",
        "        # C. 외부 자극(B, Ns) 계산\n",
        "        B, Ns, dn_dt = get_external_forces(t, y_current, params, df.index, df['lux'].values, df['is_sleeping'].values)\n",
        "        F = B + Ns # 외부 힘의 총합\n",
        "\n",
        "        # D. 위상 민감도 계산\n",
        "        Z = get_phase_sensitivity(current_phase_angle)\n",
        "\n",
        "        # E. 위상 변화율 계산 및 업데이트\n",
        "        d_phase_dt = omega_intrinsic + kappa * Z * F\n",
        "        current_phase_angle += d_phase_dt * dt\n",
        "        current_n += dn_dt * dt\n",
        "\n",
        "    print(\"--- 안정적인 기준 궤도 생성 완료 ---\")\n",
        "    return full_trajectory\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 시각화 함수\n",
        "# =============================================================================\n",
        "def plot_multiday_data_summary(df_raw, df_with_baseline, start_day_index, num_days, output_dir):\n",
        "    \"\"\"생성된 데이터 중 연속된 여러 날에 대한 종합 그래프를 저장합니다.\"\"\"\n",
        "    print(f\"--- {start_day_index+1}일부터 {num_days}일간 데이터 요약 그래프 생성 ---\\n\")\n",
        "    start = start_day_index * DAY_MINUTES\n",
        "    end = (start_day_index + num_days) * DAY_MINUTES\n",
        "\n",
        "    multiday_raw = df_raw.iloc[start:end]\n",
        "    multiday_baseline = df_with_baseline.iloc[start:end]\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 25))\n",
        "    gs = fig.add_gridspec(5, 1)\n",
        "    fig.suptitle(f'Data Summary for Days {start_day_index + 1} to {start_day_index + num_days} (v0.14.9 - Scaled Phase Model)', fontsize=20)\n",
        "\n",
        "    # Plot 1: Heart Rate and Sleep\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    color1 = 'red'\n",
        "    ax0.set_ylabel('Heart Rate (bpm)', color=color1, fontsize=12)\n",
        "    ax0.plot(multiday_raw.index, multiday_raw['heart_rate'], label='Heart Rate (bpm)', color=color1)\n",
        "    ax0.tick_params(axis='y', labelcolor=color1)\n",
        "    ax0.set_title('Heart Rate and Sleep', fontsize=14)\n",
        "    ax0.grid(True)\n",
        "\n",
        "    ax0_twin = ax0.twinx()\n",
        "    color2 = 'gray'\n",
        "    ax0_twin.set_ylabel('Sleep State', color=color2, fontsize=12)\n",
        "    ax0_twin.fill_between(multiday_raw.index, 0, multiday_raw['is_sleeping'],\n",
        "                          label='Is Sleeping', color=color2, alpha=0.3, step='post')\n",
        "    ax0_twin.tick_params(axis='y', labelcolor=color2)\n",
        "    ax0_twin.set_ylim(-0.1, 1.1)\n",
        "\n",
        "    # Plot 2: Light Exposure (Lux)\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.plot(multiday_raw.index, multiday_raw['lux'], label='Light Exposure (Lux)', color='orange')\n",
        "    ax1.set_ylabel('Lux')\n",
        "    ax1.set_title('Light Exposure (Zeitgeber)', fontsize=14)\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot 3: Baseline Circadian States (Time Series)\n",
        "    ax2 = fig.add_subplot(gs[2, 0], sharex=ax0)\n",
        "    ax2.plot(multiday_baseline.index, multiday_baseline['x_base'], label='Core Body Temp Rhythm (x)', color='blue')\n",
        "    ax2.plot(multiday_baseline.index, multiday_baseline['xc_base'], label='Sleep/Wake Rhythm (xc)', color='green')\n",
        "    ax2.set_ylabel('State Value')\n",
        "    ax2.set_title('Baseline Circadian States (Time Series)', fontsize=14)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 4: Circadian State-Space Plot (Phase Portrait)\n",
        "    ax3 = fig.add_subplot(gs[3, 0])\n",
        "    ax3.plot(multiday_baseline['x_base'], multiday_baseline['xc_base'], color='purple')\n",
        "    ax3.set_xlabel('Core Body Temp Rhythm (x)')\n",
        "    ax3.set_ylabel('Sleep/Wake Rhythm (xc)')\n",
        "    ax3.set_title('Circadian State-Space Cycle', fontsize=14)\n",
        "    ax3.grid(True)\n",
        "    ax3.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # Plot 5: Other Biometric Data\n",
        "    ax4 = fig.add_subplot(gs[4, 0], sharex=ax0)\n",
        "    color1 = 'purple'\n",
        "    ax4.set_xlabel('Time')\n",
        "    ax4.set_ylabel('Skin Temp (°C)', color=color1, fontsize=12)\n",
        "    ax4.plot(multiday_raw.index, multiday_raw['skin_temp'], label='Skin Temp (°C)', color=color1)\n",
        "    ax4.tick_params(axis='y', labelcolor=color1)\n",
        "    ax4.grid(True)\n",
        "\n",
        "    ax4_twin = ax4.twinx()\n",
        "    color2 = 'brown'\n",
        "    ax4_twin.set_ylabel('Respiration Rate', color=color2, fontsize=12)\n",
        "    ax4_twin.plot(multiday_raw.index, multiday_raw['respiration_rate'], label='Respiration Rate', color=color2, alpha=0.7)\n",
        "    ax4_twin.tick_params(axis='y', labelcolor=color2)\n",
        "    ax4.set_title('Other Biometric Signals', fontsize=14)\n",
        "\n",
        "    lines, labels = ax4.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
        "    ax4_twin.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "\n",
        "    plt.setp(ax0.get_xticklabels(), visible=False)\n",
        "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
        "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
        "\n",
        "    fig.autofmt_xdate()\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "    save_path = os.path.join(output_dir, f'days_{start_day_index+1}_to_{start_day_index+num_days}_summary.png')\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "    print(f\"데이터 요약 그래프가 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "\n",
        "def plot_learning_curve(history, output_dir):\n",
        "    \"\"\"학습 및 검증 손실 그래프를 저장합니다.\"\"\"\n",
        "    print(\"--- 학습 과정 손실 그래프 생성 ---\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.plot(history['train_loss'], label='Training Loss', marker='o')\n",
        "    ax.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    ax.set_title('Model Loss Over Epochs')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss (Scaled MSE) - Log Scale')\n",
        "    ax.set_yscale('log')\n",
        "    ax.legend()\n",
        "    ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    save_path = os.path.join(output_dir, 'learning_curve.png')\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "    print(f\"학습 손실 그래프가 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "def plot_performance_summary(actual, predicted, output_dir):\n",
        "    \"\"\"예측 성능 요약 그래프(산점도, 오차 히스토그램)를 저장합니다.\"\"\"\n",
        "    print(\"--- 예측 성능 요약 그래프 생성 ---\")\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Scatter plot of Actual vs. Predicted\n",
        "    axs[0].scatter(actual, predicted, alpha=0.5, s=10, label=f'R²: {np.corrcoef(actual, predicted)[0,1]**2:.3f}')\n",
        "    min_val = min(np.min(actual), np.min(predicted))\n",
        "    max_val = max(np.max(actual), np.max(predicted))\n",
        "    axs[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n",
        "    axs[0].set_xlabel('Actual Heart Rate')\n",
        "    axs[0].set_ylabel('Predicted Heart Rate')\n",
        "    axs[0].set_title('Actual vs. Predicted Heart Rate')\n",
        "    axs[0].legend()\n",
        "    axs[0].grid(True)\n",
        "    axs[0].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # Histogram of Prediction Errors\n",
        "    errors = predicted - actual\n",
        "    axs[1].hist(errors, bins=50, edgecolor='black')\n",
        "    axs[1].axvline(errors.mean(), color='red', linestyle='dashed', linewidth=2, label=f'Mean Error: {errors.mean():.2f}')\n",
        "    axs[1].axvline(np.median(errors), color='green', linestyle='dashed', linewidth=2, label=f'Median Error: {np.median(errors):.2f}')\n",
        "    axs[1].set_xlabel('Prediction Error (Predicted - Actual)')\n",
        "    axs[1].set_ylabel('Frequency')\n",
        "    axs[1].set_title('Distribution of Prediction Errors')\n",
        "    axs[1].legend()\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(output_dir, 'performance_summary.png')\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "    print(f\"성능 요약 그래프가 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 학습, 평가, 예측 파이프라인\n",
        "# =============================================================================\n",
        "def train_and_evaluate(train_tfrecord, val_tfrecord, num_train_samples, num_val_samples, config):\n",
        "    print(\"\\n--- 최종 모델 학습 및 검증 시작 ---\")\n",
        "\n",
        "    def _parse_function(example_proto):\n",
        "        feature_description = {\n",
        "            'full_inputs': tf.io.FixedLenFeature([], tf.string),\n",
        "            'baseline_inputs': tf.io.FixedLenFeature([], tf.string),\n",
        "            'past_info': tf.io.FixedLenFeature([], tf.string),\n",
        "            'encoded_features': tf.io.FixedLenFeature([], tf.string),\n",
        "            'label': tf.io.FixedLenFeature([], tf.string),\n",
        "        }\n",
        "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "        full_inputs = tf.io.parse_tensor(parsed_features['full_inputs'], out_type=tf.float32)\n",
        "        baseline_inputs = tf.io.parse_tensor(parsed_features['baseline_inputs'], out_type=tf.float32)\n",
        "        past_info = tf.io.parse_tensor(parsed_features['past_info'], out_type=tf.float32)\n",
        "        encoded_features = tf.io.parse_tensor(parsed_features['encoded_features'], out_type=tf.float32)\n",
        "        label = tf.io.parse_tensor(parsed_features['label'], out_type=tf.float32)\n",
        "\n",
        "        inputs = {\n",
        "            \"full_inputs\": full_inputs,\n",
        "            \"baseline_inputs\": baseline_inputs,\n",
        "            \"past_info\": past_info,\n",
        "            \"encoded_features\": encoded_features\n",
        "        }\n",
        "        return inputs, label\n",
        "\n",
        "    def make_dataset(file_path):\n",
        "        dataset = tf.data.TFRecordDataset(file_path, num_parallel_reads=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.shuffle(buffer_size=256, reshuffle_each_iteration=True)\n",
        "        dataset = dataset.batch(config['batch_size'])\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "    train_dataset = make_dataset(train_tfrecord)\n",
        "    val_dataset = make_dataset(val_tfrecord)\n",
        "\n",
        "    phase_correction_model = build_phase_correction_model(config['correction_lookback'], config['num_harmonics'])\n",
        "    main_predictor_config = {k: v for k, v in config.items() if k in ['num_layers', 'd_model', 'num_heads', 'dff', 'rate']}\n",
        "    main_predictor = MainPredictor(prediction_horizon=config['pred_horizon'], **main_predictor_config)\n",
        "    integrated_model = IntegratedModel(phase_correction_model, main_predictor, config)\n",
        "\n",
        "    target_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'target_scaler.gz'))\n",
        "    target_scaler_mean = tf.constant(target_scaler.mean_[0], dtype=tf.float32)\n",
        "    target_scaler_scale = tf.constant(target_scaler.scale_[0], dtype=tf.float32)\n",
        "    mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    optimizer = Adam(learning_rate=config['learning_rate'])\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(batch_x, batch_y):\n",
        "        batch_y_scaled = (batch_y - target_scaler_mean) / target_scaler_scale\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = integrated_model(batch_x, training=True)\n",
        "            main_loss = mse_loss_fn(batch_y_scaled, y_pred)\n",
        "            total_loss = main_loss + sum(integrated_model.losses)\n",
        "        grads = tape.gradient(total_loss, integrated_model.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
        "        optimizer.apply_gradients(zip(grads, integrated_model.trainable_variables))\n",
        "        return total_loss\n",
        "\n",
        "    @tf.function\n",
        "    def val_step(batch_x, batch_y):\n",
        "        batch_y_scaled = (batch_y - target_scaler_mean) / target_scaler_scale\n",
        "        y_pred = integrated_model(batch_x, training=False)\n",
        "        return mse_loss_fn(batch_y_scaled, y_pred)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    best_val_loss = float('inf')\n",
        "    model_save_path = os.path.join(OUTPUT_DIR, \"best_model.weights.h5\")\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "        num_train_batches = num_train_samples // config['batch_size']\n",
        "        progbar = tf.keras.utils.Progbar(num_train_batches, stateful_metrics=['train_loss'])\n",
        "\n",
        "        epoch_train_losses = []\n",
        "        for i, (batch_x, batch_y) in enumerate(train_dataset):\n",
        "            loss = train_step(batch_x, batch_y)\n",
        "            epoch_train_losses.append(loss)\n",
        "            progbar.update(i + 1, values=[('train_loss', loss)])\n",
        "\n",
        "        history['train_loss'].append(np.mean(epoch_train_losses))\n",
        "\n",
        "        epoch_val_losses = []\n",
        "        for batch_x, batch_y in val_dataset:\n",
        "            epoch_val_losses.append(val_step(batch_x, batch_y))\n",
        "\n",
        "        val_loss = tf.reduce_mean(epoch_val_losses)\n",
        "        history['val_loss'].append(val_loss.numpy())\n",
        "\n",
        "        print(f\"\\nValidation Loss (scaled): {val_loss.numpy():.4f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            print(f\"Validation loss improved. Saving model weights to {model_save_path}\")\n",
        "            best_val_loss = val_loss\n",
        "            integrated_model.save_weights(model_save_path)\n",
        "\n",
        "    print(\"\\n--- 학습 및 검증 완료 ---\")\n",
        "    return integrated_model, val_dataset, history\n",
        "\n",
        "def predict_future_hr(model, feature_extractor, data_slice_unscaled, config, feature_scaler, target_scaler):\n",
        "    print(\"\\n--- 미래 심박수 예측 시작 ---\")\n",
        "    expected_len = config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon']\n",
        "    if len(data_slice_unscaled) != expected_len:\n",
        "        raise ValueError(f\"입력 데이터의 길이는 {expected_len}이어야 합니다. 현재 길이: {len(data_slice_unscaled)}\")\n",
        "\n",
        "    feature_cols = [c for c in data_slice_unscaled.columns if c not in ['x', 'xc']]\n",
        "    data_slice_scaled_features = feature_scaler.transform(data_slice_unscaled[feature_cols])\n",
        "    data_slice_scaled = data_slice_unscaled.copy()\n",
        "    data_slice_scaled[feature_cols] = data_slice_scaled_features\n",
        "\n",
        "    x_slice_unscaled = data_slice_unscaled.values\n",
        "    x_slice_scaled = data_slice_scaled.values\n",
        "\n",
        "    full_inputs = x_slice_scaled\n",
        "    baseline_cols = ['x_base', 'xc_base']\n",
        "    baseline_indices = [data_slice_unscaled.columns.get_loc(c) for c in baseline_cols]\n",
        "    baseline_inputs = x_slice_scaled[:, baseline_indices]\n",
        "\n",
        "    main_input_start = config['correction_lookback']\n",
        "    main_input_end = main_input_start + config['input_seq_len']\n",
        "    main_input_unscaled = x_slice_unscaled[main_input_start:main_input_end]\n",
        "    main_input_scaled = x_slice_scaled[main_input_start:main_input_end]\n",
        "\n",
        "    cnn_inputs_np = [\n",
        "        main_input_scaled[:, 9:11], main_input_scaled[:, 0:1],\n",
        "        main_input_scaled[:, 1:2],   main_input_scaled[:, 2:5],\n",
        "        main_input_scaled[:, 5:6],   main_input_scaled[:, 7:8],\n",
        "        main_input_scaled[:, 8:9],   main_input_scaled[:, 6:7],\n",
        "    ]\n",
        "    cnn_inputs_tf = [tf.convert_to_tensor(inp[np.newaxis, ...], dtype=tf.float32) for inp in cnn_inputs_np]\n",
        "\n",
        "    encoded_features = feature_extractor(cnn_inputs_tf, training=False)\n",
        "\n",
        "    past_info = generate_past_info_np(main_input_unscaled, encoded_features.numpy().squeeze(0), config['d_model'])\n",
        "\n",
        "    model_input = {\n",
        "        \"full_inputs\": tf.constant(full_inputs[np.newaxis, ...], dtype=tf.float32),\n",
        "        \"baseline_inputs\": tf.constant(baseline_inputs[np.newaxis, ...], dtype=tf.float32),\n",
        "        \"past_info\": tf.constant(past_info[np.newaxis, ...], dtype=tf.float32),\n",
        "        \"encoded_features\": encoded_features\n",
        "    }\n",
        "\n",
        "    prediction_scaled = model.predict(model_input)\n",
        "    prediction_original = target_scaler.inverse_transform(prediction_scaled)\n",
        "    print(\"--- 예측 완료 ---\")\n",
        "    return prediction_original.flatten()\n",
        "\n",
        "# =============================================================================\n",
        "# 7. 메인 실행 블록 및 모듈화된 함수\n",
        "# =============================================================================\n",
        "def generate_and_visualize_data(config):\n",
        "    \"\"\"가상 생체 데이터를 생성하고 요약 그래프를 출력합니다.\"\"\"\n",
        "    print(\"--- 데이터 생성 및 시각화 시작 ---\")\n",
        "\n",
        "    # 데이터 생성\n",
        "    max_len_minutes = (config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon'])\n",
        "    data_duration = config.get('data_duration_days', DATA_DURATION_DAYS)\n",
        "    total_duration_to_generate = data_duration + (max_len_minutes // DAY_MINUTES) + 2\n",
        "\n",
        "    df_raw = generate_initial_dataframe(OUTPUT_DIR, total_duration_to_generate)\n",
        "    baseline_trajectory = generate_baseline_trajectory(df_raw, PARAMS)\n",
        "    df = df_raw.copy()\n",
        "    df[['x_base', 'xc_base', 'n_base']] = baseline_trajectory\n",
        "    df['x'] = 0.0; df['xc'] = 0.0\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object' or pd.api.types.is_integer_dtype(df[col]):\n",
        "            df[col] = df[col].astype(float)\n",
        "\n",
        "    # 데이터 시각화\n",
        "    num_days_to_plot = 3\n",
        "    latest_start_day = max(0, data_duration - num_days_to_plot)\n",
        "    random_start_day = np.random.randint(0, latest_start_day + 1) if latest_start_day > 0 else 0\n",
        "\n",
        "    plot_multiday_data_summary(df_raw, df, start_day_index=random_start_day, num_days=num_days_to_plot, output_dir=OUTPUT_DIR)\n",
        "\n",
        "    print(\"--- 데이터 생성 및 시각화 완료 ---\")\n",
        "    return df_raw, df\n",
        "\n",
        "def run_training_pipeline(config):\n",
        "    \"\"\"데이터 생성부터 학습, 예측, 시각화까지 전체 파이프라인을 실행합니다.\"\"\"\n",
        "    print(\"--- 'train' 모드 시작: 전체 파이프라인을 실행합니다. ---\")\n",
        "\n",
        "    df_raw, df = generate_and_visualize_data(config)\n",
        "\n",
        "    # 데이터 분할 및 스케일링\n",
        "    print(\"\\n--- 데이터 분할 및 스케일링 시작 ---\")\n",
        "    max_len = (config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon'])\n",
        "    num_samples = len(df) - max_len\n",
        "    all_indices = np.arange(num_samples)\n",
        "    np.random.shuffle(all_indices)\n",
        "    train_end = int(num_samples * TRAIN_RATIO)\n",
        "    val_end = train_end + int(num_samples * VALIDATION_RATIO)\n",
        "    train_indices = all_indices[:train_end]\n",
        "    val_indices = all_indices[train_end:val_end]\n",
        "\n",
        "    train_df_for_scaling = df.iloc[:train_indices[-1] + max_len] if len(train_indices) > 0 else df\n",
        "\n",
        "    feature_cols = [c for c in df.columns if c not in ['x', 'xc']]\n",
        "    feature_scaler = StandardScaler()\n",
        "    target_scaler = StandardScaler()\n",
        "    feature_scaler.fit(train_df_for_scaling[feature_cols])\n",
        "    target_scaler.fit(train_df_for_scaling[['heart_rate']])\n",
        "\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[feature_cols] = feature_scaler.transform(df[feature_cols])\n",
        "\n",
        "    joblib.dump(feature_scaler, os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "    joblib.dump(target_scaler, os.path.join(OUTPUT_DIR, 'target_scaler.gz'))\n",
        "    print(\"스케일러 학습 및 저장 완료.\")\n",
        "\n",
        "    # TFRecord 생성\n",
        "    temp_feature_extractor = build_feature_extractor(config['input_seq_len'], config['d_model'])\n",
        "    train_tfrecord_path = os.path.join(OUTPUT_DIR, \"train.tfrecord\")\n",
        "    val_tfrecord_path = os.path.join(OUTPUT_DIR, \"val.tfrecord\")\n",
        "    create_tfrecords(df, df_scaled, train_indices, config, temp_feature_extractor, train_tfrecord_path)\n",
        "    create_tfrecords(df, df_scaled, val_indices, config, temp_feature_extractor, val_tfrecord_path)\n",
        "\n",
        "    # 모델 학습\n",
        "    _, val_dataset_for_build, history = train_and_evaluate(\n",
        "        train_tfrecord_path, val_tfrecord_path,\n",
        "        len(train_indices), len(val_indices),\n",
        "        config\n",
        "    )\n",
        "\n",
        "    # 학습 결과 시각화\n",
        "    plot_learning_curve(history, OUTPUT_DIR)\n",
        "\n",
        "    # 예측 및 성능 평가\n",
        "    run_prediction_only(config, df, df_raw, val_dataset_for_build)\n",
        "\n",
        "    # 모델 구조도 시각화\n",
        "    visualize_architecture_only(config)\n",
        "\n",
        "    print(\"\\n\\n===========================================================\")\n",
        "    print(\"    생체리듬 예측 AI 전체 파이프라인 실행 완료 (v0.14.9 - 스케일 보정)   \")\n",
        "    print(\"===========================================================\")\n",
        "\n",
        "def run_data_generation_only(config):\n",
        "    \"\"\"데이터 생성 및 시각화 기능만 독립적으로 실행합니다.\"\"\"\n",
        "    print(\"\\n--- 'generate_data' 모드 시작: 데이터 생성 및 그래프 출력만 수행합니다. ---\")\n",
        "    generate_and_visualize_data(config)\n",
        "    print(\"\\n===========================================================\")\n",
        "    print(\"    데이터 생성 및 시각화 완료 (generate_data 모드)   \")\n",
        "    print(\"===========================================================\")\n",
        "\n",
        "def run_prediction_only(config, df=None, df_raw=None, val_dataset_for_build=None):\n",
        "    \"\"\"저장된 모델을 불러와 예측 및 결과 시각화만 수행합니다.\"\"\"\n",
        "    print(\"\\n--- 'predict' 모드 시작: 저장된 모델로 예측을 수행합니다. ---\")\n",
        "\n",
        "    best_model_path = os.path.join(OUTPUT_DIR, \"best_model.weights.h5\")\n",
        "    if not os.path.exists(best_model_path):\n",
        "        print(f\"오류: 모델 가중치 파일({best_model_path})을 찾을 수 없습니다.\")\n",
        "        print(\"'train' 모드를 먼저 실행하여 모델을 학습시켜주세요.\")\n",
        "        return\n",
        "\n",
        "    # 모델 로드\n",
        "    phase_correction_model_reloaded = build_phase_correction_model(config['correction_lookback'], config['num_harmonics'])\n",
        "    main_predictor_config_reloaded = {k: v for k, v in config.items() if k in ['num_layers', 'd_model', 'num_heads', 'dff', 'rate']}\n",
        "    main_predictor_reloaded = MainPredictor(prediction_horizon=config['pred_horizon'], **main_predictor_config_reloaded)\n",
        "    loaded_model = IntegratedModel(phase_correction_model_reloaded, main_predictor_reloaded, config)\n",
        "    feature_extractor_for_pred = build_feature_extractor(time_steps=config['input_seq_len'], d_model=config['d_model'])\n",
        "\n",
        "    if val_dataset_for_build is None:\n",
        "        print(\"검증 데이터셋이 없어 더미 데이터로 모델을 빌드합니다.\")\n",
        "        dummy_inputs = {\n",
        "            \"full_inputs\": tf.zeros((1, config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon'], 14)),\n",
        "            \"baseline_inputs\": tf.zeros((1, config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon'], 2)),\n",
        "            \"past_info\": tf.zeros((1, NUM_MARKERS_TO_KEEP * 3, config['d_model'])),\n",
        "            \"encoded_features\": tf.zeros((1, config['input_seq_len'], config['d_model']))\n",
        "        }\n",
        "        loaded_model(dummy_inputs, training=False)\n",
        "    else:\n",
        "        dummy_x, _ = next(iter(val_dataset_for_build))\n",
        "        loaded_model(dummy_x, training=False)\n",
        "\n",
        "    loaded_model.load_weights(best_model_path)\n",
        "    print(\"모델 가중치 로드 완료.\")\n",
        "\n",
        "    loaded_feature_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "    loaded_target_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'target_scaler.gz'))\n",
        "    print(\"스케일러 로드 완료.\")\n",
        "\n",
        "    if df is None or df_raw is None:\n",
        "        print(\"예측을 위한 새로운 데이터를 생성합니다.\")\n",
        "        max_len_minutes = (config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon'])\n",
        "        total_duration_to_generate = DATA_DURATION_DAYS + (max_len_minutes // DAY_MINUTES) + 2\n",
        "        df_raw = generate_initial_dataframe(OUTPUT_DIR, total_duration_to_generate)\n",
        "        baseline_trajectory = generate_baseline_trajectory(df_raw, PARAMS)\n",
        "        df = df_raw.copy()\n",
        "        df[['x_base', 'xc_base', 'n_base']] = baseline_trajectory\n",
        "        df['x'] = 0.0; df['xc'] = 0.0\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object' or pd.api.types.is_integer_dtype(df[col]):\n",
        "                df[col] = df[col].astype(float)\n",
        "\n",
        "    max_len = (config['correction_lookback'] + config['input_seq_len'] + config['pred_horizon'])\n",
        "    test_slice_start = len(df) - max_len\n",
        "    test_data_slice = df.iloc[test_slice_start : test_slice_start + max_len]\n",
        "\n",
        "    predicted_hr = predict_future_hr(\n",
        "        loaded_model, feature_extractor_for_pred, test_data_slice, config,\n",
        "        loaded_feature_scaler, loaded_target_scaler\n",
        "    )\n",
        "\n",
        "    actual_hr = df_raw.iloc[test_slice_start + max_len - config['pred_horizon'] : test_slice_start + max_len]['heart_rate'].values\n",
        "    actual_hr_df = df_raw.iloc[test_slice_start + max_len - config['pred_horizon'] : test_slice_start + max_len]\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.plot(actual_hr_df.index, actual_hr, label='Actual Heart Rate', color='blue')\n",
        "    plt.plot(actual_hr_df.index, predicted_hr, label='Predicted Heart Rate', color='red', linestyle='--')\n",
        "    plt.title('Heart Rate Prediction vs Actual (Time Series)')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Heart Rate (bpm)')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(OUTPUT_DIR, \"prediction_vs_actual.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "    print(f\"\\n예측 결과 그래프가 {plot_path}에 저장되었습니다.\")\n",
        "\n",
        "    plot_performance_summary(actual_hr, predicted_hr, OUTPUT_DIR)\n",
        "\n",
        "def visualize_architecture_only(config):\n",
        "    \"\"\"모델 구조도 이미지만 생성합니다.\"\"\"\n",
        "    print(\"\\n--- 'visualize_arch' 모드 시작: 모델 구조도를 생성합니다. ---\")\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    try:\n",
        "        phase_correction_model = build_phase_correction_model(config['correction_lookback'], config['num_harmonics'])\n",
        "        feature_extractor = build_feature_extractor(time_steps=config['input_seq_len'], d_model=config['d_model'])\n",
        "        main_predictor_config = {k: v for k, v in config.items() if k in ['num_layers', 'd_model', 'num_heads', 'dff', 'rate']}\n",
        "        main_predictor = MainPredictor(prediction_horizon=config['pred_horizon'], **main_predictor_config)\n",
        "\n",
        "        tf.keras.utils.plot_model(phase_correction_model, to_file=os.path.join(OUTPUT_DIR, 'phase_correction_model.png'), show_shapes=True)\n",
        "        tf.keras.utils.plot_model(feature_extractor, to_file=os.path.join(OUTPUT_DIR, 'feature_extractor_model.png'), show_shapes=True)\n",
        "\n",
        "        dummy_encoded_features = tf.keras.Input(shape=(config['input_seq_len'], config['d_model']))\n",
        "        dummy_past_info = tf.keras.Input(shape=(NUM_MARKERS_TO_KEEP * 3, config['d_model']))\n",
        "        dummy_last_lco = tf.keras.Input(shape=(2,))\n",
        "        dummy_outputs = main_predictor((dummy_encoded_features, dummy_past_info, dummy_last_lco))\n",
        "        plot_model = Model(inputs=[dummy_encoded_features, dummy_past_info, dummy_last_lco], outputs=dummy_outputs)\n",
        "        tf.keras.utils.plot_model(plot_model, to_file=os.path.join(OUTPUT_DIR, 'main_predictor_model.png'), show_shapes=True, expand_nested=True)\n",
        "\n",
        "        print(f\"모델 구조 이미지가 {OUTPUT_DIR}에 저장되었습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"모델 시각화 중 오류 발생: {e}\")\n",
        "        print(\"pydot과 graphviz가 설치되어 있는지 확인해주세요. (pip install pydot graphviz)\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # =========================================================================\n",
        "    # 실행할 모드를 여기서 직접 설정하세요:\n",
        "    # 'train', 'predict', 'visualize_arch', 'generate_data'\n",
        "    # =========================================================================\n",
        "    EXECUTION_MODE = 'generate_data'\n",
        "\n",
        "    config = {\n",
        "        'batch_size': BATCH_SIZE, 'input_seq_len': INPUT_SEQUENCE_LENGTH,\n",
        "        'pred_horizon': PREDICTION_HORIZON, 'correction_lookback': PHASE_CORRECTION_LOOKBACK,\n",
        "        'lambda_reg': LAMBDA_REG, 'lambda_cont': LAMBDA_CONT, 'lambda_anchor': LAMBDA_ANCHOR,\n",
        "        'num_layers': NUM_LAYERS, 'd_model': D_MODEL, 'num_heads': NUM_HEADS, 'dff': DFF, 'rate': DROPOUT_RATE,\n",
        "        'epochs': EPOCHS, 'learning_rate': LEARNING_RATE, 'num_harmonics': NUM_FOURIER_HARMONICS,\n",
        "        'data_duration_days': DATA_DURATION_DAYS\n",
        "    }\n",
        "\n",
        "    if EXECUTION_MODE == 'train':\n",
        "        run_training_pipeline(config)\n",
        "    elif EXECUTION_MODE == 'predict':\n",
        "        run_prediction_only(config)\n",
        "    elif EXECUTION_MODE == 'visualize_arch':\n",
        "        visualize_architecture_only(config)\n",
        "    elif EXECUTION_MODE == 'generate_data':\n",
        "        run_data_generation_only(config)\n",
        "    else:\n",
        "        print(f\"알 수 없는 모드입니다: {EXECUTION_MODE}\")\n",
        "        print(\"'train', 'predict', 'visualize_arch', 'generate_data' 중에서 선택해주세요.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eHSuugwUMhsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}