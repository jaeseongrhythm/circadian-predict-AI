{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "일주기 생체리듬 예측 AI v0.16.1 (past_info 로직 개선)\n",
        "\n",
        "[v0.16.1 변경 사항]:\n",
        "- past_info 추출 로직 개선:\n",
        "  - 기존: past_info 탐색 시, 5일 길이의 과거 구간에서 양 끝의 불완전한 하루 데이터를 버리고 계산하여 정보 손실 발생.\n",
        "  - 개선:\n",
        "    1. 탐색 영역 확장: 불완전한 첫날과 마지막 날을 보완하기 위해, 앞뒤의 lookback, current 구간 데이터를 임시로 참조하여 '완전한 하루'를 구성. 이를 통해 경계선 부근에서도 안정적인 생체 마커 계산이 가능해짐.\n",
        "    2. 선택 영역 유지: 마커 탐색은 확장된 영역에서 수행하되, 최종 마커는 반드시 원래의 5일 '과거' 구간 내에서만 선택.\n",
        "\n",
        "[v0.16.0 최종 아키텍처 주요 변경]:\n",
        "1.  버퍼 및 정렬 (데이터 파이프라인):\n",
        "    - create_tfrecords: 이제 각 데이터 샘플은 자정부터 시작하여 자정으로 끝나는\n",
        "      더 긴 데이터 조각(약 8~9일)을 포함합니다. 이를 통해 모델이 항상 자정 기준의\n",
        "      데이터를 사용할 수 있도록 보장.\n",
        "    - 원래 샘플의 시작 시간 오프셋(offset) 정보도 함께 저장하여, 모델이 최종 보정\n",
        "      곡선을 원래 시간축에 정확히 정렬하는 데 사용.\n",
        "\n",
        "2.  슬라이딩 윈도우 보정 (모델 아키텍처):\n",
        "    - IntegratedModel: 모델 내부에 tf.TensorArray를 사용한 루프를 구현하여,\n",
        "      입력된 긴 데이터 조각 안에서 3일짜리 lookback 창을 하루씩 이동.\n",
        "    - 매일 이동된 창의 데이터를 FourierCorrectionModel에 입력하여, 해당 날짜에만\n",
        "      적용될 고유한 일일 보정 곡선을 생성.\n",
        "    - 이 과정을 반복하여, 각 날짜가 항상 직전 3일의 최신 정보를 바탕으로 보정되도록\n",
        "      하는 슬라이딩 윈도우 보정을 완벽하게 구현.\n",
        "\n",
        "3.  일일 재조정 기준 궤도 (CPU Pre-processing):\n",
        "    - 물리 모델(ODE)은 학습 루프에서 분리되어, CPU에서 미리 하루 단위의 불연속적인\n",
        "      기준 궤도를 계산.\n",
        "\n",
        "4.  End-to-End 학습 및 강화된 손실 함수:\n",
        "    - 위의 모든 딥러닝 모델은 최종 예측 오차를 바탕으로 가중치가 한 번에 업데이트되는\n",
        "      End-to-End 방식으로 학습.\n",
        "    - 궤도의 연속성과 앵커 일치를 강제하는 손실 함수의 가중치를 높여, 물리적으로\n",
        "      더 타당하고 안정적인 궤적 생성을 유도.\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 0. 라이브러리 임포트 및 파이프라인 설정\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.interpolate import interp1d\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Concatenate, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# --- 파이프라인 제어 설정 ---\n",
        "OUTPUT_DIR = \"final_biometric_pipeline_output_v0.16.1_PastInfoFix\"\n",
        "\n",
        "# --- 데이터 및 모델 설정 (PC 환경 최적화) ---\n",
        "DATA_DURATION_DAYS = 30\n",
        "INPUT_SEQUENCE_LENGTH = 7 * 24 * 60\n",
        "PREDICTION_HORIZON = 60\n",
        "PHASE_CORRECTION_LOOKBACK_DAYS = 3\n",
        "DAY_MINUTES = 24 * 60\n",
        "NUM_MARKERS_TO_KEEP = 5\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VALIDATION_RATIO = 0.1\n",
        "\n",
        "# --- 모델 하이퍼파라미터 (성능 튜닝) ---\n",
        "D_MODEL = 192\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 8\n",
        "DFF = 384\n",
        "DROPOUT_RATE = 0.05\n",
        "NUM_FOURIER_HARMONICS = 5\n",
        "LSTM_UNITS = 64\n",
        "\n",
        "# --- 학습 하이퍼파라미터 (성능 튜닝) ---\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0005\n",
        "LAMBDA_REG = 0.1\n",
        "LAMBDA_CONT = 10.0\n",
        "LAMBDA_ANCHOR = 5.0\n",
        "\n",
        "# --- 하이퍼파라미터 검증 ---\n",
        "assert D_MODEL % NUM_HEADS == 0, f\"D_MODEL({D_MODEL})은 NUM_HEADS({NUM_HEADS})로 나누어떨어져야 합니다.\"\n",
        "\n",
        "# --- 물리 모델 파라미터 (St. Hilaire et al., 2007 기반) ---\n",
        "PARAMS = {\n",
        "    'mu': 0.13, 'q': 1/3, 'k': 0.55, 'alpha0': 0.1, 'I0': 9500,\n",
        "    'p': 0.5, 'beta': 0.007, 'G': 37, 'rho': 0.032, 'tau_x': 24.2,\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 생체리듬 물리 모델 (SCN 코어 엔진) 및 헬퍼 함수\n",
        "# =============================================================================\n",
        "def find_hr_nadir(heart_rate_data, is_sleeping_data, day_minutes=1440):\n",
        "    num_days = len(heart_rate_data) // day_minutes\n",
        "    daily_nadirs = []\n",
        "    for day in range(num_days):\n",
        "        day_start, day_end = day * day_minutes, (day + 1) * day_minutes\n",
        "        day_hr, day_sleep = heart_rate_data[day_start:day_end], is_sleeping_data[day_start:day_end]\n",
        "        sleep_hr = day_hr[day_sleep == 1]\n",
        "        if len(sleep_hr) > 0:\n",
        "            original_indices = np.where(day_sleep == 1)[0]\n",
        "            daily_nadirs.append(original_indices[np.argmin(sleep_hr)])\n",
        "        else:\n",
        "            daily_nadirs.append(np.argmin(day_hr))\n",
        "    return np.mean(daily_nadirs) if daily_nadirs else day_minutes / 2\n",
        "\n",
        "def _sigmoid(x, k=2, x0=0):\n",
        "    return 1 / (1 + np.exp(-k * (x - x0)))\n",
        "\n",
        "def lco_model_ode(t, y, params, light_func, sleep_func):\n",
        "    x, xc, n = y\n",
        "    if not np.all(np.isfinite(y)): return [0,0,0]\n",
        "\n",
        "    mu, q, k, alpha0, I0, p, beta, G, rho, tau_x = params.values()\n",
        "    I, sigma = light_func(t), sleep_func(t)\n",
        "    I = max(I, 0)\n",
        "    alpha = alpha0 * ((I / I0)**p) * (I / (I + 100.0)) if I > 0 else 0\n",
        "    B_hat = G * (1 - n) * alpha\n",
        "    B = B_hat * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "\n",
        "    cbt_min_phase_angle = -170.7 * np.pi / 180.0\n",
        "    current_phase = np.arctan2(xc, x)\n",
        "    phase_diff_rad = (current_phase - cbt_min_phase_angle + np.pi) % (2 * np.pi) - np.pi\n",
        "    psi_c_x = phase_diff_rad * (tau_x / (2 * np.pi)) + (tau_x / 2)\n",
        "    weight_enter = _sigmoid(psi_c_x, k=2, x0=16.5)\n",
        "    weight_exit = 1 - _sigmoid(psi_c_x, k=2, x0=21.0)\n",
        "    wmz_weight = weight_enter * weight_exit * sigma\n",
        "    Ns_hat_normal = rho * (1/3.0 - sigma)\n",
        "    Ns_hat_wmz = rho * (1/3.0)\n",
        "    Ns_hat = Ns_hat_normal * (1 - wmz_weight) + Ns_hat_wmz * wmz_weight\n",
        "    Ns = Ns_hat * (1 - np.tanh(10 * x))\n",
        "\n",
        "    dxdt = (np.pi / 12.0) * (xc + mu * (x/3.0 + (4.0/3.0)*x**3 - (256.0/105.0)*x**7) + B + Ns)\n",
        "    tau_term_sq = (24.0 / (0.99729 * tau_x))**2\n",
        "    dxc_dt = (np.pi / 12.0) * (q * B * xc - x * (tau_term_sq + k * B))\n",
        "    dn_dt = 60.0 * (alpha * (1 - n) - beta * n)\n",
        "    return [dxdt, dxc_dt, dn_dt]\n",
        "\n",
        "def lco_model_jacobian(t, y, params, light_func, sleep_func):\n",
        "    x, xc, n = y\n",
        "    if not np.all(np.isfinite(y)): return np.zeros((3,3))\n",
        "\n",
        "    mu, q, k, alpha0, I0, p, beta, G, rho, tau_x = params.values()\n",
        "    I, sigma = light_func(t), sleep_func(t)\n",
        "    I = max(I, 0)\n",
        "    alpha = alpha0 * ((I / I0)**p) * (I / (I + 100.0)) if I > 0 else 0\n",
        "    cbt_min_phase_angle = -170.7 * np.pi / 180.0\n",
        "    current_phase = np.arctan2(xc, x)\n",
        "    phase_diff_rad = (current_phase - cbt_min_phase_angle + np.pi) % (2 * np.pi) - np.pi\n",
        "    psi_c_x = phase_diff_rad * (tau_x / (2 * np.pi)) + (tau_x / 2)\n",
        "    weight_enter = _sigmoid(psi_c_x, k=2, x0=16.5)\n",
        "    weight_exit = 1 - _sigmoid(psi_c_x, k=2, x0=21.0)\n",
        "    wmz_weight = weight_enter * weight_exit * sigma\n",
        "    Ns_hat_normal = rho * (1/3.0 - sigma)\n",
        "    Ns_hat_wmz = rho * (1/3.0)\n",
        "    Ns_hat = Ns_hat_normal * (1 - wmz_weight) + Ns_hat_wmz * wmz_weight\n",
        "    dB_dx = -0.4 * G * alpha * (1 - n) * (1 - 0.4 * xc)\n",
        "    dB_dxc = -0.4 * G * alpha * (1 - n) * (1 - 0.4 * x)\n",
        "    dB_dn = -G * alpha * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    dNs_dx = -Ns_hat * 10.0 * (1.0 / np.cosh(10 * x))**2\n",
        "    J = np.zeros((3, 3))\n",
        "    J[0, 0] = (np.pi / 12.0) * (mu * (1/3.0 + 4.0 * x**2 - (256.0*7.0/105.0) * x**6) + dB_dx + dNs_dx)\n",
        "    J[0, 1] = (np.pi / 12.0) * (1.0 + dB_dxc)\n",
        "    J[0, 2] = (np.pi / 12.0) * dB_dn\n",
        "    B = G * alpha * (1 - n) * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    tau_term_sq = (24.0 / (0.99729 * tau_x))**2\n",
        "    J[1, 0] = (np.pi / 12.0) * (q * xc * dB_dx - (tau_term_sq + k * B) - k * x * dB_dx)\n",
        "    J[1, 1] = (np.pi / 12.0) * (q * B + q * xc * dB_dxc - k * x * dB_dxc)\n",
        "    J[1, 2] = (np.pi / 12.0) * (q * xc * dB_dn - k * x * dB_dn)\n",
        "    J[2, 2] = 60.0 * (-alpha - beta)\n",
        "    return J\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 딥러닝 모델 정의\n",
        "# =============================================================================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class FourierTrajectoryLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_harmonics, **kwargs):\n",
        "        super(FourierTrajectoryLayer, self).__init__(**kwargs)\n",
        "        self.num_harmonics = num_harmonics\n",
        "        self.output_dim = DAY_MINUTES\n",
        "        self.t = tf.linspace(0.0, 2 * np.pi, self.output_dim)\n",
        "        self.t = tf.cast(self.t, dtype=tf.float32)\n",
        "\n",
        "    def call(self, coeffs):\n",
        "        coeffs_x = coeffs[:, :(1 + 2 * self.num_harmonics)]\n",
        "        coeffs_xc = coeffs[:, (1 + 2 * self.num_harmonics):]\n",
        "        traj_x = self._build_trajectory(coeffs_x)\n",
        "        traj_xc = self._build_trajectory(coeffs_xc)\n",
        "        return tf.stack([traj_x, traj_xc], axis=-1)\n",
        "\n",
        "    def _build_trajectory(self, coeffs):\n",
        "        a0 = coeffs[:, 0:1]\n",
        "        a_n = coeffs[:, 1:self.num_harmonics + 1]\n",
        "        b_n = coeffs[:, self.num_harmonics + 1:]\n",
        "        trajectory = a0\n",
        "        for n in range(1, self.num_harmonics + 1):\n",
        "            trajectory += a_n[:, n-1:n] * tf.cos(n * self.t)\n",
        "            trajectory += b_n[:, n-1:n] * tf.sin(n * self.t)\n",
        "        return trajectory\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(FourierTrajectoryLayer, self).get_config()\n",
        "        config.update({\"num_harmonics\": self.num_harmonics})\n",
        "        return config\n",
        "\n",
        "def build_fourier_correction_model(lookback_minutes, num_harmonics, lstm_units):\n",
        "    num_coeffs_per_traj = 1 + 2 * num_harmonics\n",
        "    output_size = num_coeffs_per_traj * 2\n",
        "\n",
        "    input_lux = Input(shape=(lookback_minutes, 1), name='corr_input_lux')\n",
        "    input_sleep = Input(shape=(lookback_minutes, 1), name='corr_input_sleep')\n",
        "    input_body1 = Input(shape=(lookback_minutes, 3), name='corr_input_body1')\n",
        "    input_body2 = Input(shape=(lookback_minutes, 1), name='corr_input_body2')\n",
        "    input_zeit1 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit1')\n",
        "    input_zeit2 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit2')\n",
        "    input_zeit3 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit3')\n",
        "\n",
        "    def create_feat_extractor(inp, name):\n",
        "        x = Conv1D(16, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn1')(inp)\n",
        "        x = Conv1D(8, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn2')(x)\n",
        "        return x\n",
        "\n",
        "    features = [\n",
        "        create_feat_extractor(input_lux, 'lux'), create_feat_extractor(input_sleep, 'sleep'),\n",
        "        create_feat_extractor(input_body1, 'body1'), create_feat_extractor(input_body2, 'body2'),\n",
        "        create_feat_extractor(input_zeit1, 'zeit1'), create_feat_extractor(input_zeit2, 'zeit2'),\n",
        "        create_feat_extractor(input_zeit3, 'zeit3'),\n",
        "    ]\n",
        "\n",
        "    combined_feature_sequence = Concatenate(axis=-1)(features)\n",
        "    lstm_output = LSTM(lstm_units, return_sequences=False, name='correction_lstm')(combined_feature_sequence)\n",
        "    x = Dropout(0.2)(lstm_output)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    fourier_coeffs = Dense(output_size, activation='linear', name='fourier_coeffs')(x)\n",
        "\n",
        "    model_inputs = [input_lux, input_sleep, input_body1, input_body2, input_zeit1, input_zeit2, input_zeit3]\n",
        "    model = Model(inputs=model_inputs, outputs=fourier_coeffs, name='FourierCorrectionModel')\n",
        "    return model\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\"position\": self.position, \"d_model\": self.d_model})\n",
        "        return config\n",
        "\n",
        "def build_main_feature_extractors(time_steps=INPUT_SEQUENCE_LENGTH, d_model=D_MODEL):\n",
        "    feature_proportions = {\n",
        "        'lco': 0.25, 'lux': 0.125, 'sleep': 0.125, 'body1': 0.25,\n",
        "        'body2': 0.0625, 'zeit1': 0.0625, 'zeit2': 0.0625, 'zeit3': 0.0625,\n",
        "    }\n",
        "    cnn_block_map = { name: max(2, int(d_model * prop) // 2 * 2) for name, prop in feature_proportions.items() }\n",
        "    current_sum = sum(cnn_block_map.values())\n",
        "    if current_sum != d_model: cnn_block_map['lco'] += d_model - current_sum\n",
        "    assert sum(cnn_block_map.values()) == d_model, \"Sum of feature dimensions must equal d_model\"\n",
        "\n",
        "    def create_cnn_block(n_features, name_prefix):\n",
        "        return tf.keras.Sequential([\n",
        "            Conv1D(filters=32, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn1\"),\n",
        "            Conv1D(filters=n_features, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn2\")\n",
        "        ], name=f\"{name_prefix}_cnn_block\")\n",
        "\n",
        "    input_lco = Input(shape=(time_steps, 2), name='input_lco')\n",
        "    features_lco = create_cnn_block(cnn_block_map['lco'], 'lco')(input_lco)\n",
        "    lco_feature_extractor = Model(inputs=input_lco, outputs=features_lco, name='LCOFeatureExtractor')\n",
        "\n",
        "    input_lux = Input(shape=(time_steps, 1), name='input_lux')\n",
        "    input_sleep = Input(shape=(time_steps, 1), name='input_sleep')\n",
        "    input_body1 = Input(shape=(time_steps, 3), name='input_body1')\n",
        "    input_body2 = Input(shape=(time_steps, 1), name='input_body2')\n",
        "    input_zeit1 = Input(shape=(time_steps, 1), name='input_zeit1')\n",
        "    input_zeit2 = Input(shape=(time_steps, 1), name='input_zeit2')\n",
        "    input_zeit3 = Input(shape=(time_steps, 1), name='input_zeit3')\n",
        "    features_lux = create_cnn_block(cnn_block_map['lux'], 'lux')(input_lux)\n",
        "    features_sleep = create_cnn_block(cnn_block_map['sleep'], 'sleep')(input_sleep)\n",
        "    features_body1 = create_cnn_block(cnn_block_map['body1'], 'body1')(input_body1)\n",
        "    features_body2 = create_cnn_block(cnn_block_map['body2'], 'body2')(input_body2)\n",
        "    features_zeit1 = create_cnn_block(cnn_block_map['zeit1'], 'zeit1')(input_zeit1)\n",
        "    features_zeit2 = create_cnn_block(cnn_block_map['zeit2'], 'zeit2')(input_zeit2)\n",
        "    features_zeit3 = create_cnn_block(cnn_block_map['zeit3'], 'zeit3')(input_zeit3)\n",
        "    combined_other_features = Concatenate(axis=-1, name='combined_other_features')([\n",
        "        features_lux, features_sleep, features_body1,\n",
        "        features_body2, features_zeit1, features_zeit2, features_zeit3\n",
        "    ])\n",
        "    other_inputs = [input_lux, input_sleep, input_body1, input_body2, input_zeit1, input_zeit2, input_zeit3]\n",
        "    other_feature_extractor = Model(inputs=other_inputs, outputs=combined_other_features, name='OtherFeatureExtractor')\n",
        "\n",
        "    return lco_feature_extractor, other_feature_extractor\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ContextualTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(ContextualTransformerBlock, self).__init__(**kwargs)\n",
        "        self.d_model, self.num_heads, self.dff, self.rate = d_model, num_heads, dff, rate\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1, self.layernorm2, self.layernorm3 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1, self.dropout2, self.dropout3 = Dropout(rate), Dropout(rate), Dropout(rate)\n",
        "    def call(self, inputs, training=False):\n",
        "        past_info, current_info = inputs\n",
        "        attn_output_current = self.mha1(query=current_info, key=current_info, value=current_info, training=training)\n",
        "        current_info_sa = self.layernorm1(current_info + self.dropout1(attn_output_current, training=training))\n",
        "        attn_output_cross = self.mha2(query=current_info_sa, key=past_info, value=past_info, training=training)\n",
        "        current_info_contextualized = self.layernorm2(current_info_sa + self.dropout2(attn_output_cross, training=training))\n",
        "        ffn_output = self.ffn(current_info_contextualized)\n",
        "        final_output = self.layernorm3(current_info_contextualized + self.dropout3(ffn_output, training=training))\n",
        "        return final_output\n",
        "    def get_config(self):\n",
        "        config = super(ContextualTransformerBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(SelfAttentionBlock, self).__init__(**kwargs)\n",
        "        self.d_model, self.num_heads, self.dff, self.rate = d_model, num_heads, dff, rate\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1, self.layernorm2 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1, self.dropout2 = Dropout(rate), Dropout(rate)\n",
        "    def call(self, x, training=False):\n",
        "        attn_output = self.mha(query=x, key=x, value=x, training=training)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "    def get_config(self):\n",
        "        config = super(SelfAttentionBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class MainPredictor(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, prediction_horizon, rate=0.1, **kwargs):\n",
        "        super(MainPredictor, self).__init__(**kwargs)\n",
        "        self.num_layers, self.d_model, self.num_heads, self.dff, self.prediction_horizon, self.rate = num_layers, d_model, num_heads, dff, prediction_horizon, rate\n",
        "        self.first_block = ContextualTransformerBlock(d_model, num_heads, dff, rate)\n",
        "        self.other_blocks = [SelfAttentionBlock(d_model, num_heads, dff, rate) for _ in range(num_layers - 1)]\n",
        "        self.prediction_head = tf.keras.Sequential([Dense(128, activation='relu', name=\"pred_head_dense1\"), Dropout(rate), Dense(prediction_horizon, name=\"final_prediction\")], name=\"PREDICTION_HEAD\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoded_features, past_info, last_lco = inputs\n",
        "        current_info_length = 2 * DAY_MINUTES\n",
        "        current_info = encoded_features[:, -current_info_length:, :]\n",
        "        x = self.first_block((past_info, current_info), training=training)\n",
        "        for block in self.other_blocks: x = block(x, training=training)\n",
        "        pooled_vector, last_vector = tf.reduce_mean(x, axis=1), x[:, -1, :]\n",
        "        combined_final_vector = tf.concat([pooled_vector, last_vector, last_lco], axis=-1)\n",
        "        predictions = self.prediction_head(combined_final_vector)\n",
        "        return predictions\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"num_layers\": self.num_layers, \"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"prediction_horizon\": self.prediction_horizon, \"rate\": self.rate}\n",
        "    @classmethod\n",
        "    def from_config(cls, config): return cls(**config)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class IntegratedModel(Model):\n",
        "    def __init__(self, lco_feature_extractor, other_feature_extractor, fourier_correction_model, main_predictor, config, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.lco_feature_extractor = lco_feature_extractor\n",
        "        self.other_feature_extractor = other_feature_extractor\n",
        "        self.fourier_correction_model = fourier_correction_model\n",
        "        self.main_predictor = main_predictor\n",
        "        self.config = config\n",
        "        self.fourier_layer = FourierTrajectoryLayer(config['num_harmonics'])\n",
        "        self.pos_encoding_layer = PositionalEncoding(config['input_seq_len'], config['d_model'])\n",
        "        self.lambda_reg, self.lambda_cont, self.lambda_anchor = config['lambda_reg'], config['lambda_cont'], config['lambda_anchor']\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # 1. 입력 데이터 재구성\n",
        "        time_offset = inputs['time_offset']\n",
        "        batch_size = tf.shape(time_offset)[0]\n",
        "        num_main_seq_days = self.config['input_seq_len'] // DAY_MINUTES\n",
        "\n",
        "        # 2. 슬라이딩 윈도우를 이용한 일일 보정 곡선 생성 (tf.while_loop 사용)\n",
        "        loop_len = num_main_seq_days + 1\n",
        "        lookback_minutes = self.config['lookback_days'] * DAY_MINUTES\n",
        "\n",
        "        # while_loop를 위한 초기값 설정\n",
        "        i = tf.constant(0)\n",
        "        correction_curves_ta = tf.TensorArray(tf.float32, size=loop_len, dynamic_size=False, clear_after_read=False, name=\"correction_curves_ta\")\n",
        "\n",
        "        # while_loop의 반복 조건: i < loop_len\n",
        "        def cond(i, ta):\n",
        "            return tf.less(i, loop_len)\n",
        "\n",
        "        # while_loop의 반복 본문\n",
        "        def body(i, ta):\n",
        "            day_start_idx = i * DAY_MINUTES\n",
        "\n",
        "            # 현재 날짜에 대한 lookback 데이터 슬라이싱\n",
        "            lookback_data_list = []\n",
        "            for key in ['corr_input_lux', 'corr_input_sleep', 'corr_input_body1', 'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3']:\n",
        "                lookback_data_list.append(inputs[key][:, day_start_idx : day_start_idx + lookback_minutes, :])\n",
        "\n",
        "            # 푸리에 계수 예측 및 보정 곡선 생성\n",
        "            fourier_coeffs = self.fourier_correction_model(lookback_data_list, training=training)\n",
        "            daily_curve = self.fourier_layer(fourier_coeffs)\n",
        "\n",
        "            # TensorArray에 결과 저장\n",
        "            ta = ta.write(i, daily_curve)\n",
        "            return i + 1, ta\n",
        "\n",
        "        # tf.while_loop 실행\n",
        "        _, final_ta = tf.while_loop(\n",
        "            cond,\n",
        "            body,\n",
        "            loop_vars=[i, correction_curves_ta]\n",
        "        )\n",
        "\n",
        "        # TensorArray를 일반 텐서로 변환\n",
        "        all_daily_curves = final_ta.stack()\n",
        "        all_daily_curves_swapped = tf.transpose(all_daily_curves, [1, 0, 2, 3])\n",
        "\n",
        "        # 3. 보정 곡선 및 모든 입력 데이터 정렬 (tf.gather 사용)\n",
        "        full_correction_block = tf.reshape(all_daily_curves_swapped, [batch_size, -1, 2])\n",
        "        input_seq_len = self.config['input_seq_len']\n",
        "\n",
        "        # 각 샘플의 time_offset에 맞춰 슬라이싱할 인덱스를 한 번에 생성\n",
        "        offsets = tf.cast(time_offset, dtype=tf.int32)\n",
        "        sequence_indices = tf.range(input_seq_len, dtype=tf.int32)[tf.newaxis, :]\n",
        "        time_indices = offsets + sequence_indices # 브로드캐스팅을 통해 (batch_size, input_seq_len) 모양의 인덱스 생성\n",
        "\n",
        "        # tf.gather를 사용해 각 샘플에서 해당 인덱스의 데이터를 효율적으로 추출\n",
        "        final_delta_trajectory = tf.gather(full_correction_block, time_indices, batch_dims=1)\n",
        "        aligned_baseline = tf.gather(inputs['baseline_inputs'], time_indices, batch_dims=1)\n",
        "\n",
        "        other_input_keys = ['input_lux', 'input_sleep', 'input_body1', 'input_body2', 'input_zeit1', 'input_zeit2', 'input_zeit3']\n",
        "        main_cnn_inputs_other_list = [\n",
        "            tf.gather(inputs[key], time_indices, batch_dims=1) for key in other_input_keys\n",
        "        ]\n",
        "\n",
        "        # 4. 최종 궤적 합성 및 특징 추출\n",
        "        corrected_lco_trajectory = aligned_baseline + final_delta_trajectory\n",
        "\n",
        "        lco_features = self.lco_feature_extractor(corrected_lco_trajectory, training=training)\n",
        "        other_features = self.other_feature_extractor(main_cnn_inputs_other_list, training=training)\n",
        "        combined_features = Concatenate(axis=-1)([lco_features, other_features])\n",
        "        encoded_features = self.pos_encoding_layer(combined_features)\n",
        "\n",
        "        # 5. Past Info 구성 및 최종 예측\n",
        "        past_info_indices = inputs['past_info_indices']\n",
        "        batch_indices = tf.tile(tf.range(batch_size, dtype=tf.int64)[:, tf.newaxis], [1, self.config['num_markers_to_keep'] * 3])\n",
        "        gather_indices = tf.stack([tf.reshape(batch_indices, [-1]), tf.reshape(tf.maximum(past_info_indices, 0), [-1])], axis=1)\n",
        "        past_info_flat = tf.gather_nd(encoded_features, gather_indices)\n",
        "        past_info = tf.reshape(past_info_flat, [batch_size, self.config['num_markers_to_keep'] * 3, self.config['d_model']])\n",
        "        past_info *= tf.cast(tf.not_equal(past_info_indices, -1), dtype=tf.float32)[:, :, tf.newaxis]\n",
        "\n",
        "        last_lco_corrected = corrected_lco_trajectory[:, -1, :]\n",
        "        y_pred = self.main_predictor((encoded_features, past_info, last_lco_corrected), training=training)\n",
        "\n",
        "        # 6. 손실 계산\n",
        "        if training:\n",
        "            reg_loss = tf.reduce_mean(tf.square(all_daily_curves)) * self.lambda_reg\n",
        "            self.add_loss(reg_loss)\n",
        "\n",
        "            continuity_gaps = all_daily_curves_swapped[:, :-1, -1, :] - all_daily_curves_swapped[:, 1:, 0, :]\n",
        "            cont_loss = tf.reduce_mean(tf.square(continuity_gaps)) * self.lambda_cont\n",
        "            self.add_loss(cont_loss)\n",
        "\n",
        "            last_day_corrected_x = tf.reshape(corrected_lco_trajectory, [batch_size, num_main_seq_days, DAY_MINUTES, 2])[:, -1, :, 0]\n",
        "            predicted_cbt_nadir_idx = tf.cast(tf.argmin(last_day_corrected_x, axis=1), tf.float32)\n",
        "\n",
        "            full_inputs_for_anchor = inputs['full_inputs_for_anchor']\n",
        "            hr_series = full_inputs_for_anchor[:, 0]; sleep_series = full_inputs_for_anchor[:, 1]\n",
        "            sleep_mask = tf.cast(sleep_series > 0.5, dtype=tf.float32)\n",
        "            has_sleep = tf.reduce_any(tf.cast(sleep_mask, tf.bool), axis=1)\n",
        "            hr_in_sleep = hr_series * sleep_mask + (1.0 - sleep_mask) * 1e9\n",
        "            nadir_in_sleep_indices = tf.cast(tf.argmin(hr_in_sleep, axis=1), tf.float32)\n",
        "            nadir_overall_indices = tf.cast(tf.argmin(hr_series, axis=1), tf.float32)\n",
        "            actual_hr_nadir_idx = tf.where(has_sleep, nadir_in_sleep_indices, nadir_overall_indices)\n",
        "\n",
        "            actual_cbt_nadir_idx = (actual_hr_nadir_idx + 120) % DAY_MINUTES\n",
        "            anchor_loss_val = tf.reduce_mean(tf.square(predicted_cbt_nadir_idx - actual_cbt_nadir_idx)) / (DAY_MINUTES**2)\n",
        "            anchor_loss = anchor_loss_val * self.lambda_anchor\n",
        "            self.add_loss(anchor_loss)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def get_config(self): return {\"config\": self.config}\n",
        "    @classmethod\n",
        "    def from_config(cls, config_data, custom_objects=None): return cls(**config_data)\n",
        "\n",
        "# =============================================================================\n",
        "# 3. TFRecord 생성 및 파싱 (하이브리드 파이프라인)\n",
        "# =============================================================================\n",
        "def moving_average_np(a, n=3):\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# [수정된 함수]\n",
        "# past_info 추출 로직 개선: 경계선 데이터 손실을 최소화하는 새로운 로직 적용\n",
        "def generate_past_info_indices(full_unscaled_df, main_seq_start_in_full_df, config):\n",
        "    \"\"\"\n",
        "    개선된 past_info 인덱스 생성 함수.\n",
        "    탐색 영역과 선택 영역을 분리하여 경계선 데이터의 손실을 최소화.\n",
        "\n",
        "    Args:\n",
        "        full_unscaled_df (pd.DataFrame): lookback + main_seq가 포함된 확장된 원본 데이터.\n",
        "        main_seq_start_in_full_df (int): 확장된 데이터 내에서 main_seq가 시작되는 인덱스.\n",
        "        config (dict): 모델 설정값.\n",
        "\n",
        "    Returns:\n",
        "        np.array: main_seq 기준의 상대적인 past_info 인덱스 배열 (15개).\n",
        "    \"\"\"\n",
        "    num_markers_to_keep = config['num_markers_to_keep']\n",
        "    day_minutes = config['day_minutes']\n",
        "    padding_value = -1\n",
        "\n",
        "    # 1. '선택 영역' 정의: 마커가 최종적으로 선택될 수 있는 원래의 '과거' 5일 구간.\n",
        "    #    인덱스는 full_unscaled_df 기준의 절대 인덱스.\n",
        "    selection_start_abs = main_seq_start_in_full_df\n",
        "    # '과거' 구간은 7일 입력 시퀀스의 앞 5일.\n",
        "    selection_end_abs = main_seq_start_in_full_df + (config['input_seq_len'] - 2 * day_minutes)\n",
        "\n",
        "    # 2. '탐색 영역' 정의: 생체 마커 계산을 위해 사용되는 전체 데이터.\n",
        "    #    경계 효과를 최소화하기 위해 전달받은 full_unscaled_df 전체를 사용.\n",
        "    calculation_df = full_unscaled_df.assign(\n",
        "        corrected_skin_temp=full_unscaled_df['skin_temp'] - full_unscaled_df['ambient_temp']\n",
        "    )\n",
        "    heart_rate = calculation_df['heart_rate'].values\n",
        "    is_sleeping = calculation_df['is_sleeping'].values\n",
        "    temp_series = calculation_df['corrected_skin_temp'].values\n",
        "\n",
        "    # 3. 전체 '탐색 영역'에서 모든 잠재적 마커 후보 탐색\n",
        "    # Nadir 마커 (수면 중 심박수 최저점)\n",
        "    hr_in_sleep = np.where(is_sleeping > 0.5, heart_rate, np.inf)\n",
        "    # 1시간 단위로 최저점을 찾아 여러 후보 확보\n",
        "    nadir_candidates_abs = [\n",
        "        i + np.argmin(hr_in_sleep[i:i+60])\n",
        "        for i in range(0, len(calculation_df) - 60, 60)\n",
        "        if np.any(np.isfinite(hr_in_sleep[i:i+60]))\n",
        "    ]\n",
        "    nadir_indices_abs = list(set(nadir_candidates_abs))\n",
        "\n",
        "    # Onset/Offset 마커 (피부 온도 변화율 기반)\n",
        "    if len(temp_series) > 40:\n",
        "        # 이동 평균을 적용하여 노이즈 감소 후 변화율 계산\n",
        "        temp_deriv_smoothed = moving_average_np(np.gradient(moving_average_np(temp_series, 30)), 10)\n",
        "        offset_to_align = len(temp_series) - len(temp_deriv_smoothed)\n",
        "        onset_indices_abs = list(np.where(temp_deriv_smoothed > 0.001)[0] + offset_to_align)\n",
        "        offset_indices_abs = list(np.where(temp_deriv_smoothed < -0.001)[0] + offset_to_align)\n",
        "    else:\n",
        "        onset_indices_abs, offset_indices_abs = [], []\n",
        "\n",
        "    # 4. 마커 필터링 및 선택\n",
        "    #   - 1단계: '선택 영역' 내에 있는 마커만 필터링\n",
        "    #   - 2단계: 최신순으로 정렬하여 상위 N개 선택\n",
        "    final_indices_relative = []\n",
        "    all_marker_candidates = [onset_indices_abs, nadir_indices_abs, offset_indices_abs]\n",
        "\n",
        "    for marker_candidates in all_marker_candidates:\n",
        "        # '선택 영역' (원래의 5일) 내에 있는 유효한 후보들만 필터링\n",
        "        valid_candidates = [\n",
        "            idx for idx in marker_candidates\n",
        "            if selection_start_abs <= idx < selection_end_abs\n",
        "        ]\n",
        "\n",
        "        if len(valid_candidates) > 0:\n",
        "            # '선택 영역'의 끝을 기준으로 가장 최신 마커 N개 선택\n",
        "            top_indices_abs = sorted(valid_candidates, key=lambda idx: selection_end_abs - idx)[:num_markers_to_keep]\n",
        "            # 절대 인덱스를 main_seq 기준의 상대 인덱스로 변환\n",
        "            top_indices_relative = [idx - main_seq_start_in_full_df for idx in top_indices_abs]\n",
        "            final_indices_relative.extend(top_indices_relative)\n",
        "            # 필요 시 패딩 추가\n",
        "            if len(top_indices_relative) < num_markers_to_keep:\n",
        "                final_indices_relative.extend([padding_value] * (num_markers_to_keep - len(top_indices_relative)))\n",
        "        else:\n",
        "            # 유효한 마커가 없으면 패딩으로 채움\n",
        "            final_indices_relative.extend([padding_value] * num_markers_to_keep)\n",
        "\n",
        "    return np.array(final_indices_relative, dtype=np.int64)\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "def serialize_example(inputs_dict):\n",
        "    feature = {key: _bytes_feature(tf.io.serialize_tensor(value).numpy()) for key, value in inputs_dict.items()}\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "\n",
        "def create_tfrecords(df_unscaled, df_scaled, indices, config, file_path):\n",
        "    print(f\"--- TFRecord 파일 생성 시작: {file_path} ---\")\n",
        "\n",
        "    main_cnn_other_cols = { 'input_lux': ['lux'], 'input_sleep': ['is_sleeping'], 'input_body1': ['heart_rate', 'hrv', 'respiration_rate'], 'input_body2': ['skin_temp'], 'input_zeit1': ['meal_event'], 'input_zeit2': ['exercise_event'], 'input_zeit3': ['ambient_temp'], }\n",
        "    fourier_corr_cols = { 'corr_input_lux': ['lux'], 'corr_input_sleep': ['is_sleeping'], 'corr_input_body1': ['heart_rate', 'hrv', 'respiration_rate'], 'corr_input_body2': ['skin_temp'], 'corr_input_zeit1': ['meal_event'], 'corr_input_zeit2': ['exercise_event'], 'corr_input_zeit3': ['ambient_temp'], }\n",
        "    anchor_loss_cols = ['heart_rate', 'is_sleeping']\n",
        "\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "\n",
        "    with tf.io.TFRecordWriter(file_path) as writer:\n",
        "        for main_seq_start_idx in tqdm(indices, desc=f\"{os.path.basename(file_path)} 생성 중\"):\n",
        "            # 1. 동적 버퍼링: 자정 기준으로 데이터 정렬\n",
        "            time_offset = main_seq_start_idx % DAY_MINUTES\n",
        "\n",
        "            # 자정 기준의 전체 데이터 슬라이스 시작/종료 인덱스 계산\n",
        "            slice_start_midnight = main_seq_start_idx - time_offset\n",
        "            total_days_needed = config['lookback_days'] + num_main_seq_days\n",
        "            slice_end_midnight = slice_start_midnight + total_days_needed * DAY_MINUTES\n",
        "\n",
        "            # 예측에 필요한 추가 데이터 길이\n",
        "            final_slice_end = main_seq_start_idx + config['input_seq_len'] + config['pred_horizon']\n",
        "\n",
        "            if slice_end_midnight > len(df_unscaled) or final_slice_end > len(df_unscaled):\n",
        "                continue\n",
        "\n",
        "            # [수정] past_info 계산을 위해 더 긴 unscaled slice를 사용\n",
        "            df_slice_unscaled = df_unscaled.iloc[slice_start_midnight:final_slice_end]\n",
        "            df_slice_scaled = df_scaled.iloc[slice_start_midnight:slice_end_midnight]\n",
        "            len_past_info_calc = slice_end_midnight - slice_start_midnight\n",
        "            past_info_calc_df = df_slice_unscaled.iloc[:len_past_info_calc]\n",
        "\n",
        "            # 2. 데이터 준비\n",
        "            example_dict = {}\n",
        "\n",
        "            # 보정 모델용 (자정 기준, lookback + main_seq 길이)\n",
        "            for key, cols in fourier_corr_cols.items():\n",
        "                example_dict[key] = tf.constant(df_slice_scaled[cols].values, dtype=tf.float32)\n",
        "\n",
        "            # 메인 모델용 (자정 기준, lookback + main_seq 길이)\n",
        "            for key, cols in main_cnn_other_cols.items():\n",
        "                example_dict[key] = tf.constant(df_slice_scaled[cols].values, dtype=tf.float32)\n",
        "\n",
        "            example_dict['baseline_inputs'] = tf.constant(df_slice_scaled[['x_base', 'xc_base']].values, dtype=tf.float32)\n",
        "\n",
        "            # 시간 오프셋 저장\n",
        "            example_dict['time_offset'] = tf.constant([time_offset], dtype=tf.int32)\n",
        "\n",
        "            # 앵커 손실용 (원래 시간축 기준 마지막 날)\n",
        "            anchor_start = main_seq_start_idx + config['input_seq_len'] - DAY_MINUTES\n",
        "            anchor_end = main_seq_start_idx + config['input_seq_len']\n",
        "            example_dict['full_inputs_for_anchor'] = tf.constant(df_unscaled.iloc[anchor_start:anchor_end][anchor_loss_cols].values, dtype=tf.float32)\n",
        "\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "            # [수정된 호출] 개선된 함수를 사용하여 past_info 인덱스 생성\n",
        "            # main_seq_start_idx가 아닌, 자정 기준 slice 내에서의 시작점(time_offset)을 전달\n",
        "            example_dict['past_info_indices'] = generate_past_info_indices(\n",
        "                full_unscaled_df=past_info_calc_df,\n",
        "                main_seq_start_in_full_df=time_offset,\n",
        "                config=config\n",
        "            )\n",
        "            # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "            # 정답 (원래 시간축 기준)\n",
        "            label_start = main_seq_start_idx + config['input_seq_len']\n",
        "            label_end = label_start + config['pred_horizon']\n",
        "            example_dict['label'] = tf.constant(df_unscaled.iloc[label_start:label_end]['heart_rate'].values, dtype=tf.float32)\n",
        "\n",
        "            writer.write(serialize_example(example_dict))\n",
        "    print(f\"--- TFRecord 파일 생성 완료: {file_path} ---\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 데이터 생성 함수 (현실적 자극 모델링)\n",
        "# =============================================================================\n",
        "def generate_realistic_biometric_data(output_dir, duration_days, person_profile):\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "    total_minutes = duration_days * DAY_MINUTES\n",
        "    timestamps = pd.to_datetime('2025-07-01') + pd.to_timedelta(np.arange(total_minutes), 'm')\n",
        "    df = pd.DataFrame(index=timestamps)\n",
        "\n",
        "    event_cols = ['is_sleeping', 'meal_event', 'exercise_event', 'stress_event']\n",
        "    for col in event_cols: df[col] = 0.0\n",
        "    df['lux'] = 0.0\n",
        "\n",
        "    for day in range(duration_days):\n",
        "        day_start_idx, day_end_idx = day * DAY_MINUTES, (day + 1) * DAY_MINUTES\n",
        "        daily_peak_lux = np.random.uniform(1000, 2500)\n",
        "        minute_of_day = np.arange(DAY_MINUTES)\n",
        "        daylight_start_min, daylight_end_min = 6 * 60, 20 * 60\n",
        "        daylight_duration = daylight_end_min - daylight_start_min\n",
        "        daylight_minutes = (minute_of_day - daylight_start_min)\n",
        "        daylight_intensity = daily_peak_lux * np.sin(np.pi * daylight_minutes / daylight_duration)\n",
        "        night_light = np.random.uniform(5, 20, DAY_MINUTES)\n",
        "        is_daytime = (minute_of_day >= daylight_start_min) & (minute_of_day <= daylight_end_min)\n",
        "        daily_lux = np.where(is_daytime, np.maximum(night_light, daylight_intensity), night_light)\n",
        "        df.iloc[day_start_idx:day_end_idx, df.columns.get_loc('lux')] = daily_lux.clip(min=1)\n",
        "\n",
        "    for day in range(duration_days):\n",
        "        day_start_idx, current_day_ts = day * DAY_MINUTES, timestamps[day * DAY_MINUTES]\n",
        "        is_weekend = current_day_ts.dayofweek >= 5\n",
        "\n",
        "        if np.random.rand() < 0.5:\n",
        "            activity_start = day_start_idx + np.random.randint(9 * 60, 17 * 60)\n",
        "            activity_duration = np.random.randint(60, 180)\n",
        "            activity_end = min(activity_start + activity_duration, total_minutes)\n",
        "            df.iloc[activity_start:activity_end, df.columns.get_loc('lux')] += np.random.uniform(200, 500)\n",
        "\n",
        "        sunrise_effect = np.random.normal(0, 0.25)\n",
        "        wake_up_hour = np.random.normal(person_profile[f'wake_up_{\"weekend\" if is_weekend else \"weekday\"}_mean'],\n",
        "                                        person_profile[f'wake_up_{\"weekend\" if is_weekend else \"weekday\"}_std']) - sunrise_effect\n",
        "        sleep_duration = np.random.normal(person_profile['sleep_duration_mean'], 1.0 if is_weekend else 0.5)\n",
        "        wake_up_minute = int(np.clip(wake_up_hour * 60, 0, DAY_MINUTES - 1))\n",
        "        sleep_end_abs = day_start_idx + wake_up_minute\n",
        "        sleep_start_abs = sleep_end_abs - int(sleep_duration * 60)\n",
        "        df.iloc[max(0, sleep_start_abs) : min(total_minutes, sleep_end_abs), df.columns.get_loc('is_sleeping')] = 1\n",
        "\n",
        "        meal_times = [wake_up_minute + np.random.randint(90, 180), int(np.random.normal(19, 1.0) * 60)] if is_weekend else [wake_up_minute + np.random.randint(30, 60), int(np.random.normal(12.5, 0.5) * 60), int(np.random.normal(19.5, 0.75) * 60)]\n",
        "        for meal_time in meal_times:\n",
        "            event_start = day_start_idx + meal_time\n",
        "            if event_start < total_minutes: df.iloc[event_start : min(event_start + 60, total_minutes), df.columns.get_loc('meal_event')] = 1\n",
        "\n",
        "        if np.random.rand() < person_profile['exercise_freq_prob']:\n",
        "            exercise_start_time = int(np.random.normal(18.5, 1.0) * 60)\n",
        "            event_start = day_start_idx + exercise_start_time\n",
        "            if event_start < total_minutes: df.iloc[event_start : min(event_start + 60, total_minutes), df.columns.get_loc('exercise_event')] = 1\n",
        "\n",
        "    minute_of_day_series = df.index.hour * 60 + df.index.minute\n",
        "    df['heart_rate'] = 65 - 10 * df['is_sleeping'] - 8 * np.cos(2 * np.pi * (minute_of_day_series - 12*60) / DAY_MINUTES)\n",
        "    df['hrv'] = 55 + 25 * df['is_sleeping'] + 10 * np.cos(2 * np.pi * (minute_of_day_series - 4*60) / DAY_MINUTES)\n",
        "    df['respiration_rate'] = 16 - 3 * df['is_sleeping']\n",
        "    df['skin_temp'] = 33.5 + 1.5 * np.sin(2 * np.pi * (minute_of_day_series - 20*60) / DAY_MINUTES)\n",
        "\n",
        "    for event_type, event_col in [('meal', 'meal_event'), ('exercise', 'exercise_event')]:\n",
        "        for start_idx in np.where(df[event_col].diff() == 1)[0]:\n",
        "            duration = 60\n",
        "            end_idx = start_idx + duration\n",
        "            hr_peak, hrv_dip, resp_peak, peak_time, decay_rate = (8, -10, 2, start_idx + 45, 40) if event_type == 'meal' else (45, -30, 8, start_idx + 15, 30)\n",
        "            time_indices = np.arange(start_idx, end_idx + 3*decay_rate)\n",
        "            time_indices = time_indices[time_indices < total_minutes]\n",
        "            effect = np.exp(-0.5 * ((time_indices - peak_time) / decay_rate)**2)\n",
        "            df.iloc[time_indices, df.columns.get_loc('heart_rate')] += hr_peak * effect\n",
        "            df.iloc[time_indices, df.columns.get_loc('hrv')] += hrv_dip * effect\n",
        "            df.iloc[time_indices, df.columns.get_loc('respiration_rate')] += resp_peak * effect\n",
        "\n",
        "    blackout_prob = 0.3\n",
        "    is_blackout_night = (df['is_sleeping'] == 1) & (np.random.rand() < blackout_prob)\n",
        "    light_penetration = np.where(is_blackout_night, 0.001, 0.05)\n",
        "    df['perceived_lux'] = df['lux'] * np.where(df['is_sleeping'] == 1, light_penetration, 1.0)\n",
        "    df['integrated_lux'] = df['perceived_lux'].rolling(window=30, min_periods=1).mean()\n",
        "    df['ambient_temp'] = 22 + 5 * np.sin(2 * np.pi * (minute_of_day_series - 15*60) / DAY_MINUTES)\n",
        "\n",
        "    for col, std in [('heart_rate', 1.5), ('hrv', 4), ('respiration_rate', 0.75), ('skin_temp', 0.15)]:\n",
        "        df[col] += np.random.normal(0, std, total_minutes)\n",
        "\n",
        "    df.index.name = 'timestamp'\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# 4-1. 안정적인 기준 궤도 생성 함수\n",
        "# =============================================================================\n",
        "def get_stable_limit_cycle(params, output_dir, force_recalculate=False):\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "    cache_path = os.path.join(output_dir, \"stable_limit_cycle.npy\")\n",
        "    if os.path.exists(cache_path) and not force_recalculate: return np.load(cache_path)\n",
        "\n",
        "    print(\"안정 궤도를 새로 계산합니다 (최초 1회 실행)...\")\n",
        "    standard_sleep, standard_light = np.zeros(DAY_MINUTES), np.zeros(DAY_MINUTES)\n",
        "    standard_sleep[0:8*60] = 1\n",
        "    standard_light[8*60:10*60], standard_light[10*60:18*60], standard_light[18*60:22*60], standard_light[22*60:24*60] = 150, 400, 150, 50\n",
        "    light_func_spin_up = lambda t: standard_light[int((t % 24) * 60)]\n",
        "    sleep_func_spin_up = lambda t: standard_sleep[int((t % 24) * 60)]\n",
        "    sol_spin_up = solve_ivp(fun=lco_model_ode, t_span=[0, 10 * 24], y0=[1.0, 0.0, 0.5], method='BDF', args=(params, light_func_spin_up, sleep_func_spin_up), dense_output=True, rtol=1e-6, atol=1e-9)\n",
        "    if not sol_spin_up.success: raise RuntimeError(f\"안정 궤도 계산 실패: {sol_spin_up.message}\")\n",
        "\n",
        "    limit_cycle_map = sol_spin_up.sol(np.arange(9 * 24, 10 * 24, 1.0/60.0)).T\n",
        "    np.save(cache_path, limit_cycle_map)\n",
        "    print(f\"새로운 안정 궤도를 저장했습니다: {cache_path}\")\n",
        "    return limit_cycle_map\n",
        "\n",
        "def generate_daily_baseline_trajectories(df, params, output_dir):\n",
        "    print(\"--- [1단계] 일일 재조정 기준 궤도 생성 시작 ---\")\n",
        "    total_minutes, num_days = len(df), len(df) // DAY_MINUTES\n",
        "    limit_cycle_map = get_stable_limit_cycle(params, output_dir)\n",
        "    map_phases = np.arctan2(limit_cycle_map[:, 1], limit_cycle_map[:, 0])\n",
        "    final_trajectory = np.zeros((total_minutes, 3))\n",
        "    theoretical_anchor_minute, last_known_anchor_minute = 4 * 60, -1\n",
        "    t_eval_day_hours = np.arange(DAY_MINUTES) / 60.0\n",
        "\n",
        "    for day in tqdm(range(num_days), desc=\"일일 기준 궤도 생성 중\"):\n",
        "        day_start_idx, day_end_idx = day * DAY_MINUTES, (day + 1) * DAY_MINUTES\n",
        "        day_df = df.iloc[day_start_idx:day_end_idx]\n",
        "        day_hr, day_sleep = day_df['heart_rate'].values, day_df['is_sleeping'].values\n",
        "        sleep_indices = np.where(day_sleep > 0.5)[0]\n",
        "\n",
        "        current_anchor_minute = sleep_indices[np.argmin(day_hr[sleep_indices])] if len(sleep_indices) > 0 else -1\n",
        "        if current_anchor_minute != -1: last_known_anchor_minute = current_anchor_minute\n",
        "        anchor_to_use = last_known_anchor_minute if last_known_anchor_minute != -1 else theoretical_anchor_minute\n",
        "\n",
        "        cbt_nadir_minute_in_day = (anchor_to_use + 120) % DAY_MINUTES\n",
        "        initial_phase_at_midnight = (-170.7 * np.pi / 180.0) - (cbt_nadir_minute_in_day * (2 * np.pi) / DAY_MINUTES)\n",
        "        initial_phase_at_midnight = (initial_phase_at_midnight + np.pi) % (2 * np.pi) - np.pi\n",
        "        y0 = limit_cycle_map[np.argmin(np.abs((map_phases - initial_phase_at_midnight + np.pi) % (2 * np.pi) - np.pi))]\n",
        "\n",
        "        day_df_smoothed = pd.DataFrame(index=day_df.index)\n",
        "        day_df_smoothed['lux_smoothed'] = day_df['integrated_lux'].rolling(window=15, min_periods=1, center=True).mean()\n",
        "        day_df_smoothed['sleep_smoothed'] = day_df['is_sleeping'].rolling(window=15, min_periods=1, center=True).mean()\n",
        "        light_func = interp1d(t_eval_day_hours, day_df_smoothed['lux_smoothed'].values, kind='linear', fill_value=\"extrapolate\")\n",
        "        sleep_func = interp1d(t_eval_day_hours, day_df_smoothed['sleep_smoothed'].values, kind='linear', fill_value=\"extrapolate\")\n",
        "\n",
        "        sol_day = solve_ivp(fun=lco_model_ode, t_span=[0, 24], y0=y0, method='BDF', jac=lco_model_jacobian, args=(params, light_func, sleep_func), dense_output=True, t_eval=t_eval_day_hours, rtol=1e-5, atol=1e-8)\n",
        "\n",
        "        if sol_day.success and sol_day.y.shape[1] == DAY_MINUTES:\n",
        "            final_trajectory[day_start_idx:day_end_idx, :] = sol_day.y.T\n",
        "        elif day > 0:\n",
        "            print(f\"경고: Day {day+1} 시뮬레이션 실패. 이전 날 궤도를 복사합니다.\")\n",
        "            final_trajectory[day_start_idx:day_end_idx, :] = final_trajectory[(day-1)*DAY_MINUTES:day*DAY_MINUTES, :]\n",
        "\n",
        "    if total_minutes > num_days * DAY_MINUTES and num_days > 0:\n",
        "        remaining_start, remaining_len = num_days * DAY_MINUTES, total_minutes - num_days * DAY_MINUTES\n",
        "        final_trajectory[remaining_start:, :] = final_trajectory[(num_days-1)*DAY_MINUTES : (num_days-1)*DAY_MINUTES+remaining_len, :]\n",
        "\n",
        "    print(\"--- [1단계] 일일 재조정 기준 궤도 생성 완료 ---\")\n",
        "    return final_trajectory\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 시각화 함수 (상세 리포트 기능 추가)\n",
        "# =============================================================================\n",
        "def find_all_markers(df_unscaled):\n",
        "    print(\"--- 전체 데이터에 대한 생체 마커 탐색 시작 ---\")\n",
        "    df = df_unscaled.copy()\n",
        "    df['corrected_skin_temp'] = df['skin_temp'] - df['ambient_temp']\n",
        "    all_markers = {'onset': [], 'nadir': [], 'offset': []}\n",
        "    num_days = len(df) // DAY_MINUTES\n",
        "    for day in range(num_days):\n",
        "        day_start, day_end = day * DAY_MINUTES, (day + 1) * DAY_MINUTES\n",
        "        day_df = df.iloc[day_start:day_end]\n",
        "        sleep_hr = day_df['heart_rate'][day_df['is_sleeping'] > 0.5]\n",
        "        if not sleep_hr.empty: all_markers['nadir'].append(sleep_hr.idxmin())\n",
        "        temp_series = day_df['corrected_skin_temp'].values\n",
        "        if len(temp_series) > 40:\n",
        "            temp_deriv_smoothed = moving_average_np(np.gradient(moving_average_np(temp_series, 30)), 10)\n",
        "            offset_to_align = len(temp_series) - len(temp_deriv_smoothed)\n",
        "            onset_candidates, offset_candidates = np.where(temp_deriv_smoothed > 0.001)[0] + offset_to_align, np.where(temp_deriv_smoothed < -0.001)[0] + offset_to_align\n",
        "            if len(onset_candidates) > 0: all_markers['onset'].append(day_df.index[onset_candidates[0]])\n",
        "            if len(offset_candidates) > 0: all_markers['offset'].append(day_df.index[offset_candidates[-1]])\n",
        "    print(\"--- 생체 마커 탐색 완료 ---\")\n",
        "    return all_markers\n",
        "\n",
        "def plot_full_data_report(df_raw, all_markers, output_dir):\n",
        "    print(\"--- 전체 데이터 상세 리포트 생성 시작 ---\")\n",
        "    total_days, days_per_chunk = len(df_raw) // DAY_MINUTES, 3\n",
        "    num_chunks = math.ceil(total_days / days_per_chunk)\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_day, end_day = i * days_per_chunk, min((i + 1) * days_per_chunk, total_days)\n",
        "        chunk_df = df_raw.iloc[start_day * DAY_MINUTES : end_day * DAY_MINUTES]\n",
        "        if chunk_df.empty: continue\n",
        "\n",
        "        fig, axs = plt.subplots(4, 1, figsize=(20, 22), sharex=True)\n",
        "        fig.suptitle(f'Biometric Data Report (Days {start_day + 1}-{end_day})', fontsize=20, y=0.95)\n",
        "\n",
        "        ax1, ax1_twin = axs[0], axs[0].twinx()\n",
        "        ax1.set_title('Heart Rate, Skin Temperature & Sleep', fontsize=14)\n",
        "        ax1.set_ylabel('Heart Rate (bpm)', color='tab:red'); ax1.plot(chunk_df.index, chunk_df['heart_rate'], color='tab:red', label='Heart Rate', zorder=10); ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "        ax1_twin.set_ylabel('Skin Temp (°C)', color='tab:purple'); ax1_twin.plot(chunk_df.index, chunk_df['skin_temp'], color='tab:purple', label='Skin Temp', zorder=10); ax1_twin.tick_params(axis='y', labelcolor='tab:purple')\n",
        "        ax1.fill_between(chunk_df.index, ax1.get_ylim()[0], ax1.get_ylim()[1], where=chunk_df['is_sleeping'] > 0.5, color='gray', alpha=0.2, label='Sleep', zorder=0)\n",
        "\n",
        "        ax2, ax2_twin = axs[1], axs[1].twinx()\n",
        "        ax2.set_title('HRV, Respiration Rate & Exercise', fontsize=14)\n",
        "        ax2.set_ylabel('HRV (ms)', color='tab:green'); ax2.plot(chunk_df.index, chunk_df['hrv'], color='tab:green', label='HRV', zorder=10); ax2.tick_params(axis='y', labelcolor='tab:green')\n",
        "        ax2_twin.set_ylabel('Respiration (brpm)', color='tab:brown'); ax2_twin.plot(chunk_df.index, chunk_df['respiration_rate'], color='tab:brown', label='Respiration Rate', zorder=10); ax2_twin.tick_params(axis='y', labelcolor='tab:brown')\n",
        "        ax2.fill_between(chunk_df.index, ax2.get_ylim()[0], ax2.get_ylim()[1], where=chunk_df['exercise_event'] > 0.5, color='orange', alpha=0.3, label='Exercise', zorder=0)\n",
        "\n",
        "        ax3 = axs[2]; ax3.set_title('Light Exposure', fontsize=14); ax3.set_ylabel('Lux'); ax3.plot(chunk_df.index, chunk_df['lux'], color='orange', label='Lux'); ax3.set_yscale('log'); ax3.set_ylim(bottom=1)\n",
        "        ax4 = axs[3]; ax4.set_title('Meal Events', fontsize=14); ax4.set_ylabel('Meal Event'); ax4.fill_between(chunk_df.index, 0, 1, where=chunk_df['meal_event'] > 0.5, color='skyblue', alpha=0.5, label='Meal'); ax4.set_yticks([0, 1])\n",
        "\n",
        "        marker_colors = {'onset': 'green', 'nadir': 'blue', 'offset': 'purple'}\n",
        "        for ax in axs:\n",
        "            ax.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
        "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d\\n%H:%M')); ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
        "            for marker_type, timestamps in all_markers.items():\n",
        "                for ts in timestamps:\n",
        "                    if chunk_df.index[0] <= ts < chunk_df.index[-1]: ax.axvline(x=ts, color=marker_colors[marker_type], linestyle='--', linewidth=1.5, label=f'_{marker_type}')\n",
        "\n",
        "        lines1, labels1 = ax1.get_legend_handles_labels(); lines1_t, labels1_t = ax1_twin.get_legend_handles_labels(); ax1.legend(lines1 + lines1_t, labels1 + labels1_t, loc='upper left')\n",
        "        lines2, labels2 = ax2.get_legend_handles_labels(); lines2_t, labels2_t = ax2_twin.get_legend_handles_labels(); ax2.legend(lines2 + lines2_t, labels2 + labels2_t, loc='upper left')\n",
        "        ax3.legend(loc='upper left'); ax4.legend(loc='upper left')\n",
        "\n",
        "        from matplotlib.lines import Line2D\n",
        "        legend_elements = [Line2D([0], [0], color='green', lw=2, linestyle='--', label='Onset'), Line2D([0], [0], color='blue', lw=2, linestyle='--', label='Nadir'), Line2D([0], [0], color='purple', lw=2, linestyle='--', label='Offset')]\n",
        "        fig.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        save_path = os.path.join(output_dir, f'report_days_{start_day+1}-{end_day}.png')\n",
        "        plt.savefig(save_path); plt.close(fig)\n",
        "        # print(f\"상세 리포트 그래프가 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 학습, 평가, 예측 파이프라인\n",
        "# =============================================================================\n",
        "def train_and_evaluate(train_tfrecord, val_tfrecord, num_train_samples, num_val_samples, config):\n",
        "    print(\"\\n--- 최종 모델 학습 및 검증 시작 ---\")\n",
        "\n",
        "    main_cnn_other_keys = [ 'input_lux', 'input_sleep', 'input_body1', 'input_body2', 'input_zeit1', 'input_zeit2', 'input_zeit3', ]\n",
        "    fourier_corr_keys = [ 'corr_input_lux', 'corr_input_sleep', 'corr_input_body1', 'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3', ]\n",
        "    other_keys = ['baseline_inputs', 'full_inputs_for_anchor', 'past_info_indices', 'label', 'time_offset']\n",
        "    feature_spec = {key: tf.io.FixedLenFeature([], tf.string) for key in main_cnn_other_keys + fourier_corr_keys + other_keys}\n",
        "\n",
        "    def _parse_function(example_proto):\n",
        "        parsed = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "\n",
        "        inputs = {}\n",
        "        for key in main_cnn_other_keys: inputs[key] = tf.io.parse_tensor(parsed[key], out_type=tf.float32)\n",
        "        for key in fourier_corr_keys: inputs[key] = tf.io.parse_tensor(parsed[key], out_type=tf.float32)\n",
        "\n",
        "        inputs['baseline_inputs'] = tf.io.parse_tensor(parsed['baseline_inputs'], out_type=tf.float32)\n",
        "        inputs['past_info_indices'] = tf.io.parse_tensor(parsed['past_info_indices'], out_type=tf.int64)\n",
        "        inputs['full_inputs_for_anchor'] = tf.io.parse_tensor(parsed['full_inputs_for_anchor'], out_type=tf.float32)\n",
        "        inputs['time_offset'] = tf.io.parse_tensor(parsed['time_offset'], out_type=tf.int32)\n",
        "\n",
        "        label = tf.io.parse_tensor(parsed['label'], out_type=tf.float32)\n",
        "        return inputs, label\n",
        "\n",
        "    def make_dataset(file_path):\n",
        "        return tf.data.TFRecordDataset(file_path, num_parallel_reads=tf.data.AUTOTUNE).map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE).shuffle(256).batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    train_dataset, val_dataset = make_dataset(train_tfrecord), make_dataset(val_tfrecord)\n",
        "\n",
        "    lco_feature_extractor, other_feature_extractor = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "    fourier_correction_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "    main_predictor_config = {k: v for k, v in config.items() if k in ['num_layers', 'd_model', 'num_heads', 'dff', 'rate']}\n",
        "    main_predictor = MainPredictor(prediction_horizon=config['pred_horizon'], **main_predictor_config)\n",
        "    integrated_model = IntegratedModel(lco_feature_extractor, other_feature_extractor, fourier_correction_model, main_predictor, config)\n",
        "\n",
        "    target_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'target_scaler.gz'))\n",
        "    target_scaler_mean, target_scaler_scale = tf.constant(target_scaler.mean_[0], dtype=tf.float32), tf.constant(target_scaler.scale_[0], dtype=tf.float32)\n",
        "    mse_loss_fn, optimizer = tf.keras.losses.MeanSquaredError(), Adam(learning_rate=config['learning_rate'])\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = integrated_model(x, training=True)\n",
        "            main_loss = mse_loss_fn((y - target_scaler_mean) / target_scaler_scale, y_pred)\n",
        "            total_loss = main_loss + sum(integrated_model.losses)\n",
        "        grads = tape.gradient(total_loss, integrated_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(tf.clip_by_global_norm(grads, 1.0)[0], integrated_model.trainable_variables))\n",
        "        return total_loss\n",
        "\n",
        "    @tf.function\n",
        "    def val_step(x, y):\n",
        "        y_pred = integrated_model(x, training=False)\n",
        "        return mse_loss_fn((y - target_scaler_mean) / target_scaler_scale, y_pred)\n",
        "\n",
        "    history, best_val_loss = {'train_loss': [], 'val_loss': []}, float('inf')\n",
        "    model_save_path = os.path.join(OUTPUT_DIR, \"best_model.weights.h5\")\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "        progbar = tf.keras.utils.Progbar(num_train_samples // config['batch_size'], stateful_metrics=['train_loss'])\n",
        "        epoch_train_losses = []\n",
        "        for i, (x, y) in enumerate(train_dataset):\n",
        "            loss = train_step(x, y)\n",
        "            progbar.update(i + 1, values=[('train_loss', loss)])\n",
        "            epoch_train_losses.append(loss)\n",
        "        history['train_loss'].append(np.mean(epoch_train_losses))\n",
        "\n",
        "        val_losses = [val_step(x, y) for x, y in val_dataset]\n",
        "        val_loss = tf.reduce_mean(val_losses)\n",
        "        history['val_loss'].append(val_loss.numpy())\n",
        "        print(f\"\\nValidation Loss (scaled): {val_loss.numpy():.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            print(f\"Validation loss improved. Saving model weights to {model_save_path}\")\n",
        "            best_val_loss = val_loss\n",
        "            integrated_model.save_weights(model_save_path)\n",
        "\n",
        "    print(\"\\n--- 학습 및 검증 완료 ---\")\n",
        "    return integrated_model, val_dataset, history\n",
        "\n",
        "def predict_future_hr(model, data_slice_unscaled, data_slice_scaled, config):\n",
        "    print(\"\\n--- 미래 심박수 예측 시작 ---\")\n",
        "\n",
        "    main_cnn_other_cols = { 'input_lux': ['lux'], 'input_sleep': ['is_sleeping'], 'input_body1': ['heart_rate', 'hrv', 'respiration_rate'], 'input_body2': ['skin_temp'], 'input_zeit1': ['meal_event'], 'input_zeit2': ['exercise_event'], 'input_zeit3': ['ambient_temp'], }\n",
        "    fourier_corr_cols = { 'corr_input_lux': ['lux'], 'corr_input_sleep': ['is_sleeping'], 'corr_input_body1': ['heart_rate', 'hrv', 'respiration_rate'], 'corr_input_body2': ['skin_temp'], 'corr_input_zeit1': ['meal_event'], 'corr_input_zeit2': ['exercise_event'], 'corr_input_zeit3': ['ambient_temp'], }\n",
        "    anchor_loss_cols = ['heart_rate', 'is_sleeping']\n",
        "\n",
        "    # 예측에서는 단일 샘플이므로, 시작점을 0으로 가정 (오프셋 0)\n",
        "    time_offset = 0\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "    total_days_needed = config['lookback_days'] + num_main_seq_days\n",
        "\n",
        "    full_slice_scaled = data_slice_scaled.iloc[:total_days_needed * DAY_MINUTES]\n",
        "\n",
        "    model_input = {}\n",
        "    for key, cols in main_cnn_other_cols.items(): model_input[key] = tf.constant(full_slice_scaled[cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    for key, cols in fourier_corr_cols.items(): model_input[key] = tf.constant(full_slice_scaled[cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    model_input['baseline_inputs'] = tf.constant(full_slice_scaled[['x_base', 'xc_base']].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    model_input['time_offset'] = tf.constant([[time_offset]], dtype=tf.int32)\n",
        "\n",
        "    anchor_data = data_slice_unscaled.iloc[config['input_seq_len'] - DAY_MINUTES : config['input_seq_len']]\n",
        "    model_input['full_inputs_for_anchor'] = tf.constant(anchor_data[anchor_loss_cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "    # [수정된 호출] 예측 시에도 개선된 함수를 사용하여 past_info 인덱스 생성\n",
        "    model_input['past_info_indices'] = tf.constant(\n",
        "        generate_past_info_indices(\n",
        "            # [수정] iloc을 사용해 정확히 10일치 데이터만 전달\n",
        "            full_unscaled_df=data_slice_unscaled.iloc[:total_days_needed * DAY_MINUTES],\n",
        "            main_seq_start_in_full_df=time_offset, # 예측 시에는 0\n",
        "            config=config\n",
        "        )[np.newaxis, ...],\n",
        "        dtype=tf.int64\n",
        "    )\n",
        "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "    target_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'target_scaler.gz'))\n",
        "    prediction_original = target_scaler.inverse_transform(model.predict(model_input))\n",
        "    print(\"--- 예측 완료 ---\")\n",
        "    return prediction_original.flatten()\n",
        "\n",
        "def plot_learning_curve(history, output_dir):\n",
        "    plt.figure(figsize=(10, 6)); plt.plot(history['train_loss'], label='Training Loss'); plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Learning Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss (Scaled MSE)'); plt.legend(); plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"learning_curve.png\")); plt.close()\n",
        "    print(f\"학습 곡선 그래프가 {os.path.join(output_dir, 'learning_curve.png')}에 저장되었습니다.\")\n",
        "\n",
        "def plot_performance_summary(actual, predicted, output_dir):\n",
        "    mae, rmse = np.mean(np.abs(actual - predicted)), np.sqrt(np.mean((actual - predicted)**2))\n",
        "    plt.figure(figsize=(8, 8)); plt.scatter(actual, predicted, alpha=0.5); plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2, label='Ideal')\n",
        "    plt.title(f'Prediction vs Actual\\nMAE: {mae:.2f}, RMSE: {rmse:.2f}'); plt.xlabel('Actual Heart Rate (bpm)'); plt.ylabel('Predicted Heart Rate (bpm)'); plt.legend(); plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"performance_summary.png\")); plt.close()\n",
        "    print(f\"성능 요약 그래프가 {os.path.join(output_dir, 'performance_summary.png')}에 저장되었습니다.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 7. 메인 실행 블록 및 모듈화된 함수\n",
        "# =============================================================================\n",
        "def generate_and_visualize_data(config, person_profile):\n",
        "    print(\"--- 데이터 생성 및 시각화 시작 ---\")\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "    total_days_to_generate = config.get('data_duration_days', DATA_DURATION_DAYS) + config['lookback_days'] + num_main_seq_days + (config['pred_horizon'] // DAY_MINUTES) + 2\n",
        "\n",
        "    df_full = generate_realistic_biometric_data(OUTPUT_DIR, total_days_to_generate, person_profile)\n",
        "    canonical_columns = ['heart_rate', 'hrv', 'respiration_rate', 'skin_temp', 'ambient_temp', 'lux', 'is_sleeping', 'meal_event', 'exercise_event', 'integrated_lux']\n",
        "    df_raw = df_full[canonical_columns]\n",
        "\n",
        "    baseline_trajectory = generate_daily_baseline_trajectories(df_full, PARAMS, OUTPUT_DIR)\n",
        "    df = df_raw.copy(); df[['x_base', 'xc_base', 'n_base']] = baseline_trajectory\n",
        "    df['x'], df['xc'] = 0.0, 0.0\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object' or pd.api.types.is_integer_dtype(df[col]): df[col] = df[col].astype(float)\n",
        "\n",
        "    all_markers = find_all_markers(df_raw.iloc[:config.get('data_duration_days', DATA_DURATION_DAYS) * DAY_MINUTES])\n",
        "    plot_full_data_report(df_raw.iloc[:config.get('data_duration_days', DATA_DURATION_DAYS) * DAY_MINUTES], all_markers, OUTPUT_DIR)\n",
        "    print(\"--- 데이터 생성 및 시각화 완료 ---\")\n",
        "    return df_raw, df\n",
        "\n",
        "def run_training_pipeline(config, person_profile):\n",
        "    print(\"--- 'train' 모드 시작: 전체 파이프라인을 실행합니다. ---\")\n",
        "    df_raw, df = generate_and_visualize_data(config, person_profile)\n",
        "    print(\"\\n--- 데이터 분할 및 스케일링 시작 ---\")\n",
        "\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "    total_days_needed_for_sample = config['lookback_days'] + num_main_seq_days\n",
        "    max_len_for_sample = total_days_needed_for_sample * DAY_MINUTES + config['pred_horizon']\n",
        "\n",
        "    num_samples = len(df) - max_len_for_sample\n",
        "    all_indices = np.arange(num_samples); np.random.shuffle(all_indices)\n",
        "    train_end, val_end = int(num_samples * TRAIN_RATIO), int(num_samples * (TRAIN_RATIO + VALIDATION_RATIO))\n",
        "    train_indices, val_indices = all_indices[:train_end], all_indices[train_end:val_end]\n",
        "\n",
        "    feature_cols = [c for c in df.columns if c not in ['x', 'xc']]\n",
        "    feature_scaler, target_scaler = StandardScaler(), StandardScaler()\n",
        "    feature_scaler.fit(df.iloc[:train_indices[-1] if len(train_indices) > 0 else 0][feature_cols])\n",
        "    target_scaler.fit(df.iloc[:train_indices[-1] if len(train_indices) > 0 else 0][['heart_rate']])\n",
        "    df_scaled = df.copy(); df_scaled[feature_cols] = feature_scaler.transform(df[feature_cols])\n",
        "    joblib.dump(feature_scaler, os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "    joblib.dump(target_scaler, os.path.join(OUTPUT_DIR, 'target_scaler.gz'))\n",
        "    print(\"스케일러 학습 및 저장 완료.\")\n",
        "\n",
        "    train_tfrecord_path, val_tfrecord_path = os.path.join(OUTPUT_DIR, \"train.tfrecord\"), os.path.join(OUTPUT_DIR, \"val.tfrecord\")\n",
        "    create_tfrecords(df, df_scaled, train_indices, config, train_tfrecord_path)\n",
        "    create_tfrecords(df, df_scaled, val_indices, config, val_tfrecord_path)\n",
        "\n",
        "    _, val_dataset_for_build, history = train_and_evaluate(train_tfrecord_path, val_tfrecord_path, len(train_indices), len(val_indices), config)\n",
        "    plot_learning_curve(history, OUTPUT_DIR)\n",
        "\n",
        "    run_prediction_only(config, person_profile, df, df_raw, val_dataset_for_build)\n",
        "    visualize_architecture_only(config)\n",
        "    print(\"\\n\\n\" + \"=\"*59 + \"\\n    생체리듬 예측 AI 전체 파이프라인 실행 완료 (v0.16.1)   \\n\" + \"=\"*59)\n",
        "\n",
        "def run_data_generation_only(config, person_profile):\n",
        "    print(\"\\n--- 'generate_data' 모드 시작: 데이터 생성 및 그래프 출력만 수행합니다. ---\")\n",
        "    generate_and_visualize_data(config, person_profile)\n",
        "    print(\"\\n\" + \"=\"*59 + \"\\n    데이터 생성 및 시각화 완료 (generate_data 모드)   \\n\" + \"=\"*59)\n",
        "\n",
        "def run_prediction_only(config, person_profile, df=None, df_raw=None, val_dataset_for_build=None):\n",
        "    print(\"\\n--- 'predict' 모드 시작: 저장된 모델로 예측을 수행합니다. ---\")\n",
        "    model_weights_path = os.path.join(OUTPUT_DIR, \"best_model.weights.h5\")\n",
        "    if not os.path.exists(model_weights_path):\n",
        "        print(f\"오류: 모델 가중치 파일({model_weights_path})을 찾을 수 없습니다. 'train' 모드를 먼저 실행해주세요.\"); return\n",
        "\n",
        "    lco_feat, other_feat = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "    fourier_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "    predictor_config = {k: v for k, v in config.items() if k in ['num_layers', 'd_model', 'num_heads', 'dff', 'rate']}\n",
        "    predictor = MainPredictor(prediction_horizon=config['pred_horizon'], **predictor_config)\n",
        "    loaded_model = IntegratedModel(lco_feat, other_feat, fourier_model, predictor, config)\n",
        "\n",
        "    if val_dataset_for_build is None:\n",
        "        print(\"경고: 검증 데이터셋이 없어 더미 데이터로 모델을 빌드합니다.\")\n",
        "        dummy_batch_size = 2\n",
        "        dummy_x = {}\n",
        "        num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "        total_days_needed = config['lookback_days'] + num_main_seq_days\n",
        "        total_minutes_needed = total_days_needed * DAY_MINUTES\n",
        "\n",
        "        main_cnn_other_keys = [ 'input_lux', 'input_sleep', 'input_body1', 'input_body2', 'input_zeit1', 'input_zeit2', 'input_zeit3', ]\n",
        "        main_cnn_other_shapes = [1, 1, 3, 1, 1, 1, 1]\n",
        "        for key, shape in zip(main_cnn_other_keys, main_cnn_other_shapes):\n",
        "            dummy_x[key] = tf.zeros((dummy_batch_size, total_minutes_needed, shape))\n",
        "\n",
        "        fourier_corr_keys = [ 'corr_input_lux', 'corr_input_sleep', 'corr_input_body1', 'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3', ]\n",
        "        for key, shape in zip(fourier_corr_keys, main_cnn_other_shapes):\n",
        "            dummy_x[key] = tf.zeros((dummy_batch_size, total_minutes_needed, shape))\n",
        "\n",
        "        dummy_x['baseline_inputs'] = tf.zeros((dummy_batch_size, total_minutes_needed, 2))\n",
        "        dummy_x['past_info_indices'] = tf.zeros((dummy_batch_size, config['num_markers_to_keep'] * 3), dtype=tf.int64)\n",
        "        dummy_x['full_inputs_for_anchor'] = tf.zeros((dummy_batch_size, DAY_MINUTES, 2))\n",
        "        dummy_x['time_offset'] = tf.zeros((dummy_batch_size, 1), dtype=tf.int32)\n",
        "    else: dummy_x, _ = next(iter(val_dataset_for_build))\n",
        "\n",
        "    loaded_model(dummy_x, training=False); loaded_model.load_weights(model_weights_path)\n",
        "    print(\"모델 가중치 로드 완료.\")\n",
        "\n",
        "    if df is None or df_raw is None: df_raw, df = generate_and_visualize_data(config, person_profile)\n",
        "    feature_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "    df_scaled = df.copy(); df_scaled[[c for c in df.columns if c not in ['x', 'xc']]] = feature_scaler.transform(df[[c for c in df.columns if c not in ['x', 'xc']]])\n",
        "\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "    total_days_needed = config['lookback_days'] + num_main_seq_days\n",
        "    max_len = total_days_needed * DAY_MINUTES + config['pred_horizon']\n",
        "\n",
        "    test_slice_start = len(df) - max_len\n",
        "    test_data_slice_unscaled = df.iloc[test_slice_start : test_slice_start + max_len]\n",
        "    test_data_slice_scaled = df_scaled.iloc[test_slice_start : test_slice_start + max_len]\n",
        "\n",
        "    predicted_hr = predict_future_hr(loaded_model, test_data_slice_unscaled, test_data_slice_scaled, config)\n",
        "\n",
        "    actual_hr_df = df_raw.iloc[-config['pred_horizon']:]\n",
        "\n",
        "    plt.figure(figsize=(15, 7)); plt.plot(actual_hr_df.index, actual_hr_df['heart_rate'].values, label='Actual Heart Rate', color='blue'); plt.plot(actual_hr_df.index, predicted_hr, label='Predicted Heart Rate', color='red', linestyle='--')\n",
        "    plt.title('Heart Rate Prediction vs Actual (Time Series)'); plt.xlabel('Time'); plt.ylabel('Heart Rate (bpm)'); plt.legend(); plt.xticks(rotation=45); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"prediction_vs_actual.png\")); plt.close()\n",
        "    print(f\"\\n예측 결과 그래프가 {os.path.join(output_dir, 'prediction_vs_actual.png')}에 저장되었습니다.\")\n",
        "    plot_performance_summary(actual_hr_df['heart_rate'].values, predicted_hr, OUTPUT_DIR)\n",
        "\n",
        "def visualize_architecture_only(config):\n",
        "    print(\"\\n--- 'visualize_arch' 모드 시작: 모델 구조도를 생성합니다. ---\")\n",
        "    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
        "    try:\n",
        "        lco_feat, other_feat = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "        fourier_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "        tf.keras.utils.plot_model(lco_feat, to_file=os.path.join(OUTPUT_DIR, 'lco_feature_extractor.png'), show_shapes=True)\n",
        "        tf.keras.utils.plot_model(other_feat, to_file=os.path.join(OUTPUT_DIR, 'other_feature_extractor.png'), show_shapes=True)\n",
        "        tf.keras.utils.plot_model(fourier_model, to_file=os.path.join(OUTPUT_DIR, 'fourier_correction_model.png'), show_shapes=True)\n",
        "        print(f\"모델 구조 이미지가 {OUTPUT_DIR}에 저장되었습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"모델 시각화 중 오류 발생: {e}\\npydot과 graphviz가 설치되어 있는지 확인해주세요. (pip install pydot graphviz)\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    EXECUTION_MODE = 'train'\n",
        "\n",
        "    person_profile_office_worker = {\n",
        "        'wake_up_weekday_mean': 7.5, 'wake_up_weekday_std': 0.75,\n",
        "        'wake_up_weekend_mean': 9.5, 'wake_up_weekend_std': 1.5,\n",
        "        'sleep_duration_mean': 7.0, 'exercise_freq_prob': 0.4,\n",
        "    }\n",
        "\n",
        "    config = {\n",
        "        'batch_size': BATCH_SIZE, 'input_seq_len': INPUT_SEQUENCE_LENGTH,\n",
        "        'pred_horizon': PREDICTION_HORIZON, 'lookback_days': PHASE_CORRECTION_LOOKBACK_DAYS,\n",
        "        'day_minutes': DAY_MINUTES, 'num_markers_to_keep': NUM_MARKERS_TO_KEEP,\n",
        "        'lambda_reg': LAMBDA_REG, 'lambda_cont': LAMBDA_CONT, 'lambda_anchor': LAMBDA_ANCHOR,\n",
        "        'num_layers': NUM_LAYERS, 'd_model': D_MODEL, 'num_heads': NUM_HEADS, 'dff': DFF, 'rate': DROPOUT_RATE,\n",
        "        'epochs': EPOCHS, 'learning_rate': LEARNING_RATE, 'num_harmonics': NUM_FOURIER_HARMONICS,\n",
        "        'data_duration_days': DATA_DURATION_DAYS,\n",
        "        'lstm_units': LSTM_UNITS,\n",
        "    }\n",
        "\n",
        "    if EXECUTION_MODE == 'train':\n",
        "        run_training_pipeline(config, person_profile_office_worker)\n",
        "    elif EXECUTION_MODE == 'predict':\n",
        "        run_prediction_only(config, person_profile_office_worker)\n",
        "    elif EXECUTION_MODE == 'visualize_arch':\n",
        "        visualize_architecture_only(config)\n",
        "    elif EXECUTION_MODE == 'generate_data':\n",
        "        run_data_generation_only(config, person_profile_office_worker)\n",
        "    else:\n",
        "        print(f\"알 수 없는 모드입니다: {EXECUTION_MODE}. 'train', 'predict', 'visualize_arch', 'generate_data' 중에서 선택해주세요.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 'train' 모드 시작: 전체 파이프라인을 실행합니다. ---\n",
            "--- 데이터 생성 및 시각화 시작 ---\n",
            "--- [1단계] 일일 재조정 기준 궤도 생성 시작 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "일일 기준 궤도 생성 중: 100%|██████████| 42/42 [00:06<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [1단계] 일일 재조정 기준 궤도 생성 완료 ---\n",
            "--- 전체 데이터에 대한 생체 마커 탐색 시작 ---\n",
            "--- 생체 마커 탐색 완료 ---\n",
            "--- 전체 데이터 상세 리포트 생성 시작 ---\n",
            "--- 데이터 생성 및 시각화 완료 ---\n",
            "\n",
            "--- 데이터 분할 및 스케일링 시작 ---\n",
            "스케일러 학습 및 저장 완료.\n",
            "--- TFRecord 파일 생성 시작: final_biometric_pipeline_output_v0.16.1_PastInfoFix/train.tfrecord ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train.tfrecord 생성 중: 100%|██████████| 36816/36816 [17:39<00:00, 34.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TFRecord 파일 생성 완료: final_biometric_pipeline_output_v0.16.1_PastInfoFix/train.tfrecord ---\n",
            "--- TFRecord 파일 생성 시작: final_biometric_pipeline_output_v0.16.1_PastInfoFix/val.tfrecord ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val.tfrecord 생성 중: 100%|██████████| 4602/4602 [02:13<00:00, 34.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TFRecord 파일 생성 완료: final_biometric_pipeline_output_v0.16.1_PastInfoFix/val.tfrecord ---\n",
            "\n",
            "--- 최종 모델 학습 및 검증 시작 ---\n",
            "\n",
            "Epoch 1/15\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VlUF0At6dDw",
        "outputId": "6380823a-78a7-4c48-d187-8a63145cb2d6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLQ94cOB9-v7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}