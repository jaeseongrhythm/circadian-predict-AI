{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "of99X71DalVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install causal-conv1d\n",
        "!pip install mamba-tf"
      ],
      "metadata": {
        "id": "cC34L9b6r2Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "(사전학습용) 생체 기준선 사후평가 AI\n",
        "  ㄴ 일주기 생체리듬 예측 AI v0.23.0\n",
        "\n",
        "[모델 설명]\n",
        "- 일주기 상태의 기준선(baseline)인 기상 후 30분~90분을 예측하도록 학습.\n",
        "- 특정 이벤트의 기준선 변화 유발 효과 정량 평가 가능(delta_baseline).\n",
        "- 과거 데이터의 종합적 맥락에 기반한 예측 달성 -> fine tuning에 유리.\n",
        "- LCO 궤도 보정에 미래 데이터 참조(사후평가) -> 실시간 예측 모델에 fine tuning 유리.\n",
        "- 구조: Fourier correction(CNN. LSTM), CNN, FiLM layer, Mamba, Transformer, Prediction Head\n",
        "\n",
        "[v0.23.0 주요 변경사항]:\n",
        "- Mamba (Selective State Space Model) 아키텍처 적용\n",
        "  - 기존의 하드코딩된 인덱싱 기반 past_info 추출 방식을 Mamba 레이어로 대체\n",
        "  - 과거 시퀀스 전체를 입력으로 받아 동적으로 정보를 요약하고 장기 의존성 학습\n",
        "  - 기존의 중요 이벤트 인덱스는 Mamba에 '강조 피처'로 제공되어 학습에 활용\n",
        "  - 계산 효율성을 높여 더 긴 과거 맥락을 처리할 수 있는 기반 마련\n",
        "- 위상 기반 위치 인코딩 적용\n",
        "  - LCO 기반의 생체시각(위상) 정보 주입\n",
        "\n",
        "[v0.21.2 주요 변경사항]:\n",
        "- FiLM (Feature-wise Linear Modulation) 레이어 적용\n",
        "  - 개인 프로필(성별, 나이, BMI) 정보를 모델에 주입하여 개인화된 예측 수행\n",
        "\n",
        "[폴더 구조]\n",
        "|project_root\n",
        "|--biometirc_data\n",
        "|  |--person_1\n",
        "|  |  |--biometric_data_person_1.csv\n",
        "|  |  |--biometric_data_person_1_baseline.csv\n",
        "|  |--...\n",
        "|--baseline_prediction_ai.py\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 0. 라이브러리 임포트 및 파이프라인 설정\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.interpolate import interp1d\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Concatenate, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, LSTM, Reshape, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import mixed_precision\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "import bisect\n",
        "from datetime import datetime, timedelta\n",
        "import yaml\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# [OPTIMIZATION] 혼합 정밀도 정책 설정\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# --- 파이프라인 제어 설정 ---\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/real_time_bp_prediction_output_v0.23.0\"\n",
        "ROOT_DATA_DIR = \"/content/drive/MyDrive/biometric_data\"\n",
        "PROFILE_DATA_DIR = \"/content/drive/MyDrive/biometric_data\"\n",
        "\n",
        "# --- 데이터 및 모델 설정 ---\n",
        "INPUT_SEQUENCE_LENGTH = 14 * 24 * 60\n",
        "BASELINE_WINDOW = 60\n",
        "NUM_METRICS = 4\n",
        "PHASE_CORRECTION_LOOKBACK_DAYS = 3\n",
        "DAY_MINUTES = 24 * 60\n",
        "NUM_MARKERS_TO_KEEP = 12\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VALIDATION_RATIO = 0.1\n",
        "\n",
        "# --- 모델 하이퍼파라미터 ---\n",
        "D_MODEL = 128\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 8\n",
        "DFF = 256\n",
        "DROPOUT_RATE = 0.05\n",
        "NUM_FOURIER_HARMONICS = 5\n",
        "LSTM_UNITS = 64\n",
        "PROFILE_EMBEDDING_DIM = 64\n",
        "MARKER_SELECTION_EXTENSION_MINUTES = 8 * 60\n",
        "MAMBA_D_STATE = 16\n",
        "MAMBA_D_CONV = 4\n",
        "MAMBA_EXPAND = 2\n",
        "\n",
        "# --- 학습 하이퍼파라미터 ---\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 3\n",
        "LEARNING_RATE = 0.0005\n",
        "LAMBDA_REG = 0.1\n",
        "LAMBDA_CONT = 10.0\n",
        "LAMBDA_ANCHOR = 5.0\n",
        "\n",
        "# --- 물리 모델 파라미터 ---\n",
        "PARAMS = {\n",
        "    'mu': 0.13, 'q': 1/3, 'k': 0.55, 'alpha0': 0.1, 'I0': 9500,\n",
        "    'p': 0.5, 'beta': 0.007, 'G': 37, 'rho': 0.032, 'tau_x': 24.2,\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# 1. FiLM 관련 기능 정의\n",
        "# =============================================================================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class FiLMLayer(Layer):\n",
        "    \"\"\"\n",
        "    FiLM(Feature-wise Linear Modulation) 레이어.\n",
        "    조건부 벡터(profile_embedding)를 사용하여 입력 텐서(features)를 조절합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FiLMLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_shape, condition_shape = input_shape\n",
        "        self.gamma_beta_generator = Dense(feature_shape[-1] * 2, name='film_gamma_beta_generator')\n",
        "        super(FiLMLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features, condition = inputs\n",
        "        gamma_beta = self.gamma_beta_generator(condition)\n",
        "        gamma, beta = tf.split(gamma_beta, num_or_size_splits=2, axis=-1)\n",
        "        gamma = tf.expand_dims(gamma, axis=1)\n",
        "        beta = tf.expand_dims(beta, axis=1)\n",
        "        return gamma * features + beta\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(FiLMLayer, self).get_config()\n",
        "\n",
        "def load_and_preprocess_profile(person_id, profile_dir=PROFILE_DATA_DIR):\n",
        "    profile_path = os.path.join(profile_dir, f\"person_{person_id}\", f\"profile_person_{person_id}.txt\")\n",
        "    if not os.path.exists(profile_path):\n",
        "        # 프로필이 없는 경우 기본값 반환\n",
        "        print(f\"경고: person_{person_id}의 프로필 파일을 찾을 수 없습니다. 기본값을 사용합니다.\")\n",
        "        age, bmi, gender = 40, 22, 1.0 # 기본값: 40세, BMI 22, 남성\n",
        "    else:\n",
        "        with open(profile_path, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                profile_data = yaml.safe_load(\"\\n\".join(f.read().splitlines()[1:]))\n",
        "                gender = 1.0 if profile_data.get('gender', 'male') == 'male' else 0.0\n",
        "                age = profile_data.get('age', 40)\n",
        "                bmi = profile_data.get('bmi', 22)\n",
        "            except Exception as e:\n",
        "                print(f\"경고: {profile_path} 파싱 오류. 기본값을 사용합니다. 오류: {e}\")\n",
        "                age, bmi, gender = 40, 22, 1.0\n",
        "\n",
        "    age_normalized = (age - 18) / (80 - 18)\n",
        "    bmi_normalized = (bmi - 15) / (40 - 15)\n",
        "    return np.array([gender, age_normalized, bmi_normalized], dtype=np.float32)\n",
        "\n",
        "def build_profile_embedding_model(profile_vector_dim, embedding_dim=PROFILE_EMBEDDING_DIM):\n",
        "    input_profile = Input(shape=(profile_vector_dim,), name=\"input_profile\")\n",
        "    x = Dense(32, activation='relu')(input_profile)\n",
        "    embedding = Dense(embedding_dim, activation='relu', name=\"profile_embedding\")(x)\n",
        "    model = Model(inputs=input_profile, outputs=embedding, name=\"ProfileEmbeddingModel\")\n",
        "    return model\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 생체리듬 물리 모델 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "def find_hr_nadir(heart_rate_data, is_sleeping_data, day_minutes=1440):\n",
        "    num_days = len(heart_rate_data) // day_minutes\n",
        "    daily_nadirs = []\n",
        "    for day in range(num_days):\n",
        "        day_start, day_end = day * day_minutes, (day + 1) * day_minutes\n",
        "        day_hr, day_sleep = heart_rate_data[day_start:day_end], is_sleeping_data[day_start:day_end]\n",
        "        sleep_hr = day_hr[day_sleep == 1]\n",
        "        if len(sleep_hr) > 0:\n",
        "            original_indices = np.where(day_sleep == 1)[0]\n",
        "            daily_nadirs.append(original_indices[np.argmin(sleep_hr)])\n",
        "        else:\n",
        "            daily_nadirs.append(np.argmin(day_hr))\n",
        "    return np.mean(daily_nadirs) if daily_nadirs else day_minutes / 2\n",
        "\n",
        "def _sigmoid(x, k=2, x0=0):\n",
        "    return 1 / (1 + np.exp(-k * (x - x0)))\n",
        "\n",
        "def lco_model_ode(t, y, params, light_func, sleep_func):\n",
        "    x, xc, n = y\n",
        "    if not np.all(np.isfinite(y)): return [0,0,0]\n",
        "    mu, q, k, alpha0, I0, p, beta, G, rho, tau_x = params.values()\n",
        "    I, sigma = light_func(t), sleep_func(t)\n",
        "    I = max(I, 0)\n",
        "    alpha = alpha0 * ((I / I0)**p) * (I / (I + 100.0)) if I > 0 else 0\n",
        "    B_hat = G * (1 - n) * alpha\n",
        "    B = B_hat * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    cbt_min_phase_angle = -170.7 * np.pi / 180.0\n",
        "    current_phase = np.arctan2(xc, x)\n",
        "    phase_diff_rad = (current_phase - cbt_min_phase_angle + np.pi) % (2 * np.pi) - np.pi\n",
        "    psi_c_x = phase_diff_rad * (tau_x / (2 * np.pi)) + (tau_x / 2)\n",
        "    weight_enter = _sigmoid(psi_c_x, k=2, x0=16.5)\n",
        "    weight_exit = 1 - _sigmoid(psi_c_x, k=2, x0=21.0)\n",
        "    wmz_weight = weight_enter * weight_exit * sigma\n",
        "    Ns_hat_normal = rho * (1/3.0 - sigma)\n",
        "    Ns_hat_wmz = rho * (1/3.0)\n",
        "    Ns_hat = Ns_hat_normal * (1 - wmz_weight) + Ns_hat_wmz * wmz_weight\n",
        "    Ns = Ns_hat * (1 - np.tanh(10 * x))\n",
        "    dxdt = (np.pi / 12.0) * (xc + mu * (x/3.0 + (4.0/3.0)*x**3 - (256.0/105.0)*x**7) + B + Ns)\n",
        "    tau_term_sq = (24.0 / (0.99729 * tau_x))**2\n",
        "    dxc_dt = (np.pi / 12.0) * (q * B * xc - x * (tau_term_sq + k * B))\n",
        "    dn_dt = 60.0 * (alpha * (1 - n) - beta * n)\n",
        "    return [dxdt, dxc_dt, dn_dt]\n",
        "\n",
        "def lco_model_jacobian(t, y, params, light_func, sleep_func):\n",
        "    x, xc, n = y\n",
        "    if not np.all(np.isfinite(y)): return np.zeros((3,3))\n",
        "    mu, q, k, alpha0, I0, p, beta, G, rho, tau_x = params.values()\n",
        "    I, sigma = light_func(t), sleep_func(t)\n",
        "    I = max(I, 0)\n",
        "    alpha = alpha0 * ((I / I0)**p) * (I / (I + 100.0)) if I > 0 else 0\n",
        "    cbt_min_phase_angle = -170.7 * np.pi / 180.0\n",
        "    current_phase = np.arctan2(xc, x)\n",
        "    phase_diff_rad = (current_phase - cbt_min_phase_angle + np.pi) % (2 * np.pi) - np.pi\n",
        "    psi_c_x = phase_diff_rad * (tau_x / (2 * np.pi)) + (tau_x / 2)\n",
        "    weight_enter = _sigmoid(psi_c_x, k=2, x0=16.5)\n",
        "    weight_exit = 1 - _sigmoid(psi_c_x, k=2, x0=21.0)\n",
        "    wmz_weight = weight_enter * weight_exit * sigma\n",
        "    Ns_hat_normal = rho * (1/3.0 - sigma)\n",
        "    Ns_hat_wmz = rho * (1/3.0)\n",
        "    Ns_hat = Ns_hat_normal * (1 - wmz_weight) + Ns_hat_wmz * wmz_weight\n",
        "    dB_dx = -0.4 * G * alpha * (1 - n) * (1 - 0.4 * xc)\n",
        "    dB_dxc = -0.4 * G * alpha * (1 - n) * (1 - 0.4 * x)\n",
        "    dB_dn = -G * alpha * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    dNs_dx = -Ns_hat * 10.0 * (1.0 / np.cosh(10 * x))**2\n",
        "    J = np.zeros((3, 3))\n",
        "    J[0, 0] = (np.pi / 12.0) * (mu * (1/3.0 + 4.0 * x**2 - (256.0*7.0/105.0) * x**6) + dB_dx + dNs_dx)\n",
        "    J[0, 1] = (np.pi / 12.0) * (1.0 + dB_dxc)\n",
        "    J[0, 2] = (np.pi / 12.0) * dB_dn\n",
        "    B = G * alpha * (1 - n) * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    tau_term_sq = (24.0 / (0.99729 * tau_x))**2\n",
        "    J[1, 0] = (np.pi / 12.0) * (q * xc * dB_dx - (tau_term_sq + k * B) - k * x * dB_dx)\n",
        "    J[1, 1] = (np.pi / 12.0) * (q * B + q * xc * dB_dxc - k * x * dB_dxc)\n",
        "    J[1, 2] = (np.pi / 12.0) * (q * xc * dB_dn - k * x * dB_dn)\n",
        "    J[2, 2] = 60.0 * (-alpha - beta)\n",
        "    return J\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 데이터 로딩 및 통합 함수 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "def load_all_biometric_data(root_dir=ROOT_DATA_DIR):\n",
        "    print(f\"--- 다중 사용자 데이터 로딩 시작: {root_dir} ---\")\n",
        "    all_data = []\n",
        "    person_ids = []\n",
        "    if not os.path.exists(root_dir):\n",
        "        raise FileNotFoundError(f\"데이터 디렉토리를 찾을 수 없습니다: {root_dir}\")\n",
        "    for person_folder in sorted(os.listdir(root_dir)):\n",
        "        if person_folder.startswith(\"person_\"):\n",
        "            person_id = int(person_folder.split(\"_\")[1])\n",
        "            csv_path = os.path.join(root_dir, person_folder, f\"biometric_data_{person_folder}.csv\")\n",
        "            if os.path.exists(csv_path):\n",
        "                df = pd.read_csv(csv_path)\n",
        "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "                df['person_id'] = person_id\n",
        "                all_data.append(df)\n",
        "                person_ids.append(person_id)\n",
        "                print(f\"  - Person {person_id}: {len(df)} 레코드 로드\")\n",
        "    if not all_data:\n",
        "        raise ValueError(\"로드된 데이터가 없습니다.\")\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"--- 총 {len(person_ids)}명의 데이터 통합 완료: {len(combined_df)} 레코드 ---\")\n",
        "    return combined_df, sorted(person_ids)\n",
        "\n",
        "def load_baseline_lookup(root_dir=ROOT_DATA_DIR):\n",
        "    print(f\"--- 기준선 데이터 로딩 시작 ---\")\n",
        "    baseline_lookup = {}\n",
        "    baseline_metric_cols = ['heart_rate', 'hrv', 'respiration_rate', 'skin_temp']\n",
        "    for person_folder in sorted(os.listdir(root_dir)):\n",
        "        if person_folder.startswith(\"person_\"):\n",
        "            person_id = int(person_folder.split(\"_\")[1])\n",
        "            baseline_path = os.path.join(root_dir, person_folder, f\"biometric_data_{person_folder}_baseline.csv\")\n",
        "            if os.path.exists(baseline_path):\n",
        "                baseline_df = pd.read_csv(baseline_path)\n",
        "                if baseline_df.empty: continue\n",
        "                baseline_df['timestamp'] = pd.to_datetime(baseline_df['timestamp'])\n",
        "                baseline_df['date'] = baseline_df['timestamp'].dt.date.astype(str)\n",
        "                for date, group_df in baseline_df.groupby('date'):\n",
        "                    if len(group_df) == BASELINE_WINDOW:\n",
        "                        values = group_df[baseline_metric_cols].values\n",
        "                        key = (person_id, date)\n",
        "                        baseline_lookup[key] = values\n",
        "    print(f\"--- 총 {len(baseline_lookup)}개의 기준선 데이터 로드 완료 ---\")\n",
        "    return baseline_lookup\n",
        "\n",
        "def preprocess_and_engineer_features(df):\n",
        "    print(\"--- 새로운 피처 엔지니어링 및 전처리 시작 ---\")\n",
        "    categorical_cols = ['meal_type', 'exercise_type', 'exercise_intensity']\n",
        "    for col in categorical_cols: df[col] = df[col].fillna('none')\n",
        "    continuous_cols = ['meal_calories', 'light_color_temp', 'caffeine_g', 'alcohol_g']\n",
        "    for col in continuous_cols: df[col] = df[col].fillna(0.0)\n",
        "    df['caffeine_event'] = (df['caffeine_g'] > 0).astype(float)\n",
        "    df['alcohol_event'] = (df['alcohol_g'] > 0).astype(float)\n",
        "    meal_categories = ['none', 'balanced', 'high_protein', 'high_fat', 'high_carb']\n",
        "    df['meal_type'] = pd.Categorical(df['meal_type'], categories=meal_categories, ordered=False)\n",
        "    df = pd.get_dummies(df, columns=['meal_type'], prefix='meal_type', dtype=float)\n",
        "    exercise_type_categories = ['none', 'aerobic', 'anaerobic']\n",
        "    df['exercise_type'] = pd.Categorical(df['exercise_type'], categories=exercise_type_categories, ordered=False)\n",
        "    df = pd.get_dummies(df, columns=['exercise_type'], prefix='exercise_type', dtype=float)\n",
        "    exercise_intensity_categories = ['none', 'low', 'medium', 'high']\n",
        "    df['exercise_intensity'] = pd.Categorical(df['exercise_intensity'], categories=exercise_intensity_categories, ordered=False)\n",
        "    df = pd.get_dummies(df, columns=['exercise_intensity'], prefix='exercise_intensity', dtype=float)\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'bool': df[col] = df[col].astype(float)\n",
        "    print(\"--- 피처 엔지니어링 완료 ---\")\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 딥러닝 모델 정의\n",
        "# =============================================================================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class FourierTrajectoryLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_harmonics, **kwargs):\n",
        "        super(FourierTrajectoryLayer, self).__init__(**kwargs)\n",
        "        self.num_harmonics = num_harmonics\n",
        "        self.output_dim = DAY_MINUTES\n",
        "        self.t = tf.constant(np.linspace(0.0, 2 * np.pi, self.output_dim), dtype=tf.float32)\n",
        "    def call(self, correction_params):\n",
        "        original_dtype = correction_params.dtype\n",
        "        correction_params_f32 = tf.cast(correction_params, tf.float32)\n",
        "        num_coeffs_per_traj = 1 + 2 * self.num_harmonics\n",
        "        num_total_coeffs = num_coeffs_per_traj * 2\n",
        "        coeffs = correction_params_f32[:, :num_total_coeffs]\n",
        "        endpoints = correction_params_f32[:, num_total_coeffs:]\n",
        "        coeffs_x = coeffs[:, :num_coeffs_per_traj]\n",
        "        coeffs_xc = coeffs[:, num_coeffs_per_traj:]\n",
        "        endpoints_x = endpoints[:, 0:2]\n",
        "        endpoints_xc = endpoints[:, 2:4]\n",
        "        traj_x = self._build_trajectory(coeffs_x, endpoints_x)\n",
        "        traj_xc = self._build_trajectory(coeffs_xc, endpoints_xc)\n",
        "        result = tf.stack([traj_x, traj_xc], axis=-1)\n",
        "        return tf.cast(result, original_dtype)\n",
        "    def _build_trajectory(self, coeffs, endpoints):\n",
        "        a0 = coeffs[:, 0:1]\n",
        "        a_n = coeffs[:, 1:self.num_harmonics + 1]\n",
        "        b_n = coeffs[:, self.num_harmonics + 1:]\n",
        "        trajectory = a0\n",
        "        for n in range(1, self.num_harmonics + 1):\n",
        "            trajectory += a_n[:, n-1:n] * tf.cos(n * self.t)\n",
        "            trajectory += b_n[:, n-1:n] * tf.sin(n * self.t)\n",
        "        raw_start, raw_end = trajectory[:, 0:1], trajectory[:, -1:]\n",
        "        target_start, target_end = endpoints[:, 0:1], endpoints[:, 1:2]\n",
        "        ramp = tf.expand_dims(tf.linspace(0.0, 1.0, self.output_dim), 0)\n",
        "        linear_correction = raw_start + (raw_end - raw_start) * ramp\n",
        "        target_linear = target_start + (target_end - target_start) * ramp\n",
        "        return trajectory - linear_correction + target_linear\n",
        "    def get_config(self):\n",
        "        config = super(FourierTrajectoryLayer, self).get_config()\n",
        "        config.update({\"num_harmonics\": self.num_harmonics})\n",
        "        return config\n",
        "\n",
        "def build_fourier_correction_model(lookback_minutes, num_harmonics, lstm_units):\n",
        "    num_coeffs_per_traj = 1 + 2 * num_harmonics\n",
        "    output_size = (num_coeffs_per_traj * 2) + 4\n",
        "    input_lux = Input(shape=(lookback_minutes, 2), name='corr_input_lux')\n",
        "    input_sleep = Input(shape=(lookback_minutes, 1), name='corr_input_sleep')\n",
        "    input_body1 = Input(shape=(lookback_minutes, 3), name='corr_input_body1')\n",
        "    input_body2 = Input(shape=(lookback_minutes, 1), name='corr_input_body2')\n",
        "    input_zeit1 = Input(shape=(lookback_minutes, 6), name='corr_input_zeit1')\n",
        "    input_zeit2 = Input(shape=(lookback_minutes, 6), name='corr_input_zeit2')\n",
        "    input_zeit3 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit3')\n",
        "    input_caffeine = Input(shape=(lookback_minutes, 2), name='corr_input_caffeine')\n",
        "    input_alcohol = Input(shape=(lookback_minutes, 2), name='corr_input_alcohol')\n",
        "    def create_feat_extractor(inp, name):\n",
        "        x = Conv1D(16, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn1')(inp)\n",
        "        x = Conv1D(8, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn2')(x)\n",
        "        return x\n",
        "    features = [\n",
        "        create_feat_extractor(input_lux, 'lux'), create_feat_extractor(input_sleep, 'sleep'),\n",
        "        create_feat_extractor(input_body1, 'body1'), create_feat_extractor(input_body2, 'body2'),\n",
        "        create_feat_extractor(input_zeit1, 'zeit1'), create_feat_extractor(input_zeit2, 'zeit2'),\n",
        "        create_feat_extractor(input_zeit3, 'zeit3'),\n",
        "        create_feat_extractor(input_caffeine, 'caffeine'), create_feat_extractor(input_alcohol, 'alcohol'),\n",
        "    ]\n",
        "    combined_feature_sequence = Concatenate(axis=-1, dtype='float32')(features)\n",
        "    lstm_output = LSTM(lstm_units, return_sequences=False, name='correction_lstm')(combined_feature_sequence)\n",
        "    x = Dropout(0.2)(lstm_output)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    correction_params = Dense(output_size, activation='linear', name='correction_params', dtype='float32')(x)\n",
        "    model_inputs = [input_lux, input_sleep, input_body1, input_body2,\n",
        "                    input_zeit1, input_zeit2, input_zeit3,\n",
        "                    input_caffeine, input_alcohol]\n",
        "    return Model(inputs=model_inputs, outputs=correction_params, name='FourierCorrectionModel')\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.position, self.d_model = position, d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model)\n",
        "        sines, cosines = tf.math.sin(angle_rads[:, 0::2]), tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return inputs + tf.cast(self.pos_encoding[:, :tf.shape(inputs)[1], :], dtype=inputs.dtype)\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\"position\": self.position, \"d_model\": self.d_model})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PhaseAnglePositionalEncoding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    LCO 궤적(x, xc)에서 계산된 위상각(Phase Angle)을 기반으로\n",
        "    동적인 위치 인코딩을 생성하는 레이어.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, **kwargs):\n",
        "        super(PhaseAnglePositionalEncoding, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.angle_rates = 1 / tf.pow(10000, (2 * (tf.range(d_model, dtype=tf.float32) // 2)) / tf.cast(d_model, tf.float32))\n",
        "\n",
        "    def call(self, lco_trajectory):\n",
        "        \"\"\"\n",
        "        lco_trajectory: (batch_size, sequence_length, 2) 형태의 LCO(x, xc) 궤적 텐서\n",
        "        \"\"\"\n",
        "        phase_angle = tf.math.atan2(lco_trajectory[:, :, 1], lco_trajectory[:, :, 0])\n",
        "        phase_angle_expanded = phase_angle[:, :, tf.newaxis]\n",
        "        angle_rads = phase_angle_expanded * self.angle_rates\n",
        "\n",
        "        sines = tf.math.sin(angle_rads[:, :, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, :, 1::2])\n",
        "\n",
        "        phase_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "\n",
        "        return tf.cast(phase_encoding, dtype=lco_trajectory.dtype)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PhaseAnglePositionalEncoding, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model})\n",
        "        return config\n",
        "\n",
        "def build_main_feature_extractors(time_steps=INPUT_SEQUENCE_LENGTH, d_model=D_MODEL):\n",
        "    feature_proportions = {\n",
        "        'lco': 0.20, 'lux': 0.125, 'sleep': 0.10, 'body1': 0.225, 'body2': 0.05,\n",
        "        'zeit1': 0.075, 'zeit2': 0.075, 'zeit3': 0.05, 'caffeine': 0.05, 'alcohol': 0.05,\n",
        "    }\n",
        "    cnn_block_map = { name: max(2, int(d_model * prop) // 2 * 2) for name, prop in feature_proportions.items() }\n",
        "    cnn_block_map['lco'] += d_model - sum(cnn_block_map.values())\n",
        "    def create_cnn_block(n_features, name_prefix):\n",
        "        return tf.keras.Sequential([\n",
        "            Conv1D(filters=32, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn1\"),\n",
        "            Conv1D(filters=n_features, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn2\")\n",
        "        ], name=f\"{name_prefix}_cnn_block\")\n",
        "    input_lco = Input(shape=(time_steps, 2), name='input_lco')\n",
        "    features_lco = create_cnn_block(cnn_block_map['lco'], 'lco')(input_lco)\n",
        "    lco_feature_extractor = Model(inputs=input_lco, outputs=features_lco, name='LCOFeatureExtractor')\n",
        "    input_lux = Input(shape=(time_steps, 2), name='input_lux')\n",
        "    input_sleep = Input(shape=(time_steps, 1), name='input_sleep')\n",
        "    input_body1 = Input(shape=(time_steps, 3), name='input_body1')\n",
        "    input_body2 = Input(shape=(time_steps, 1), name='input_body2')\n",
        "    input_zeit1 = Input(shape=(time_steps, 6), name='input_zeit1')\n",
        "    input_zeit2 = Input(shape=(time_steps, 6), name='input_zeit2')\n",
        "    input_zeit3 = Input(shape=(time_steps, 1), name='input_zeit3')\n",
        "    input_caffeine = Input(shape=(time_steps, 2), name='input_caffeine')\n",
        "    input_alcohol = Input(shape=(time_steps, 2), name='input_alcohol')\n",
        "    features_lux = create_cnn_block(cnn_block_map['lux'], 'lux')(input_lux)\n",
        "    features_sleep = create_cnn_block(cnn_block_map['sleep'], 'sleep')(input_sleep)\n",
        "    features_body1 = create_cnn_block(cnn_block_map['body1'], 'body1')(input_body1)\n",
        "    features_body2 = create_cnn_block(cnn_block_map['body2'], 'body2')(input_body2)\n",
        "    features_zeit1 = create_cnn_block(cnn_block_map['zeit1'], 'zeit1')(input_zeit1)\n",
        "    features_zeit2 = create_cnn_block(cnn_block_map['zeit2'], 'zeit2')(input_zeit2)\n",
        "    features_zeit3 = create_cnn_block(cnn_block_map['zeit3'], 'zeit3')(input_zeit3)\n",
        "    features_caffeine = create_cnn_block(cnn_block_map['caffeine'], 'caffeine')(input_caffeine)\n",
        "    features_alcohol = create_cnn_block(cnn_block_map['alcohol'], 'alcohol')(input_alcohol)\n",
        "    combined_other_features = Concatenate(axis=-1, name='combined_other_features', dtype='float32')([\n",
        "        features_lux, features_sleep, features_body1, features_body2,\n",
        "        features_zeit1, features_zeit2, features_zeit3,\n",
        "        features_caffeine, features_alcohol\n",
        "    ])\n",
        "    other_inputs = [input_lux, input_sleep, input_body1, input_body2,\n",
        "                    input_zeit1, input_zeit2, input_zeit3,\n",
        "                    input_caffeine, input_alcohol]\n",
        "    other_feature_extractor = Model(inputs=other_inputs, outputs=combined_other_features, name='OtherFeatureExtractor')\n",
        "    return lco_feature_extractor, other_feature_extractor\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ContextualTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(ContextualTransformerBlock, self).__init__(**kwargs)\n",
        "        self.d_model, self.num_heads, self.dff, self.rate = d_model, num_heads, dff, rate\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1, self.layernorm2, self.layernorm3 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1, self.dropout2, self.dropout3 = Dropout(rate), Dropout(rate), Dropout(rate)\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        past_info, current_info = inputs\n",
        "        attn_output_current = self.mha1(query=current_info, key=current_info, value=current_info, training=training)\n",
        "        current_info_sa = self.layernorm1(current_info + self.dropout1(attn_output_current, training=training))\n",
        "        attn_output_cross = self.mha2(query=current_info_sa, key=past_info, value=past_info, training=training)\n",
        "        current_info_contextualized = self.layernorm2(current_info_sa + self.dropout2(attn_output_cross, training=training))\n",
        "        ffn_output = self.ffn(current_info_contextualized)\n",
        "        final_output = self.layernorm3(current_info_contextualized + self.dropout3(ffn_output, training=training))\n",
        "        return final_output\n",
        "    def get_config(self):\n",
        "        config = super(ContextualTransformerBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(SelfAttentionBlock, self).__init__(**kwargs)\n",
        "        self.d_model, self.num_heads, self.dff, self.rate = d_model, num_heads, dff, rate\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1, self.layernorm2 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1, self.dropout2 = Dropout(rate), Dropout(rate)\n",
        "    def call(self, x, training=False, **kwargs):\n",
        "        attn_output = self.mha(query=x, key=x, value=x, training=training)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "    def get_config(self):\n",
        "        config = super(SelfAttentionBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class MainPredictor(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, baseline_window, num_metrics, rate=0.1, **kwargs):\n",
        "        super(MainPredictor, self).__init__(**kwargs)\n",
        "        self.num_layers, self.d_model, self.num_heads, self.dff = num_layers, d_model, num_heads, dff\n",
        "        self.baseline_window, self.num_metrics, self.rate = baseline_window, num_metrics, rate\n",
        "        self.first_block = ContextualTransformerBlock(d_model, num_heads, dff, rate)\n",
        "        self.other_blocks = [SelfAttentionBlock(d_model, num_heads, dff, rate) for _ in range(num_layers - 1)]\n",
        "        self.prediction_head = tf.keras.Sequential([\n",
        "            Dense(256, activation='relu', name=\"pred_head_dense1\"), Dropout(rate),\n",
        "            Dense(128, activation='relu', name=\"pred_head_dense2\"), Dropout(rate),\n",
        "            Dense(baseline_window * num_metrics, name=\"final_prediction\", dtype='float32'),\n",
        "            Reshape((baseline_window, num_metrics))\n",
        "        ], name=\"BASELINE_PREDICTION_HEAD\")\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        encoded_features, past_info, last_lco = inputs\n",
        "        current_info_length = 2 * DAY_MINUTES\n",
        "        current_info = encoded_features[:, -current_info_length:, :]\n",
        "        x = self.first_block((past_info, current_info), training=training)\n",
        "        for block in self.other_blocks: x = block(x, training=training)\n",
        "        pooled_vector, last_vector = tf.reduce_mean(x, axis=1), x[:, -1, :]\n",
        "        combined_final_vector = tf.concat([pooled_vector, last_vector, last_lco], axis=-1)\n",
        "        return self.prediction_head(combined_final_vector)\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"num_layers\": self.num_layers, \"d_model\": self.d_model, \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff, \"baseline_window\": self.baseline_window, \"num_metrics\": self.num_metrics, \"rate\": self.rate\n",
        "        }\n",
        "    @classmethod\n",
        "    def from_config(cls, config): return cls(**config)\n",
        "\n",
        "from mamba_tf import MambaLayer\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class IntegratedModel(Model):\n",
        "    \"\"\" [Mamba 최적화 버전] mamba-tf 라이브러리의 MambaLayer를 사용합니다. \"\"\"\n",
        "    def __init__(self, lco_feature_extractor, other_feature_extractor,\n",
        "                 fourier_correction_model, main_predictor, profile_embedding_model,\n",
        "                 config, **kwargs):\n",
        "        super(IntegratedModel, self).__init__(**kwargs)\n",
        "        self.lco_feature_extractor = lco_feature_extractor\n",
        "        self.other_feature_extractor = other_feature_extractor\n",
        "        self.fourier_correction_model = fourier_correction_model\n",
        "        self.main_predictor = main_predictor\n",
        "        self.profile_embedding_model = profile_embedding_model\n",
        "        self.config = config\n",
        "        self.fourier_layer = FourierTrajectoryLayer(config['num_harmonics'])\n",
        "        self.pos_encoding_layer = PositionalEncoding(config['input_seq_len'], config['d_model']) # 절대 시간\n",
        "        self.phase_pos_encoding_layer = PhaseAnglePositionalEncoding(config['d_model'])       # 위상각 기반\n",
        "        self.film_layer = FiLMLayer()\n",
        "        # self.fusion_layer = Dense(config['d_model'], activation='relu', name='pe_fusion_layer') # desnse layer\n",
        "\n",
        "        self.mamba_block = MambaLayer(\n",
        "            d_model=config['d_model'],\n",
        "            d_state=config['mamba_d_state'],\n",
        "            d_conv=config['mamba_d_conv'],\n",
        "            expand=config['mamba_expand']\n",
        "        )\n",
        "        self.highlight_projector = Dense(config['d_model'], name='highlight_projector', use_bias=False)\n",
        "        self.lambda_reg, self.lambda_cont, self.lambda_anchor = config['lambda_reg'], config['lambda_cont'], config['lambda_anchor']\n",
        "        self.return_trajectory = False\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        time_offset = inputs['time_offset']\n",
        "        batch_size = tf.shape(time_offset)[0]\n",
        "        num_main_seq_days = self.config['input_seq_len'] // DAY_MINUTES\n",
        "        lookback_minutes = self.config['lookback_days'] * DAY_MINUTES\n",
        "\n",
        "        correction_input_keys = ['corr_input_lux', 'corr_input_sleep', 'corr_input_body1', 'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3', 'corr_input_caffeine', 'corr_input_alcohol']\n",
        "        correction_inputs_list = [inputs[key] for key in correction_input_keys]\n",
        "        padding_minutes = (self.config['lookback_days'] - 1) * DAY_MINUTES\n",
        "        padded_correction_inputs = []\n",
        "        for data_tensor in correction_inputs_list:\n",
        "            paddings = tf.zeros([batch_size, padding_minutes, tf.shape(data_tensor)[-1]], dtype=data_tensor.dtype)\n",
        "            padded_correction_inputs.append(Concatenate(axis=1)([paddings, data_tensor]))\n",
        "        framed_inputs = [tf.signal.frame(data, frame_length=lookback_minutes, frame_step=DAY_MINUTES, axis=1) for data in padded_correction_inputs]\n",
        "        transposed_framed_inputs = [tf.transpose(f, [1, 0, 2, 3]) for f in framed_inputs]\n",
        "        def parallel_correction_fn(elems):\n",
        "            correction_params = self.fourier_correction_model(list(elems), training=training)\n",
        "            return self.fourier_layer(correction_params)\n",
        "        all_daily_curves = tf.map_fn(fn=parallel_correction_fn, elems=transposed_framed_inputs, fn_output_signature=tf.TensorSpec(shape=[None, DAY_MINUTES, 2], dtype=self.compute_dtype))\n",
        "        full_correction_block = tf.reshape(tf.transpose(all_daily_curves, [1, 0, 2, 3]), [batch_size, -1, 2])\n",
        "        full_corrected_trajectory = inputs['baseline_inputs'] + tf.cast(full_correction_block, inputs['baseline_inputs'].dtype)\n",
        "\n",
        "        offsets = tf.cast(time_offset, dtype=tf.int32)\n",
        "        time_indices = tf.range(self.config['input_seq_len'], dtype=tf.int32)[tf.newaxis, :] + offsets\n",
        "        final_delta_trajectory = tf.gather(full_correction_block, time_indices, batch_dims=1)\n",
        "        aligned_baseline = tf.gather(inputs['baseline_inputs'], time_indices, batch_dims=1)\n",
        "        other_input_keys = ['input_lux', 'input_sleep', 'input_body1', 'input_body2', 'input_zeit1', 'input_zeit2', 'input_zeit3', 'input_caffeine', 'input_alcohol']\n",
        "        main_cnn_inputs_other_list = [tf.gather(inputs[key], time_indices, batch_dims=1) for key in other_input_keys]\n",
        "        corrected_lco_trajectory = tf.cast(aligned_baseline + final_delta_trajectory, self.compute_dtype)\n",
        "\n",
        "        lco_features = self.lco_feature_extractor(corrected_lco_trajectory, training=training)\n",
        "        other_features = self.other_feature_extractor(main_cnn_inputs_other_list, training=training)\n",
        "        combined_features = Concatenate(axis=-1, dtype='float32')([lco_features, other_features])\n",
        "\n",
        "        profile_vector = inputs['profile_vector']\n",
        "        profile_embedding = self.profile_embedding_model(profile_vector, training=training)\n",
        "        modulated_features = self.film_layer([tf.cast(combined_features, self.compute_dtype), profile_embedding])\n",
        "\n",
        "        pe_absolute = self.pos_encoding_layer.pos_encoding[:, :tf.shape(modulated_features)[1], :]\n",
        "        pe_phase = self.phase_pos_encoding_layer(corrected_lco_trajectory)\n",
        "        encoded_features = modulated_features + tf.cast(pe_absolute, modulated_features.dtype) + pe_phase\n",
        "\n",
        "        # --- 위상 인코딩: 연결 후 투영 ---\n",
        "        # pe_absolute = self.pos_encoding_layer.pos_encoding[:, :tf.shape(modulated_features)[1], :]\n",
        "        # pe_phase = self.phase_pos_encoding_layer(corrected_lco_trajectory)\n",
        "        # combined_pe_features = tf.concat([\n",
        "        #     modulated_features,\n",
        "        #     tf.cast(pe_absolute, modulated_features.dtype),\n",
        "        #     pe_phase\n",
        "        # ], axis=-1)\n",
        "        # encoded_features = self.fusion_layer(combined_pe_features)\n",
        "\n",
        "        current_info_length = 2 * DAY_MINUTES\n",
        "        past_sequence_length = self.config['input_seq_len'] - current_info_length\n",
        "        past_sequence = encoded_features[:, :past_sequence_length, :]\n",
        "\n",
        "        past_info_indices = inputs['past_info_indices']\n",
        "        valid_mask = (past_info_indices >= 0) & (past_info_indices < past_sequence_length)\n",
        "        masked_indices = tf.where(valid_mask, past_info_indices, -1)\n",
        "        one_hot_vectors = tf.one_hot(tf.cast(masked_indices, tf.int32), depth=past_sequence_length, on_value=1.0, off_value=0.0)\n",
        "        highlight_feature = tf.reduce_sum(one_hot_vectors, axis=1)\n",
        "        highlight_feature = tf.expand_dims(highlight_feature, axis=-1)\n",
        "        highlight_feature = tf.cast(highlight_feature, dtype=past_sequence.dtype)\n",
        "\n",
        "        projected_highlight = self.highlight_projector(highlight_feature)\n",
        "\n",
        "        mamba_input_sequence = past_sequence + projected_highlight\n",
        "        mamba_output = self.mamba_block(mamba_input_sequence)\n",
        "        mamba_memory_state = mamba_output[:, -1:, :]\n",
        "\n",
        "        past_info = mamba_memory_state\n",
        "        last_lco_corrected = corrected_lco_trajectory[:, -1, :]\n",
        "        y_pred_delta = self.main_predictor((encoded_features, past_info, last_lco_corrected), training=training)\n",
        "\n",
        "        if self.return_trajectory:\n",
        "            return y_pred_delta, full_corrected_trajectory, inputs['baseline_inputs']\n",
        "        if training:\n",
        "            self.add_loss(tf.reduce_mean(tf.square(tf.cast(all_daily_curves, tf.float32))) * self.lambda_reg)\n",
        "            full_corrected_reshaped = tf.reshape(full_corrected_trajectory, [batch_size, -1, DAY_MINUTES, 2])\n",
        "            continuity_gaps = tf.cast(full_corrected_reshaped, tf.float32)[:, :-1, -1, :] - tf.cast(full_corrected_reshaped, tf.float32)[:, 1:, 0, :]\n",
        "            self.add_loss(tf.reduce_mean(tf.square(continuity_gaps)) * self.lambda_cont)\n",
        "            last_day_corrected_x = tf.reshape(tf.cast(corrected_lco_trajectory, tf.float32), [batch_size, num_main_seq_days, DAY_MINUTES, 2])[:, -1, :, 0]\n",
        "            predicted_cbt_nadir_idx = tf.cast(tf.argmin(last_day_corrected_x, axis=1), tf.float32)\n",
        "            hr_series, sleep_series = tf.cast(inputs['full_inputs_for_anchor'], tf.float32)[:, :, 0], tf.cast(inputs['full_inputs_for_anchor'], tf.float32)[:, :, 1]\n",
        "            sleep_mask = tf.cast(sleep_series > 0.5, tf.float32)\n",
        "            has_sleep = tf.reduce_any(tf.cast(sleep_mask, tf.bool), axis=1)\n",
        "            hr_in_sleep = hr_series * sleep_mask + (1.0 - sleep_mask) * 1e9\n",
        "            nadir_in_sleep_indices = tf.cast(tf.argmin(hr_in_sleep, axis=1), tf.float32)\n",
        "            nadir_overall_indices = tf.cast(tf.argmin(hr_series, axis=1), tf.float32)\n",
        "            actual_hr_nadir_idx = tf.where(has_sleep, nadir_in_sleep_indices, nadir_overall_indices)\n",
        "            actual_cbt_nadir_idx = (actual_hr_nadir_idx + 120) % DAY_MINUTES\n",
        "            anchor_loss_val = tf.reduce_mean(tf.square(predicted_cbt_nadir_idx - actual_cbt_nadir_idx)) / (DAY_MINUTES**2)\n",
        "            self.add_loss(anchor_loss_val * self.lambda_anchor)\n",
        "        if not training and 'reference_baseline' in inputs:\n",
        "            return inputs['reference_baseline'] + tf.cast(y_pred_delta, inputs['reference_baseline'].dtype)\n",
        "        return y_pred_delta\n",
        "\n",
        "    def get_config(self): return {\"config\": self.config}\n",
        "    @classmethod\n",
        "    def from_config(cls, config_data, custom_objects=None): return cls(**config_data)\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 기준선 예측을 위한 TFRecord 생성\n",
        "# =============================================================================\n",
        "def find_wake_up_time(day_data):\n",
        "    is_sleeping = day_data['is_sleeping'].values\n",
        "    for i in range(1, len(is_sleeping)):\n",
        "        if is_sleeping[i-1] == 1 and is_sleeping[i] == 0: return i\n",
        "    return None\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def serialize_example(inputs_dict):\n",
        "    feature = {key: _bytes_feature(tf.io.serialize_tensor(value).numpy()) for key, value in inputs_dict.items()}\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "\n",
        "def moving_average_np(a, n=3):\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "def precompute_all_markers_for_person(person_df):\n",
        "    calculation_df = person_df.assign(corrected_skin_temp=person_df['skin_temp'] - person_df['ambient_temp'])\n",
        "    heart_rate, is_sleeping, temp_series = calculation_df['heart_rate'].values, calculation_df['is_sleeping'].values, calculation_df['corrected_skin_temp'].values\n",
        "    hr_in_sleep = np.where(is_sleeping > 0.5, heart_rate, np.inf)\n",
        "    nadir_indices_abs = [i + np.argmin(hr_in_sleep[i:i+60]) for i in range(0, len(calculation_df) - 60, 60) if np.any(np.isfinite(hr_in_sleep[i:i+60]))]\n",
        "    if len(temp_series) > 40:\n",
        "        temp_deriv_smoothed = moving_average_np(np.gradient(moving_average_np(temp_series, 30)), 10)\n",
        "        offset_to_align = len(temp_series) - len(temp_deriv_smoothed)\n",
        "        onset_indices_abs = list(np.where(temp_deriv_smoothed > 0.001)[0] + offset_to_align)\n",
        "        offset_indices_abs = list(np.where(temp_deriv_smoothed < -0.001)[0] + offset_to_align)\n",
        "    else:\n",
        "        onset_indices_abs, offset_indices_abs = [], []\n",
        "    return {\"onset\": sorted(list(set(onset_indices_abs))), \"nadir\": sorted(list(set(nadir_indices_abs))), \"offset\": sorted(list(set(offset_indices_abs)))}\n",
        "\n",
        "def generate_past_info_indices_optimized(precomputed_markers, selection_start_abs, selection_end_abs, main_seq_start_in_full_df, config):\n",
        "    num_markers_to_keep, padding_value = config['num_markers_to_keep'], -1\n",
        "    final_indices_relative = []\n",
        "    for marker_type in [\"onset\", \"nadir\", \"offset\"]:\n",
        "        marker_candidates_abs = precomputed_markers[marker_type]\n",
        "        valid_candidates = marker_candidates_abs[bisect.bisect_left(marker_candidates_abs, selection_start_abs):bisect.bisect_right(marker_candidates_abs, selection_end_abs)]\n",
        "        if len(valid_candidates) > 0:\n",
        "            top_indices_abs = valid_candidates[-num_markers_to_keep:]\n",
        "            top_indices_relative = [idx - main_seq_start_in_full_df for idx in top_indices_abs]\n",
        "            final_indices_relative.extend(reversed(top_indices_relative))\n",
        "            if len(top_indices_relative) < num_markers_to_keep: final_indices_relative.extend([padding_value] * (num_markers_to_keep - len(top_indices_relative)))\n",
        "        else: final_indices_relative.extend([padding_value] * num_markers_to_keep)\n",
        "    return np.array(final_indices_relative, dtype=np.int64)\n",
        "\n",
        "def create_baseline_tfrecords(df_all, df_all_scaled, baseline_lookup, indices, config,\n",
        "                             file_path, person_markers_dict, person_profiles_dict):\n",
        "    print(f\"--- 기준선 TFRecord 파일 생성 시작: {file_path} ---\")\n",
        "    main_cnn_other_cols = {\n",
        "        'input_lux': ['lux', 'light_color_temp'], 'input_sleep': ['is_sleeping'], 'input_body1': ['heart_rate', 'hrv', 'respiration_rate'],\n",
        "        'input_body2': ['skin_temp'], 'input_zeit1': ['meal_event', 'meal_calories', 'meal_type_balanced', 'meal_type_high_protein', 'meal_type_high_fat', 'meal_type_high_carb'],\n",
        "        'input_zeit2': ['exercise_event', 'exercise_type_aerobic', 'exercise_type_anaerobic', 'exercise_intensity_low', 'exercise_intensity_medium', 'exercise_intensity_high'],\n",
        "        'input_zeit3': ['ambient_temp'], 'input_caffeine': ['caffeine_event', 'caffeine_g'], 'input_alcohol': ['alcohol_event', 'alcohol_g']\n",
        "    }\n",
        "    fourier_corr_cols = {k.replace('input', 'corr_input'): v for k, v in main_cnn_other_cols.items()}\n",
        "    anchor_loss_cols = ['heart_rate', 'is_sleeping']\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "    df_unscaled_np, df_scaled_np = df_all.to_numpy(), df_all_scaled.to_numpy()\n",
        "    unscaled_cols_map, scaled_cols_map = {n: i for i, n in enumerate(df_all.columns)}, {n: i for i, n in enumerate(df_all_scaled.columns)}\n",
        "    def get_indices(cols, col_map): return [col_map[c] for c in cols]\n",
        "    fourier_corr_indices = {k: get_indices(v, scaled_cols_map) for k, v in fourier_corr_cols.items()}\n",
        "    main_cnn_other_indices = {k: get_indices(v, scaled_cols_map) for k, v in main_cnn_other_cols.items()}\n",
        "    anchor_loss_indices, baseline_indices = get_indices(anchor_loss_cols, unscaled_cols_map), get_indices(['x_base', 'xc_base'], scaled_cols_map)\n",
        "    with tf.io.TFRecordWriter(file_path) as writer:\n",
        "        for sample_info in tqdm(indices, desc=f\"{os.path.basename(file_path)} 생성 중\"):\n",
        "            person_id, wake_up_idx = sample_info['person_id'], sample_info['wake_up_idx']\n",
        "            main_seq_start_idx = wake_up_idx - config['input_seq_len']\n",
        "            if main_seq_start_idx < 0: continue\n",
        "            time_offset = main_seq_start_idx % DAY_MINUTES\n",
        "            slice_start_midnight = main_seq_start_idx - time_offset\n",
        "            slice_end_midnight = slice_start_midnight + (config['lookback_days'] + num_main_seq_days) * DAY_MINUTES\n",
        "            if slice_end_midnight > len(df_unscaled_np): continue\n",
        "            example_dict = {}\n",
        "            for key, col_idxs in fourier_corr_indices.items(): example_dict[key] = tf.constant(df_scaled_np[slice_start_midnight:slice_end_midnight, col_idxs], dtype=tf.float32)\n",
        "            for key, col_idxs in main_cnn_other_indices.items(): example_dict[key] = tf.constant(df_scaled_np[slice_start_midnight:slice_end_midnight, col_idxs], dtype=tf.float32)\n",
        "            example_dict['baseline_inputs'] = tf.constant(df_scaled_np[slice_start_midnight:slice_end_midnight, baseline_indices], dtype=tf.float32)\n",
        "            example_dict['time_offset'] = tf.constant([time_offset], dtype=tf.int32)\n",
        "            anchor_start, anchor_end = main_seq_start_idx + config['input_seq_len'] - DAY_MINUTES, main_seq_start_idx + config['input_seq_len']\n",
        "            example_dict['full_inputs_for_anchor'] = tf.constant(df_unscaled_np[anchor_start:anchor_end, anchor_loss_indices], dtype=tf.float32)\n",
        "            selection_start_abs = main_seq_start_idx\n",
        "            past_sequence_end_abs = main_seq_start_idx + (config['input_seq_len'] - 2 * DAY_MINUTES)\n",
        "            selection_end_abs = past_sequence_end_abs + config.get('MARKER_SELECTION_EXTENSION_MINUTES', 0) # config에 없으면 0을 기본값으로 사용\n",
        "            example_dict['past_info_indices'] = generate_past_info_indices_optimized(person_markers_dict[person_id], selection_start_abs, selection_end_abs, main_seq_start_idx, config)\n",
        "            example_dict['reference_baseline'] = tf.constant(sample_info['y_yesterday'], dtype=tf.float32)\n",
        "            example_dict['label'] = tf.constant(sample_info['y_delta'], dtype=tf.float32)\n",
        "            example_dict['profile_vector'] = tf.constant(person_profiles_dict[person_id], dtype=tf.float32)\n",
        "            writer.write(serialize_example(example_dict))\n",
        "    print(f\"--- 기준선 TFRecord 파일 생성 완료: {file_path} ---\")\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 데이터 전처리 및 학습 파이프라인\n",
        "# =============================================================================\n",
        "def get_stable_limit_cycle(params, output_dir, force_recalculate=False):\n",
        "    cache_path = os.path.join(output_dir, \"stable_limit_cycle.npy\")\n",
        "    if os.path.exists(cache_path) and not force_recalculate: return np.load(cache_path)\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "    tqdm.write(\"안정 궤도를 새로 계산합니다...\")\n",
        "    standard_sleep, standard_light = np.zeros(DAY_MINUTES), np.zeros(DAY_MINUTES)\n",
        "    standard_sleep[0:8*60] = 1; standard_light[8*60:10*60] = 150; standard_light[10*60:18*60] = 400; standard_light[18*60:22*60] = 150; standard_light[22*60:24*60] = 50\n",
        "    light_func = lambda t: standard_light[int((t % 24) * 60)]; sleep_func = lambda t: standard_sleep[int((t % 24) * 60)]\n",
        "    sol = solve_ivp(fun=lco_model_ode, t_span=[0, 10 * 24], y0=[1.0, 0.0, 0.5], method='BDF', args=(params, light_func, sleep_func), dense_output=True, rtol=1e-6, atol=1e-9)\n",
        "    if not sol.success: raise RuntimeError(f\"안정 궤도 계산 실패: {sol.message}\")\n",
        "    limit_cycle_map = sol.sol(np.arange(9 * 24, 10 * 24, 1.0/60.0)).T\n",
        "    np.save(cache_path, limit_cycle_map)\n",
        "    tqdm.write(f\"새로운 안정 궤도를 저장했습니다: {cache_path}\")\n",
        "    return limit_cycle_map\n",
        "\n",
        "def generate_daily_baseline_trajectories_for_person(person_df, params, output_dir, pbar=None):\n",
        "    total_minutes, num_days = len(person_df), len(person_df) // DAY_MINUTES\n",
        "    limit_cycle_map = get_stable_limit_cycle(params, output_dir)\n",
        "    map_phases = np.arctan2(limit_cycle_map[:, 1], limit_cycle_map[:, 0])\n",
        "    final_trajectory = np.zeros((total_minutes, 3))\n",
        "    last_known_anchor_minute, theoretical_anchor_minute = -1, 4 * 60\n",
        "    t_eval_day_hours = np.arange(DAY_MINUTES) / 60.0\n",
        "    for day in range(num_days):\n",
        "        if pbar: pbar.set_postfix_str(f\"일일 궤도 생성 중... ({day + 1}/{num_days}일)\")\n",
        "        day_start_idx, day_end_idx = day * DAY_MINUTES, (day + 1) * DAY_MINUTES\n",
        "        day_df = person_df.iloc[day_start_idx:day_end_idx]\n",
        "        day_hr, day_sleep = day_df['heart_rate'].values, day_df['is_sleeping'].values\n",
        "        sleep_indices = np.where(day_sleep > 0.5)[0]\n",
        "        current_anchor_minute = sleep_indices[np.argmin(day_hr[sleep_indices])] if len(sleep_indices) > 0 else -1\n",
        "        if current_anchor_minute != -1: last_known_anchor_minute = current_anchor_minute\n",
        "        anchor_to_use = last_known_anchor_minute if last_known_anchor_minute != -1 else theoretical_anchor_minute\n",
        "        cbt_nadir_minute_in_day = (anchor_to_use + 120) % DAY_MINUTES\n",
        "        initial_phase_at_midnight = ((-170.7 * np.pi / 180.0) - (cbt_nadir_minute_in_day * (2 * np.pi) / DAY_MINUTES) + np.pi) % (2 * np.pi) - np.pi\n",
        "        y0 = limit_cycle_map[np.argmin(np.abs((map_phases - initial_phase_at_midnight + np.pi) % (2 * np.pi) - np.pi))]\n",
        "        day_df_smoothed = pd.DataFrame(index=day_df.index)\n",
        "        day_df_smoothed['lux_smoothed'] = day_df['integrated_lux'].rolling(window=15, min_periods=1, center=True).mean()\n",
        "        day_df_smoothed['sleep_smoothed'] = day_df['is_sleeping'].rolling(window=15, min_periods=1, center=True).mean()\n",
        "        light_func = interp1d(t_eval_day_hours, day_df_smoothed['lux_smoothed'].values, kind='linear', fill_value=\"extrapolate\")\n",
        "        sleep_func = interp1d(t_eval_day_hours, day_df_smoothed['sleep_smoothed'].values, kind='linear', fill_value=\"extrapolate\")\n",
        "        sol_day = solve_ivp(fun=lco_model_ode, t_span=[0, 24], y0=y0, method='BDF', jac=lco_model_jacobian, args=(params, light_func, sleep_func), dense_output=True, t_eval=t_eval_day_hours, rtol=1e-5, atol=1e-8)\n",
        "        if sol_day.success and sol_day.y.shape[1] == DAY_MINUTES: final_trajectory[day_start_idx:day_end_idx, :] = sol_day.y.T\n",
        "        elif day > 0: final_trajectory[day_start_idx:day_end_idx, :] = final_trajectory[(day-1)*DAY_MINUTES:day*DAY_MINUTES, :]\n",
        "    if total_minutes > num_days * DAY_MINUTES and num_days > 0:\n",
        "        remaining_start, remaining_len = num_days * DAY_MINUTES, total_minutes - num_days * DAY_MINUTES\n",
        "        final_trajectory[remaining_start:, :] = final_trajectory[(num_days-1)*DAY_MINUTES : (num_days-1)*DAY_MINUTES+remaining_len, :]\n",
        "    return final_trajectory\n",
        "\n",
        "def prepare_baseline_samples(df_all, baseline_lookup, person_ids, config):\n",
        "    print(\"--- 기준선 예측 샘플 준비 시작 ---\")\n",
        "    samples = []\n",
        "    for person_id in tqdm(person_ids, desc=\"샘플 생성 중\"):\n",
        "        person_data = df_all[df_all['person_id'] == person_id].copy().sort_values('timestamp').reset_index(drop=True)\n",
        "        for i, date in enumerate(person_data['timestamp'].dt.date.unique()[1:], 1):\n",
        "            today_mask = person_data['timestamp'].dt.date == date\n",
        "            if not np.any(today_mask): continue\n",
        "            today_indices = person_data.index[today_mask]\n",
        "            wake_up_idx_in_day = find_wake_up_time(person_data.loc[today_indices])\n",
        "            if wake_up_idx_in_day is None: continue\n",
        "            wake_up_idx = today_indices[wake_up_idx_in_day]\n",
        "            if (person_id, str(date)) not in baseline_lookup: continue\n",
        "            y_today = baseline_lookup[(person_id, str(date))]\n",
        "            y_yesterday = None\n",
        "            for days_back in range(1, 8):\n",
        "                prev_date = date - pd.Timedelta(days=days_back)\n",
        "                if (person_id, str(prev_date)) in baseline_lookup:\n",
        "                    y_yesterday = baseline_lookup[(person_id, str(prev_date))]\n",
        "                    break\n",
        "            if y_yesterday is None: continue\n",
        "            if wake_up_idx < config['input_seq_len']: continue\n",
        "            samples.append({'person_id': person_id, 'wake_up_idx': wake_up_idx, 'y_delta': y_today - y_yesterday, 'y_yesterday': y_yesterday, 'date': date})\n",
        "    print(f\"--- 총 {len(samples)}개의 샘플 생성 완료 ---\")\n",
        "    return samples\n",
        "\n",
        "def train_and_evaluate_baseline(train_tfrecord, val_tfrecord, num_train_samples, num_val_samples, config):\n",
        "    print(\"\\n--- 기준선 예측 모델 학습 및 검증 시작 ---\")\n",
        "    main_cnn_other_keys = ['input_lux', 'input_sleep', 'input_body1', 'input_body2', 'input_zeit1', 'input_zeit2', 'input_zeit3', 'input_caffeine', 'input_alcohol']\n",
        "    fourier_corr_keys = ['corr_input_lux', 'corr_input_sleep', 'corr_input_body1', 'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3', 'corr_input_caffeine', 'corr_input_alcohol']\n",
        "    other_keys = ['baseline_inputs', 'full_inputs_for_anchor', 'past_info_indices', 'label', 'time_offset', 'reference_baseline', 'profile_vector']\n",
        "    feature_spec = {key: tf.io.FixedLenFeature([], tf.string) for key in main_cnn_other_keys + fourier_corr_keys + other_keys}\n",
        "\n",
        "    def _parse_function(example_proto):\n",
        "        parsed = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "        inputs = {}\n",
        "        for key in main_cnn_other_keys + fourier_corr_keys: inputs[key] = tf.io.parse_tensor(parsed[key], out_type=tf.float32)\n",
        "        inputs['baseline_inputs'] = tf.io.parse_tensor(parsed['baseline_inputs'], out_type=tf.float32)\n",
        "        inputs['full_inputs_for_anchor'] = tf.io.parse_tensor(parsed['full_inputs_for_anchor'], out_type=tf.float32)\n",
        "        inputs['past_info_indices'] = tf.io.parse_tensor(parsed['past_info_indices'], out_type=tf.int64)\n",
        "        inputs['time_offset'] = tf.io.parse_tensor(parsed['time_offset'], out_type=tf.int32)\n",
        "        inputs['reference_baseline'] = tf.io.parse_tensor(parsed['reference_baseline'], out_type=tf.float32)\n",
        "        inputs['profile_vector'] = tf.io.parse_tensor(parsed['profile_vector'], out_type=tf.float32)\n",
        "        label = tf.io.parse_tensor(parsed['label'], out_type=tf.float32)\n",
        "        return inputs, label\n",
        "\n",
        "    def make_dataset(file_path):\n",
        "        return tf.data.TFRecordDataset(file_path, num_parallel_reads=tf.data.AUTOTUNE).map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE).shuffle(256).batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "    train_dataset, val_dataset = make_dataset(train_tfrecord), make_dataset(val_tfrecord)\n",
        "    vis_data_x, _ = next(iter(val_dataset))\n",
        "\n",
        "    lco_feat, other_feat = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "    fourier_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "    predictor = MainPredictor(config['num_layers'], config['d_model'], config['num_heads'], config['dff'], config['baseline_window'], config['num_metrics'], config['rate'])\n",
        "    profile_embed_model = build_profile_embedding_model(profile_vector_dim=3, embedding_dim=config['profile_embedding_dim'])\n",
        "    integrated_model = IntegratedModel(lco_feat, other_feat, fourier_model, predictor, profile_embed_model, config)\n",
        "\n",
        "    optimizer = mixed_precision.LossScaleOptimizer(Adam(learning_rate=config['learning_rate']))\n",
        "    mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, y_delta):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_delta = integrated_model(x, training=True)\n",
        "            total_loss = mse_loss_fn(y_delta, y_pred_delta) + sum(integrated_model.losses)\n",
        "        gradients = tape.gradient(total_loss, integrated_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, integrated_model.trainable_variables))\n",
        "        return total_loss\n",
        "\n",
        "    @tf.function\n",
        "    def val_step(x, y_delta):\n",
        "        y_pred_final = integrated_model(x, training=False)\n",
        "        y_pred_delta_recalculated = y_pred_final - tf.cast(x['reference_baseline'], y_pred_final.dtype)\n",
        "        return mse_loss_fn(y_delta, y_pred_delta_recalculated)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    best_val_loss = float('inf')\n",
        "    model_save_path = os.path.join(OUTPUT_DIR, \"best_baseline_model.weights.h5\")\n",
        "    for epoch in range(config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "        progbar = tf.keras.utils.Progbar(num_train_samples // config['batch_size'], stateful_metrics=['train_loss'])\n",
        "        epoch_train_losses = []\n",
        "        for i, (x, y) in enumerate(train_dataset):\n",
        "            loss = train_step(x, y)\n",
        "            progbar.update(i + 1, values=[('train_loss', loss)])\n",
        "            epoch_train_losses.append(loss)\n",
        "        history['train_loss'].append(np.mean(epoch_train_losses))\n",
        "        val_loss = tf.reduce_mean([val_step(x, y) for x, y in val_dataset])\n",
        "        history['val_loss'].append(val_loss.numpy())\n",
        "        print(f\"\\nValidation Loss: {val_loss.numpy():.4f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            print(f\"Validation loss improved. Saving model weights to {model_save_path}\")\n",
        "            best_val_loss = val_loss\n",
        "            integrated_model.save_weights(model_save_path)\n",
        "        print(\"\\nEpoch 종료. LCO 보정 궤적 시각화 생성 중...\")\n",
        "        integrated_model.return_trajectory = True\n",
        "        _, corrected_traj, baseline_traj = integrated_model(vis_data_x, training=False)\n",
        "        integrated_model.return_trajectory = False\n",
        "        plot_lco_comparison(epoch, baseline_traj[0].numpy(), corrected_traj[0].numpy(), config, OUTPUT_DIR)\n",
        "    print(\"\\n--- 학습 및 검증 완료 ---\")\n",
        "    return integrated_model, val_dataset, history\n",
        "\n",
        "# =============================================================================\n",
        "# 7. 예측 및 시각화\n",
        "# =============================================================================\n",
        "def plot_lco_comparison(epoch, baseline_traj, corrected_traj, config, output_dir):\n",
        "    total_minutes, total_days = corrected_traj.shape[0], corrected_traj.shape[0] // DAY_MINUTES\n",
        "    days_per_chunk, num_chunks = 3, math.ceil(total_days / 3)\n",
        "    for i in range(num_chunks):\n",
        "        start_day, end_day = i * days_per_chunk, min((i + 1) * days_per_chunk, total_days)\n",
        "        if start_day >= end_day: continue\n",
        "        chunk_days = end_day - start_day\n",
        "        fig = plt.figure(figsize=(8 * chunk_days, 12)); gs = fig.add_gridspec(2, chunk_days, height_ratios=[2, 1])\n",
        "        ax_ts = fig.add_subplot(gs[0, :]); start_idx, end_idx = start_day * DAY_MINUTES, end_day * DAY_MINUTES\n",
        "        time_axis = np.arange(start_idx, end_idx)\n",
        "        ax_ts.plot(time_axis, baseline_traj[start_idx:end_idx, 0], 'r--', alpha=0.7, label='Baseline x')\n",
        "        ax_ts.plot(time_axis, baseline_traj[start_idx:end_idx, 1], 'b--', alpha=0.7, label='Baseline xc')\n",
        "        ax_ts.plot(time_axis, corrected_traj[start_idx:end_idx, 0], 'r-', lw=2, label='Corrected x (Model Output)')\n",
        "        ax_ts.plot(time_axis, corrected_traj[start_idx:end_idx, 1], 'b-', lw=2, label='Corrected xc (Model Output)')\n",
        "        ax_ts.set_title(f'Epoch {epoch+1}: LCO Trajectory Comparison (Days {start_day+1}-{end_day})', fontsize=16)\n",
        "        ax_ts.set_xlabel('Time (minutes from start)'); ax_ts.set_ylabel('State Value'); ax_ts.legend(); ax_ts.grid(True)\n",
        "        for d_idx in range(chunk_days):\n",
        "            ax_phase = fig.add_subplot(gs[1, d_idx]); day_num = start_day + d_idx\n",
        "            day_start_idx_local, day_end_idx_local = day_num * DAY_MINUTES, (day_num + 1) * DAY_MINUTES\n",
        "            x_vals, xc_vals = corrected_traj[day_start_idx_local:day_end_idx_local, 0], corrected_traj[day_start_idx_local:day_end_idx_local, 1]\n",
        "            ax_phase.plot(x_vals, xc_vals, color='purple', label=f'Day {day_num+1} Cycle')\n",
        "            ax_phase.scatter(x_vals[0], xc_vals[0], marker='o', color='g', s=100, zorder=5, label='Start')\n",
        "            ax_phase.scatter(x_vals[-1], xc_vals[-1], marker='X', color='r', s=100, zorder=5, label='End')\n",
        "            ax_phase.set_title(f'Day {day_num+1} Phase Cycle'); ax_phase.set_xlabel('x'); ax_phase.set_ylabel('xc')\n",
        "            ax_phase.grid(True); ax_phase.set_aspect('equal', adjustable='box'); ax_phase.legend()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.savefig(os.path.join(output_dir, f'lco_comparison_epoch_{epoch+1}_days_{start_day+1}-{end_day}.png')); plt.close(fig)\n",
        "    print(f\"LCO 궤도 비교 그래프가 {output_dir}에 저장되었습니다.\")\n",
        "\n",
        "def plot_prediction_day_lco(baseline_traj, corrected_traj, prediction_date, output_dir):\n",
        "    print(\"예측일의 LCO 궤도 비교 그래프 생성 중...\")\n",
        "    baseline_day_traj, corrected_day_traj = baseline_traj[-DAY_MINUTES:], corrected_traj[-DAY_MINUTES:]\n",
        "    time_axis = np.arange(DAY_MINUTES)\n",
        "    fig = plt.figure(figsize=(18, 8)); gs = fig.add_gridspec(1, 2, width_ratios=[2, 1])\n",
        "    fig.suptitle(f'LCO Trajectory for Prediction Date: {prediction_date}', fontsize=16)\n",
        "    ax_ts = fig.add_subplot(gs[0, 0])\n",
        "    ax_ts.plot(time_axis, baseline_day_traj[:, 0], 'r--', alpha=0.8, label='Baseline x (Before)')\n",
        "    ax_ts.plot(time_axis, baseline_day_traj[:, 1], 'b--', alpha=0.8, label='Baseline xc (Before)')\n",
        "    ax_ts.plot(time_axis, corrected_day_traj[:, 0], 'r-', lw=2, label='Corrected x (After)')\n",
        "    ax_ts.plot(time_axis, corrected_day_traj[:, 1], 'b-', lw=2, label='Corrected xc (After)')\n",
        "    ax_ts.set_title('Time-Series Trajectory'); ax_ts.set_xlabel('Time (minutes in day)'); ax_ts.set_ylabel('State Value'); ax_ts.legend(); ax_ts.grid(True)\n",
        "    ax_phase = fig.add_subplot(gs[0, 1])\n",
        "    ax_phase.plot(baseline_day_traj[:, 0], baseline_day_traj[:, 1], 'gray', ls='--', label='Baseline Cycle')\n",
        "    ax_phase.plot(corrected_day_traj[:, 0], corrected_day_traj[:, 1], 'purple', lw=2, label='Corrected Cycle')\n",
        "    ax_phase.scatter(corrected_day_traj[0, 0], corrected_day_traj[0, 1], marker='o', color='g', s=120, zorder=5, label='Start')\n",
        "    ax_phase.scatter(corrected_day_traj[-1, 0], corrected_day_traj[-1, 1], marker='X', color='r', s=120, zorder=5, label='End')\n",
        "    ax_phase.set_title('Phase Plane Cycle'); ax_phase.set_xlabel('x'); ax_phase.set_ylabel('xc'); ax_phase.grid(True); ax_phase.set_aspect('equal', adjustable='box'); ax_phase.legend()\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    save_path = os.path.join(output_dir, f'prediction_day_lco_comparison_{prediction_date}.png')\n",
        "    plt.savefig(save_path); plt.show(); plt.close(fig)\n",
        "    print(f\"LCO 궤도 비교 그래프가 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "def predict_baseline_for_today(model, person_data, wake_up_idx, yesterday_baseline,\n",
        "                              person_markers, person_profile_vector, config):\n",
        "    print(\"\\n--- 오늘의 기준선 예측 시작 ---\")\n",
        "    input_start_idx = wake_up_idx - config['input_seq_len']\n",
        "    if input_start_idx < 0: raise ValueError(\"입력 시퀀스를 위한 충분한 과거 데이터가 없습니다.\")\n",
        "    time_offset = input_start_idx % DAY_MINUTES\n",
        "    slice_start_midnight = input_start_idx - time_offset\n",
        "    total_days_needed = config['lookback_days'] + config['input_seq_len'] // DAY_MINUTES\n",
        "    slice_end_midnight = slice_start_midnight + total_days_needed * DAY_MINUTES\n",
        "    if slice_end_midnight > len(person_data): raise ValueError(\"전체 슬라이스를 위한 충분한 데이터가 없습니다.\")\n",
        "    full_slice = person_data.iloc[slice_start_midnight:slice_end_midnight]\n",
        "    feature_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "    feature_cols = [c for c in full_slice.columns if c not in ['x', 'xc', 'person_id', 'timestamp']]\n",
        "    full_slice_scaled = full_slice.copy()\n",
        "    full_slice_scaled[feature_cols] = feature_scaler.transform(full_slice[feature_cols])\n",
        "    model_input = {}\n",
        "    main_cnn_other_cols = {\n",
        "        'input_lux': ['lux', 'light_color_temp'], 'input_sleep': ['is_sleeping'], 'input_body1': ['heart_rate', 'hrv', 'respiration_rate'],\n",
        "        'input_body2': ['skin_temp'], 'input_zeit1': ['meal_event', 'meal_calories', 'meal_type_balanced', 'meal_type_high_protein', 'meal_type_high_fat', 'meal_type_high_carb'],\n",
        "        'input_zeit2': ['exercise_event', 'exercise_type_aerobic', 'exercise_type_anaerobic', 'exercise_intensity_low', 'exercise_intensity_medium', 'exercise_intensity_high'],\n",
        "        'input_zeit3': ['ambient_temp'], 'input_caffeine': ['caffeine_event', 'caffeine_g'], 'input_alcohol': ['alcohol_event', 'alcohol_g']\n",
        "    }\n",
        "    fourier_corr_cols = {k.replace('input', 'corr_input'): v for k, v in main_cnn_other_cols.items()}\n",
        "    for key, cols in main_cnn_other_cols.items(): model_input[key] = tf.constant(full_slice_scaled[cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    for key, cols in fourier_corr_cols.items(): model_input[key] = tf.constant(full_slice_scaled[cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    model_input['baseline_inputs'] = tf.constant(full_slice_scaled[['x_base', 'xc_base']].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    model_input['time_offset'] = tf.constant([[time_offset]], dtype=tf.int32)\n",
        "    anchor_start, anchor_end = input_start_idx + config['input_seq_len'] - DAY_MINUTES, input_start_idx + config['input_seq_len']\n",
        "    model_input['full_inputs_for_anchor'] = tf.constant(person_data.iloc[anchor_start:anchor_end][['heart_rate', 'is_sleeping']].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    selection_start_abs = input_start_idx\n",
        "    past_sequence_end_abs = input_start_idx + (config['input_seq_len'] - 2 * DAY_MINUTES)\n",
        "    selection_end_abs = past_sequence_end_abs + config.get('MARKER_SELECTION_EXTENSION_MINUTES', 0)\n",
        "    model_input['past_info_indices'] = tf.constant(generate_past_info_indices_optimized(person_markers, selection_start_abs, selection_end_abs, input_start_idx, config)[np.newaxis, ...], dtype=tf.int64)\n",
        "    model_input['reference_baseline'] = tf.constant(yesterday_baseline[np.newaxis, ...], dtype=tf.float32)\n",
        "    model_input['profile_vector'] = tf.constant(person_profile_vector[np.newaxis, ...], dtype=tf.float32)\n",
        "    model.return_trajectory = True\n",
        "    y_pred_delta, corrected_traj, baseline_traj = model(model_input, training=False)\n",
        "    model.return_trajectory = False\n",
        "    predicted_baseline = model_input['reference_baseline'] + tf.cast(y_pred_delta, model_input['reference_baseline'].dtype)\n",
        "    print(\"--- 예측 완료 ---\")\n",
        "    return predicted_baseline[0].numpy(), corrected_traj[0].numpy(), baseline_traj[0].numpy()\n",
        "\n",
        "def plot_baseline_prediction(actual_baseline, predicted_baseline, output_dir):\n",
        "    metrics, time_axis = ['Heart Rate', 'HRV', 'Respiration Rate', 'Skin Temperature'], np.arange(30, 90)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10)); axes = axes.flatten()\n",
        "    for i, (ax, metric) in enumerate(zip(axes, metrics)):\n",
        "        if actual_baseline is not None: ax.plot(time_axis, actual_baseline[:, i], 'b-', label='Actual', linewidth=2)\n",
        "        ax.plot(time_axis, predicted_baseline[:, i], 'r--', label='Predicted', linewidth=2)\n",
        "        ax.set_title(f'{metric} Baseline Prediction'); ax.set_xlabel('Minutes after wake-up'); ax.set_ylabel(metric); ax.legend(); ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(output_dir, \"baseline_prediction_comparison.png\")); plt.show(); plt.close()\n",
        "    if actual_baseline is not None:\n",
        "        mae = np.mean(np.abs(actual_baseline - predicted_baseline), axis=0); rmse = np.sqrt(np.mean((actual_baseline - predicted_baseline)**2, axis=0))\n",
        "        print(\"\\n=== 예측 성능 ===\"); [print(f\"{m}: MAE={mae[i]:.2f}, RMSE={rmse[i]:.2f}\") for i, m in enumerate(metrics)]\n",
        "\n",
        "def plot_learning_curve(history, output_dir):\n",
        "    plt.figure(figsize=(10, 6)); plt.plot(history['train_loss'], label='Training Loss'); plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Learning Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss (MSE)'); plt.legend(); plt.grid(True)\n",
        "    save_path = os.path.join(output_dir, \"learning_curve.png\")\n",
        "    plt.savefig(save_path); plt.close(); print(f\"학습 곡선이 {save_path}에 저장되었습니다.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 8. 메인 실행 함수\n",
        "# =============================================================================\n",
        "def run_training_pipeline(config):\n",
        "    print(\"--- 기준선 예측 AI 학습 파이프라인 시작 (Mamba 적용) ---\")\n",
        "    df_all, person_ids = load_all_biometric_data()\n",
        "    baseline_lookup = load_baseline_lookup()\n",
        "    df_all = preprocess_and_engineer_features(df_all)\n",
        "\n",
        "    person_profiles_dict = {pid: load_and_preprocess_profile(pid) for pid in person_ids}\n",
        "\n",
        "    print(\"\\n--- 개인별 데이터 처리 시작 ---\")\n",
        "    processed_data, person_markers_dict = [], {}\n",
        "    with tqdm(person_ids, desc=\"개인별 처리\") as pbar:\n",
        "        for person_id in pbar:\n",
        "            pbar.set_description(f\"개인별 처리 (Person {person_id})\")\n",
        "            person_df = df_all[df_all['person_id'] == person_id].copy().sort_values('timestamp').reset_index(drop=True)\n",
        "            person_df.set_index('timestamp', inplace=True)\n",
        "            person_df['integrated_lux'] = person_df['lux'].rolling(window=30, min_periods=1).mean()\n",
        "            baseline_trajectory = generate_daily_baseline_trajectories_for_person(person_df.reset_index(), PARAMS, OUTPUT_DIR, pbar=pbar)\n",
        "            person_df[['x_base', 'xc_base', 'n_base']] = baseline_trajectory; person_df['x'] = 0.0; person_df['xc'] = 0.0\n",
        "            for col in person_df.columns:\n",
        "                if person_df[col].dtype == 'object': person_df[col] = pd.to_numeric(person_df[col], errors='coerce').fillna(0)\n",
        "                if pd.api.types.is_integer_dtype(person_df[col]): person_df[col] = person_df[col].astype(float)\n",
        "            person_df['person_id'] = person_id\n",
        "            processed_data.append(person_df.reset_index())\n",
        "            person_markers_dict[person_id] = precompute_all_markers_for_person(person_df.reset_index())\n",
        "            pbar.set_postfix_str(\"\")\n",
        "    df_all_processed = pd.concat(processed_data, ignore_index=True)\n",
        "\n",
        "    samples = prepare_baseline_samples(df_all_processed, baseline_lookup, person_ids, config)\n",
        "    np.random.shuffle(samples)\n",
        "    train_end, val_end = int(len(samples) * TRAIN_RATIO), int(len(samples) * (TRAIN_RATIO + VALIDATION_RATIO))\n",
        "    train_samples, val_samples = samples[:train_end], samples[train_end:val_end]\n",
        "    print(f\"\\n학습 샘플: {len(train_samples)}, 검증 샘플: {len(val_samples)}\")\n",
        "\n",
        "    print(\"\\n--- 데이터 스케일링 ---\")\n",
        "    feature_cols = [c for c in df_all_processed.columns if c not in ['x', 'xc', 'person_id', 'timestamp']]\n",
        "    feature_scaler = StandardScaler()\n",
        "    train_person_ids = list(set([s['person_id'] for s in train_samples]))\n",
        "    feature_scaler.fit(df_all_processed[df_all_processed['person_id'].isin(train_person_ids)][feature_cols])\n",
        "    df_all_scaled = df_all_processed.copy()\n",
        "    df_all_scaled[feature_cols] = feature_scaler.transform(df_all_processed[feature_cols])\n",
        "    joblib.dump(feature_scaler, os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "\n",
        "    train_tf_path, val_tf_path = os.path.join(OUTPUT_DIR, \"train.tfrecord\"), os.path.join(OUTPUT_DIR, \"val.tfrecord\")\n",
        "    create_baseline_tfrecords(df_all_processed, df_all_scaled, baseline_lookup, train_samples, config, train_tf_path, person_markers_dict, person_profiles_dict)\n",
        "    create_baseline_tfrecords(df_all_processed, df_all_scaled, baseline_lookup, val_samples, config, val_tf_path, person_markers_dict, person_profiles_dict)\n",
        "\n",
        "    model, _, history = train_and_evaluate_baseline(train_tf_path, val_tf_path, len(train_samples), len(val_samples), config)\n",
        "    plot_learning_curve(history, OUTPUT_DIR)\n",
        "    print(\"\\n\\n\" + \"=\"*70 + \"\\n    기준선 예측 AI 학습 파이프라인 완료 (v0.23.0 Mamba)\\n\" + \"=\"*70)\n",
        "    return model, df_all_processed, person_markers_dict, baseline_lookup\n",
        "\n",
        "def run_prediction_demo(config):\n",
        "    print(\"\\n--- 기준선 예측 평가 시작 ---\")\n",
        "    model_weights_path = os.path.join(OUTPUT_DIR, \"best_baseline_model.weights.h5\")\n",
        "    if not os.path.exists(model_weights_path):\n",
        "        print(f\"오류: 모델 파일({model_weights_path})을 찾을 수 없습니다. 먼저 'train' 모드를 실행해주세요.\"); return\n",
        "    df_all, person_ids = load_all_biometric_data()\n",
        "    baseline_lookup = load_baseline_lookup()\n",
        "    df_all = preprocess_and_engineer_features(df_all)\n",
        "\n",
        "    lco_feat, other_feat = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "    fourier_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "    predictor = MainPredictor(config['num_layers'], config['d_model'], config['num_heads'], config['dff'], config['baseline_window'], config['num_metrics'], config['rate'])\n",
        "    profile_embed_model = build_profile_embedding_model(profile_vector_dim=3, embedding_dim=config['profile_embedding_dim'])\n",
        "    loaded_model = IntegratedModel(lco_feat, other_feat, fourier_model, predictor, profile_embed_model, config)\n",
        "\n",
        "    # 더미 입력을 통한 모델 초기화\n",
        "    dummy_x = {}\n",
        "    total_mins = (config['lookback_days'] + config['input_seq_len'] // DAY_MINUTES) * DAY_MINUTES\n",
        "    keys = {'1': ['input_sleep', 'corr_input_sleep', 'input_body2', 'corr_input_body2', 'input_zeit3', 'corr_input_zeit3'],\n",
        "            '2': ['input_lux', 'corr_input_lux', 'input_caffeine', 'corr_input_caffeine', 'input_alcohol', 'corr_input_alcohol'],\n",
        "            '3': ['input_body1', 'corr_input_body1'], '6': ['input_zeit1', 'corr_input_zeit1', 'input_zeit2', 'corr_input_zeit2']}\n",
        "    for dim, key_list in keys.items():\n",
        "        for key in key_list: dummy_x[key] = tf.zeros((1, total_mins, int(dim)))\n",
        "    dummy_x['baseline_inputs'] = tf.zeros((1, total_mins, 2))\n",
        "    dummy_x['past_info_indices'] = tf.zeros((1, config['num_markers_to_keep'] * 3), dtype=tf.int64)\n",
        "    dummy_x['full_inputs_for_anchor'] = tf.zeros((1, DAY_MINUTES, 2))\n",
        "    dummy_x['time_offset'] = tf.zeros((1, 1), dtype=tf.int32)\n",
        "    dummy_x['reference_baseline'] = tf.zeros((1, BASELINE_WINDOW, NUM_METRICS))\n",
        "    dummy_x['profile_vector'] = tf.zeros((1, 3))\n",
        "    loaded_model(dummy_x, training=False)\n",
        "    loaded_model.load_weights(model_weights_path)\n",
        "    print(\"모델 로드 완료.\")\n",
        "\n",
        "    test_person_id = person_ids[-1]\n",
        "    person_data = df_all[df_all['person_id'] == test_person_id].copy().sort_values('timestamp').reset_index(drop=True)\n",
        "    person_data.set_index('timestamp', inplace=True)\n",
        "    person_data['integrated_lux'] = person_data['lux'].rolling(window=30, min_periods=1).mean()\n",
        "    baseline_trajectory = generate_daily_baseline_trajectories_for_person(person_data.reset_index(), PARAMS, OUTPUT_DIR)\n",
        "    person_data[['x_base', 'xc_base', 'n_base']] = baseline_trajectory; person_data['x'] = 0.0; person_data['xc'] = 0.0\n",
        "    for col in person_data.columns:\n",
        "        if person_data[col].dtype == 'object': person_data[col] = pd.to_numeric(person_data[col], errors='coerce').fillna(0)\n",
        "        if pd.api.types.is_integer_dtype(person_data[col]): person_data[col] = person_data[col].astype(float)\n",
        "    person_data['person_id'] = test_person_id\n",
        "    person_data = person_data.reset_index()\n",
        "    person_markers = precompute_all_markers_for_person(person_data)\n",
        "    person_profile_vector = load_and_preprocess_profile(test_person_id)\n",
        "\n",
        "    test_date, wake_up_idx = None, None\n",
        "    total_days_needed = config['lookback_days'] + config['input_seq_len'] // DAY_MINUTES\n",
        "    for date in reversed(person_data['timestamp'].dt.date.unique()[-7:]):\n",
        "        day_mask = person_data['timestamp'].dt.date == date\n",
        "        if not np.any(day_mask): continue\n",
        "        day_indices = person_data.index[day_mask]\n",
        "        wake_up_idx_in_day = find_wake_up_time(person_data.loc[day_indices])\n",
        "        if wake_up_idx_in_day is not None:\n",
        "            wake_up_idx_cand = day_indices[wake_up_idx_in_day]\n",
        "            if (test_person_id, str(date - pd.Timedelta(days=1))) in baseline_lookup:\n",
        "                slice_start_midnight = (wake_up_idx_cand - config['input_seq_len']) - ((wake_up_idx_cand - config['input_seq_len']) % DAY_MINUTES)\n",
        "                if slice_start_midnight + total_days_needed * DAY_MINUTES <= len(person_data):\n",
        "                    test_date, wake_up_idx = date, wake_up_idx_cand; break\n",
        "    if test_date is None: print(\"예측 가능한 테스트 날짜를 찾을 수 없습니다.\"); return\n",
        "\n",
        "    print(f\"\\n테스트 정보:\\n- Person ID: {test_person_id}\\n- 날짜: {test_date}\\n- 기상 시각: {person_data.loc[wake_up_idx, 'timestamp']}\")\n",
        "    yesterday_baseline = baseline_lookup[(test_person_id, str(test_date - pd.Timedelta(days=1)))]\n",
        "    actual_baseline = baseline_lookup.get((test_person_id, str(test_date)))\n",
        "    try:\n",
        "        predicted_baseline, corrected_traj, baseline_traj = predict_baseline_for_today(loaded_model, person_data, wake_up_idx, yesterday_baseline, person_markers, person_profile_vector, config)\n",
        "        plot_baseline_prediction(actual_baseline, predicted_baseline, OUTPUT_DIR)\n",
        "        plot_prediction_day_lco(baseline_traj, corrected_traj, test_date, OUTPUT_DIR)\n",
        "    except Exception as e: print(f\"예측 중 오류 발생: {e}\"); import traceback; traceback.print_exc()\n",
        "\n",
        "# =============================================================================\n",
        "# 9. 메인 실행 블록\n",
        "# =============================================================================\n",
        "if __name__ == '__main__':\n",
        "    EXECUTION_MODE = 'train'\n",
        "    config = {\n",
        "        'batch_size': BATCH_SIZE, 'input_seq_len': INPUT_SEQUENCE_LENGTH,\n",
        "        'baseline_window': BASELINE_WINDOW, 'num_metrics': NUM_METRICS,\n",
        "        'lookback_days': PHASE_CORRECTION_LOOKBACK_DAYS, 'day_minutes': DAY_MINUTES,\n",
        "        'num_markers_to_keep': NUM_MARKERS_TO_KEEP, 'lambda_reg': LAMBDA_REG,\n",
        "        'lambda_cont': LAMBDA_CONT, 'lambda_anchor': LAMBDA_ANCHOR,\n",
        "        'num_layers': NUM_LAYERS, 'd_model': D_MODEL, 'num_heads': NUM_HEADS,\n",
        "        'dff': DFF, 'rate': DROPOUT_RATE, 'epochs': EPOCHS,\n",
        "        'learning_rate': LEARNING_RATE, 'num_harmonics': NUM_FOURIER_HARMONICS,\n",
        "        'lstm_units': LSTM_UNITS, 'profile_embedding_dim': PROFILE_EMBEDDING_DIM,\n",
        "        'mamba_d_state': MAMBA_D_STATE,\n",
        "        'mamba_d_conv': MAMBA_D_CONV,\n",
        "        'mamba_expand': MAMBA_EXPAND,\n",
        "    }\n",
        "    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    mode_map = {'train': \"Training Mode: 전체 학습 파이프라인 실행\", 'predict': \"Prediction Mode: 저장된 모델로 예측 실행\"}\n",
        "    if EXECUTION_MODE in mode_map:\n",
        "        print(\"\\n\" + \"=\"*70 + f\"\\n  일주기 생체리듬 기준선 예측 AI v0.23.0 \\n  {mode_map[EXECUTION_MODE]}\\n\" + \"=\"*70 + \"\\n\")\n",
        "        try:\n",
        "            if EXECUTION_MODE == 'train':\n",
        "                run_training_pipeline(config)\n",
        "                print(\"\\n학습이 완료되었습니다. 예측 데모를 실행합니다...\")\n",
        "                run_prediction_demo(config)\n",
        "            else:\n",
        "                run_prediction_demo(config)\n",
        "        except Exception as e: print(f\"\\n오류 발생: {e}\"); import traceback; traceback.print_exc()\n",
        "    else:\n",
        "        print(f\"알 수 없는 모드입니다: {EXECUTION_MODE}. 'train' 또는 'predict' 중에서 선택해주세요.\")\n",
        "    print(\"\\n프로그램 실행이 완료되었습니다.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "0L_QCWPZae7V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgInoX9VrgB6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}