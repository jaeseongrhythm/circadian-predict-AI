{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeseongrhythm/circadian-predict-AI/blob/main/%EC%9D%BC%EC%A3%BC%EA%B8%B0_%EC%83%9D%EC%B2%B4%EB%A6%AC%EB%93%AC_%EC%98%88%EC%B8%A1_AI_v0_21_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "일주기 생체리듬 기준선 예측 AI v0.21.0 (시각화 기능 복구 버전)\n",
        "\n",
        "[v0.21.0 주요 변경사항]:\n",
        "- 예측 대상 변경: 무작위 시점 심박수 → 기상 후 30-90분 사이의 4가지 생체지표 기준선\n",
        "- 다중 사용자 데이터 지원 및 통합 학습\n",
        "- 어제 기준선 대비 변화량(delta) 예측\n",
        "\n",
        "[시각화 기능 복구]:\n",
        "- v0.19.0을 참고하여 LCO 보정 과정을 추적할 수 있는 'plot_lco_comparison' 함수를 복구 및 통합했습니다.\n",
        "- 매 에포크 종료 시, 기준 LCO 궤적과 모델에 의해 보정된 궤도를 비교하는 그래프를 생성하여 모델의 학습 상태를 직관적으로 확인할 수 있습니다.\n",
        "\n",
        "[폴더 구조]\n",
        "|project_root\n",
        "|--biometirc_data\n",
        "|  |--person_1\n",
        "|  |  |--biometric_data_person_1.csv\n",
        "|  |  |--biometric_data_person_1_baseline.csv\n",
        "|  |--...\n",
        "|--baseline_prediction_ai.py\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 0. 라이브러리 임포트 및 파이프라인 설정\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from scipy.interpolate import interp1d\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Concatenate, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, LSTM, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import mixed_precision\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import random\n",
        "import math\n",
        "import bisect\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# [OPTIMIZATION] 혼합 정밀도 정책 설정\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# --- 파이프라인 제어 설정 ---\n",
        "OUTPUT_DIR = \"baseline_prediction_output_v0.21\"\n",
        "ROOT_DATA_DIR = \"biometric_data\"\n",
        "\n",
        "# --- 데이터 및 모델 설정 ---\n",
        "INPUT_SEQUENCE_LENGTH = 4 * 24 * 60  # 4일\n",
        "BASELINE_WINDOW = 60  # 기상 후 30-90분 = 60분 윈도우\n",
        "NUM_METRICS = 4  # heart_rate, hrv, respiration_rate, skin_temp\n",
        "PHASE_CORRECTION_LOOKBACK_DAYS = 3\n",
        "DAY_MINUTES = 24 * 60\n",
        "NUM_MARKERS_TO_KEEP = 3\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "VALIDATION_RATIO = 0.1\n",
        "\n",
        "# --- 모델 하이퍼파라미터 ---\n",
        "D_MODEL = 128\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 8\n",
        "DFF = 256\n",
        "DROPOUT_RATE = 0.05\n",
        "NUM_FOURIER_HARMONICS = 5\n",
        "LSTM_UNITS = 64\n",
        "\n",
        "# --- 학습 하이퍼파라미터 ---\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0005\n",
        "LAMBDA_REG = 0.1\n",
        "LAMBDA_CONT = 10.0\n",
        "LAMBDA_ANCHOR = 5.0\n",
        "\n",
        "# --- 물리 모델 파라미터 ---\n",
        "PARAMS = {\n",
        "    'mu': 0.13, 'q': 1/3, 'k': 0.55, 'alpha0': 0.1, 'I0': 9500,\n",
        "    'p': 0.5, 'beta': 0.007, 'G': 37, 'rho': 0.032, 'tau_x': 24.2,\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# 1. 생체리듬 물리 모델 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "def find_hr_nadir(heart_rate_data, is_sleeping_data, day_minutes=1440):\n",
        "    num_days = len(heart_rate_data) // day_minutes\n",
        "    daily_nadirs = []\n",
        "    for day in range(num_days):\n",
        "        day_start, day_end = day * day_minutes, (day + 1) * day_minutes\n",
        "        day_hr, day_sleep = heart_rate_data[day_start:day_end], is_sleeping_data[day_start:day_end]\n",
        "        sleep_hr = day_hr[day_sleep == 1]\n",
        "        if len(sleep_hr) > 0:\n",
        "            original_indices = np.where(day_sleep == 1)[0]\n",
        "            daily_nadirs.append(original_indices[np.argmin(sleep_hr)])\n",
        "        else:\n",
        "            daily_nadirs.append(np.argmin(day_hr))\n",
        "    return np.mean(daily_nadirs) if daily_nadirs else day_minutes / 2\n",
        "\n",
        "def _sigmoid(x, k=2, x0=0):\n",
        "    return 1 / (1 + np.exp(-k * (x - x0)))\n",
        "\n",
        "def lco_model_ode(t, y, params, light_func, sleep_func):\n",
        "    x, xc, n = y\n",
        "    if not np.all(np.isfinite(y)): return [0,0,0]\n",
        "\n",
        "    mu, q, k, alpha0, I0, p, beta, G, rho, tau_x = params.values()\n",
        "    I, sigma = light_func(t), sleep_func(t)\n",
        "    I = max(I, 0)\n",
        "    alpha = alpha0 * ((I / I0)**p) * (I / (I + 100.0)) if I > 0 else 0\n",
        "    B_hat = G * (1 - n) * alpha\n",
        "    B = B_hat * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "\n",
        "    cbt_min_phase_angle = -170.7 * np.pi / 180.0\n",
        "    current_phase = np.arctan2(xc, x)\n",
        "    phase_diff_rad = (current_phase - cbt_min_phase_angle + np.pi) % (2 * np.pi) - np.pi\n",
        "    psi_c_x = phase_diff_rad * (tau_x / (2 * np.pi)) + (tau_x / 2)\n",
        "    weight_enter = _sigmoid(psi_c_x, k=2, x0=16.5)\n",
        "    weight_exit = 1 - _sigmoid(psi_c_x, k=2, x0=21.0)\n",
        "    wmz_weight = weight_enter * weight_exit * sigma\n",
        "    Ns_hat_normal = rho * (1/3.0 - sigma)\n",
        "    Ns_hat_wmz = rho * (1/3.0)\n",
        "    Ns_hat = Ns_hat_normal * (1 - wmz_weight) + Ns_hat_wmz * wmz_weight\n",
        "    Ns = Ns_hat * (1 - np.tanh(10 * x))\n",
        "\n",
        "    dxdt = (np.pi / 12.0) * (xc + mu * (x/3.0 + (4.0/3.0)*x**3 - (256.0/105.0)*x**7) + B + Ns)\n",
        "    tau_term_sq = (24.0 / (0.99729 * tau_x))**2\n",
        "    dxc_dt = (np.pi / 12.0) * (q * B * xc - x * (tau_term_sq + k * B))\n",
        "    dn_dt = 60.0 * (alpha * (1 - n) - beta * n)\n",
        "    return [dxdt, dxc_dt, dn_dt]\n",
        "\n",
        "def lco_model_jacobian(t, y, params, light_func, sleep_func):\n",
        "    x, xc, n = y\n",
        "    if not np.all(np.isfinite(y)): return np.zeros((3,3))\n",
        "\n",
        "    mu, q, k, alpha0, I0, p, beta, G, rho, tau_x = params.values()\n",
        "    I, sigma = light_func(t), sleep_func(t)\n",
        "    I = max(I, 0)\n",
        "    alpha = alpha0 * ((I / I0)**p) * (I / (I + 100.0)) if I > 0 else 0\n",
        "    cbt_min_phase_angle = -170.7 * np.pi / 180.0\n",
        "    current_phase = np.arctan2(xc, x)\n",
        "    phase_diff_rad = (current_phase - cbt_min_phase_angle + np.pi) % (2 * np.pi) - np.pi\n",
        "    psi_c_x = phase_diff_rad * (tau_x / (2 * np.pi)) + (tau_x / 2)\n",
        "    weight_enter = _sigmoid(psi_c_x, k=2, x0=16.5)\n",
        "    weight_exit = 1 - _sigmoid(psi_c_x, k=2, x0=21.0)\n",
        "    wmz_weight = weight_enter * weight_exit * sigma\n",
        "    Ns_hat_normal = rho * (1/3.0 - sigma)\n",
        "    Ns_hat_wmz = rho * (1/3.0)\n",
        "    Ns_hat = Ns_hat_normal * (1 - wmz_weight) + Ns_hat_wmz * wmz_weight\n",
        "    dB_dx = -0.4 * G * alpha * (1 - n) * (1 - 0.4 * xc)\n",
        "    dB_dxc = -0.4 * G * alpha * (1 - n) * (1 - 0.4 * x)\n",
        "    dB_dn = -G * alpha * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    dNs_dx = -Ns_hat * 10.0 * (1.0 / np.cosh(10 * x))**2\n",
        "    J = np.zeros((3, 3))\n",
        "    J[0, 0] = (np.pi / 12.0) * (mu * (1/3.0 + 4.0 * x**2 - (256.0*7.0/105.0) * x**6) + dB_dx + dNs_dx)\n",
        "    J[0, 1] = (np.pi / 12.0) * (1.0 + dB_dxc)\n",
        "    J[0, 2] = (np.pi / 12.0) * dB_dn\n",
        "    B = G * alpha * (1 - n) * (1 - 0.4 * x) * (1 - 0.4 * xc)\n",
        "    tau_term_sq = (24.0 / (0.99729 * tau_x))**2\n",
        "    J[1, 0] = (np.pi / 12.0) * (q * xc * dB_dx - (tau_term_sq + k * B) - k * x * dB_dx)\n",
        "    J[1, 1] = (np.pi / 12.0) * (q * B + q * xc * dB_dxc - k * x * dB_dxc)\n",
        "    J[1, 2] = (np.pi / 12.0) * (q * xc * dB_dn - k * x * dB_dn)\n",
        "    J[2, 2] = 60.0 * (-alpha - beta)\n",
        "    return J\n",
        "\n",
        "# =============================================================================\n",
        "# 2. 데이터 로딩 및 통합 함수 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "def load_all_biometric_data(root_dir=ROOT_DATA_DIR):\n",
        "    \"\"\"모든 person의 데이터를 통합하여 로드\"\"\"\n",
        "    print(f\"--- 다중 사용자 데이터 로딩 시작: {root_dir} ---\")\n",
        "    all_data = []\n",
        "    person_ids = []\n",
        "\n",
        "    if not os.path.exists(root_dir):\n",
        "        raise FileNotFoundError(f\"데이터 디렉토리를 찾을 수 없습니다: {root_dir}\")\n",
        "\n",
        "    for person_folder in sorted(os.listdir(root_dir)):\n",
        "        if person_folder.startswith(\"person_\"):\n",
        "            person_id = int(person_folder.split(\"_\")[1])\n",
        "            csv_path = os.path.join(root_dir, person_folder, f\"biometric_data_{person_folder}.csv\")\n",
        "\n",
        "            if os.path.exists(csv_path):\n",
        "                df = pd.read_csv(csv_path)\n",
        "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "                df['person_id'] = person_id\n",
        "                all_data.append(df)\n",
        "                person_ids.append(person_id)\n",
        "                print(f\"  - Person {person_id}: {len(df)} 레코드 로드\")\n",
        "\n",
        "    if not all_data:\n",
        "        raise ValueError(\"로드된 데이터가 없습니다.\")\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"--- 총 {len(person_ids)}명의 데이터 통합 완료: {len(combined_df)} 레코드 ---\")\n",
        "\n",
        "    return combined_df, sorted(person_ids)\n",
        "\n",
        "def load_baseline_lookup(root_dir=ROOT_DATA_DIR):\n",
        "    \"\"\"\n",
        "    [수정된 함수]\n",
        "    'Long' 형식의 baseline CSV를 읽어 AI가 사용하는 (person_id, date) 키의 딕셔너리로 변환합니다.\n",
        "    \"\"\"\n",
        "    print(f\"--- 기준선 데이터 로딩 시작 (수정된 방식) ---\")\n",
        "    baseline_lookup = {}\n",
        "    baseline_metric_cols = ['heart_rate', 'hrv', 'respiration_rate', 'skin_temp']\n",
        "\n",
        "    for person_folder in sorted(os.listdir(root_dir)):\n",
        "        if person_folder.startswith(\"person_\"):\n",
        "            person_id = int(person_folder.split(\"_\")[1])\n",
        "            baseline_path = os.path.join(root_dir, person_folder,\n",
        "                                       f\"biometric_data_{person_folder}_baseline.csv\")\n",
        "\n",
        "            if os.path.exists(baseline_path):\n",
        "                baseline_df = pd.read_csv(baseline_path)\n",
        "                if baseline_df.empty:\n",
        "                    continue\n",
        "\n",
        "                baseline_df['timestamp'] = pd.to_datetime(baseline_df['timestamp'])\n",
        "                baseline_df['date'] = baseline_df['timestamp'].dt.date.astype(str)\n",
        "\n",
        "                for date, group_df in baseline_df.groupby('date'):\n",
        "                    if len(group_df) == BASELINE_WINDOW:\n",
        "                        values = group_df[baseline_metric_cols].values\n",
        "                        key = (person_id, date)\n",
        "                        baseline_lookup[key] = values\n",
        "\n",
        "    print(f\"--- 총 {len(baseline_lookup)}개의 기준선 데이터 로드 완료 ---\")\n",
        "    return baseline_lookup\n",
        "\n",
        "# =============================================================================\n",
        "# 3. 딥러닝 모델 정의 (IntegratedModel 수정됨)\n",
        "# =============================================================================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class FourierTrajectoryLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    푸리에 계수와 동적 경계점(시작/끝)을 입력받아,\n",
        "    해당 경계 조건을 만족하는 일일 보정 궤적을 생성하는 레이어.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_harmonics, **kwargs):\n",
        "        super(FourierTrajectoryLayer, self).__init__(**kwargs)\n",
        "        self.num_harmonics = num_harmonics\n",
        "        self.output_dim = DAY_MINUTES\n",
        "        self.t = tf.constant(np.linspace(0.0, 2 * np.pi, self.output_dim), dtype=tf.float32)\n",
        "\n",
        "    def call(self, correction_params):\n",
        "        original_dtype = correction_params.dtype\n",
        "        correction_params_f32 = tf.cast(correction_params, tf.float32)\n",
        "\n",
        "        num_coeffs_per_traj = 1 + 2 * self.num_harmonics\n",
        "        num_total_coeffs = num_coeffs_per_traj * 2\n",
        "        coeffs = correction_params_f32[:, :num_total_coeffs]\n",
        "        endpoints = correction_params_f32[:, num_total_coeffs:]\n",
        "\n",
        "        coeffs_x = coeffs[:, :num_coeffs_per_traj]\n",
        "        coeffs_xc = coeffs[:, num_coeffs_per_traj:]\n",
        "        endpoints_x = endpoints[:, 0:2]\n",
        "        endpoints_xc = endpoints[:, 2:4]\n",
        "\n",
        "        traj_x = self._build_trajectory(coeffs_x, endpoints_x)\n",
        "        traj_xc = self._build_trajectory(coeffs_xc, endpoints_xc)\n",
        "\n",
        "        result = tf.stack([traj_x, traj_xc], axis=-1)\n",
        "        return tf.cast(result, original_dtype)\n",
        "\n",
        "    def _build_trajectory(self, coeffs, endpoints):\n",
        "        a0 = coeffs[:, 0:1]\n",
        "        a_n = coeffs[:, 1:self.num_harmonics + 1]\n",
        "        b_n = coeffs[:, self.num_harmonics + 1:]\n",
        "\n",
        "        trajectory = a0\n",
        "        for n in range(1, self.num_harmonics + 1):\n",
        "            trajectory += a_n[:, n-1:n] * tf.cos(n * self.t)\n",
        "            trajectory += b_n[:, n-1:n] * tf.sin(n * self.t)\n",
        "\n",
        "        raw_start = trajectory[:, 0:1]\n",
        "        raw_end = trajectory[:, -1:]\n",
        "        target_start = endpoints[:, 0:1]\n",
        "        target_end = endpoints[:, 1:2]\n",
        "\n",
        "        ramp = tf.linspace(0.0, 1.0, self.output_dim)\n",
        "        ramp = tf.expand_dims(ramp, 0)\n",
        "\n",
        "        linear_correction = raw_start + (raw_end - raw_start) * ramp\n",
        "        target_linear = target_start + (target_end - target_start) * ramp\n",
        "\n",
        "        adjusted_trajectory = trajectory - linear_correction + target_linear\n",
        "        return adjusted_trajectory\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(FourierTrajectoryLayer, self).get_config()\n",
        "        config.update({\"num_harmonics\": self.num_harmonics})\n",
        "        return config\n",
        "\n",
        "def build_fourier_correction_model(lookback_minutes, num_harmonics, lstm_units):\n",
        "    \"\"\"기존 코드 유지\"\"\"\n",
        "    num_coeffs_per_traj = 1 + 2 * num_harmonics\n",
        "    output_size = (num_coeffs_per_traj * 2) + 4\n",
        "\n",
        "    input_lux = Input(shape=(lookback_minutes, 1), name='corr_input_lux')\n",
        "    input_sleep = Input(shape=(lookback_minutes, 1), name='corr_input_sleep')\n",
        "    input_body1 = Input(shape=(lookback_minutes, 3), name='corr_input_body1')\n",
        "    input_body2 = Input(shape=(lookback_minutes, 1), name='corr_input_body2')\n",
        "    input_zeit1 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit1')\n",
        "    input_zeit2 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit2')\n",
        "    input_zeit3 = Input(shape=(lookback_minutes, 1), name='corr_input_zeit3')\n",
        "\n",
        "    def create_feat_extractor(inp, name):\n",
        "        x = Conv1D(16, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn1')(inp)\n",
        "        x = Conv1D(8, 30, activation='relu', padding='causal', name=f'corr_{name}_cnn2')(x)\n",
        "        return x\n",
        "\n",
        "    features = [\n",
        "        create_feat_extractor(input_lux, 'lux'), create_feat_extractor(input_sleep, 'sleep'),\n",
        "        create_feat_extractor(input_body1, 'body1'), create_feat_extractor(input_body2, 'body2'),\n",
        "        create_feat_extractor(input_zeit1, 'zeit1'), create_feat_extractor(input_zeit2, 'zeit2'),\n",
        "        create_feat_extractor(input_zeit3, 'zeit3'),\n",
        "    ]\n",
        "    combined_feature_sequence = Concatenate(axis=-1, dtype='float32')(features)\n",
        "    lstm_output = LSTM(lstm_units, return_sequences=False, name='correction_lstm')(combined_feature_sequence)\n",
        "    x = Dropout(0.2)(lstm_output)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    correction_params = Dense(output_size, activation='linear', name='correction_params', dtype='float32')(x)\n",
        "\n",
        "    model_inputs = [input_lux, input_sleep, input_body1, input_body2, input_zeit1, input_zeit2, input_zeit3]\n",
        "    model = Model(inputs=model_inputs, outputs=correction_params, name='FourierCorrectionModel')\n",
        "    return model\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    \"\"\"기존 코드 유지\"\"\"\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return inputs + tf.cast(self.pos_encoding[:, :tf.shape(inputs)[1], :], dtype=inputs.dtype)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\"position\": self.position, \"d_model\": self.d_model})\n",
        "        return config\n",
        "\n",
        "def build_main_feature_extractors(time_steps=INPUT_SEQUENCE_LENGTH, d_model=D_MODEL):\n",
        "    \"\"\"기존 코드 유지\"\"\"\n",
        "    feature_proportions = {\n",
        "        'lco': 0.25, 'lux': 0.125, 'sleep': 0.125, 'body1': 0.25,\n",
        "        'body2': 0.0625, 'zeit1': 0.0625, 'zeit2': 0.0625, 'zeit3': 0.0625,\n",
        "    }\n",
        "    cnn_block_map = { name: max(2, int(d_model * prop) // 2 * 2) for name, prop in feature_proportions.items() }\n",
        "    current_sum = sum(cnn_block_map.values())\n",
        "    if current_sum != d_model: cnn_block_map['lco'] += d_model - current_sum\n",
        "\n",
        "    def create_cnn_block(n_features, name_prefix):\n",
        "        return tf.keras.Sequential([\n",
        "            Conv1D(filters=32, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn1\"),\n",
        "            Conv1D(filters=n_features, kernel_size=5, activation='relu', padding='causal', name=f\"{name_prefix}_cnn2\")\n",
        "        ], name=f\"{name_prefix}_cnn_block\")\n",
        "\n",
        "    input_lco = Input(shape=(time_steps, 2), name='input_lco')\n",
        "    features_lco = create_cnn_block(cnn_block_map['lco'], 'lco')(input_lco)\n",
        "    lco_feature_extractor = Model(inputs=input_lco, outputs=features_lco, name='LCOFeatureExtractor')\n",
        "\n",
        "    input_lux = Input(shape=(time_steps, 1), name='input_lux')\n",
        "    input_sleep = Input(shape=(time_steps, 1), name='input_sleep')\n",
        "    input_body1 = Input(shape=(time_steps, 3), name='input_body1')\n",
        "    input_body2 = Input(shape=(time_steps, 1), name='input_body2')\n",
        "    input_zeit1 = Input(shape=(time_steps, 1), name='input_zeit1')\n",
        "    input_zeit2 = Input(shape=(time_steps, 1), name='input_zeit2')\n",
        "    input_zeit3 = Input(shape=(time_steps, 1), name='input_zeit3')\n",
        "\n",
        "    features_lux = create_cnn_block(cnn_block_map['lux'], 'lux')(input_lux)\n",
        "    features_sleep = create_cnn_block(cnn_block_map['sleep'], 'sleep')(input_sleep)\n",
        "    features_body1 = create_cnn_block(cnn_block_map['body1'], 'body1')(input_body1)\n",
        "    features_body2 = create_cnn_block(cnn_block_map['body2'], 'body2')(input_body2)\n",
        "    features_zeit1 = create_cnn_block(cnn_block_map['zeit1'], 'zeit1')(input_zeit1)\n",
        "    features_zeit2 = create_cnn_block(cnn_block_map['zeit2'], 'zeit2')(input_zeit2)\n",
        "    features_zeit3 = create_cnn_block(cnn_block_map['zeit3'], 'zeit3')(input_zeit3)\n",
        "\n",
        "    combined_other_features = Concatenate(axis=-1, name='combined_other_features', dtype='float32')([\n",
        "        features_lux, features_sleep, features_body1,\n",
        "        features_body2, features_zeit1, features_zeit2, features_zeit3\n",
        "    ])\n",
        "\n",
        "    other_inputs = [input_lux, input_sleep, input_body1, input_body2, input_zeit1, input_zeit2, input_zeit3]\n",
        "    other_feature_extractor = Model(inputs=other_inputs, outputs=combined_other_features, name='OtherFeatureExtractor')\n",
        "\n",
        "    return lco_feature_extractor, other_feature_extractor\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ContextualTransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"기존 코드 유지\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(ContextualTransformerBlock, self).__init__(**kwargs)\n",
        "        self.d_model, self.num_heads, self.dff, self.rate = d_model, num_heads, dff, rate\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1, self.layernorm2, self.layernorm3 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1, self.dropout2, self.dropout3 = Dropout(rate), Dropout(rate), Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        past_info, current_info = inputs\n",
        "        attn_output_current = self.mha1(query=current_info, key=current_info, value=current_info, training=training)\n",
        "        current_info_sa = self.layernorm1(current_info + self.dropout1(attn_output_current, training=training))\n",
        "        attn_output_cross = self.mha2(query=current_info_sa, key=past_info, value=past_info, training=training)\n",
        "        current_info_contextualized = self.layernorm2(current_info_sa + self.dropout2(attn_output_cross, training=training))\n",
        "        ffn_output = self.ffn(current_info_contextualized)\n",
        "        final_output = self.layernorm3(current_info_contextualized + self.dropout3(ffn_output, training=training))\n",
        "        return final_output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ContextualTransformerBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"기존 코드 유지\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super(SelfAttentionBlock, self).__init__(**kwargs)\n",
        "        self.d_model, self.num_heads, self.dff, self.rate = d_model, num_heads, dff, rate\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.ffn = tf.keras.Sequential([Dense(dff, activation='relu'), Dense(d_model)])\n",
        "        self.layernorm1, self.layernorm2 = LayerNormalization(epsilon=1e-6), LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1, self.dropout2 = Dropout(rate), Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=False, **kwargs):\n",
        "        attn_output = self.mha(query=x, key=x, value=x, training=training)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SelfAttentionBlock, self).get_config()\n",
        "        config.update({\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff, \"rate\": self.rate})\n",
        "        return config\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class MainPredictor(tf.keras.Model):\n",
        "    \"\"\"수정된 버전 - 기준선 예측을 위해\"\"\"\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, baseline_window, num_metrics, rate=0.1, **kwargs):\n",
        "        super(MainPredictor, self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.baseline_window = baseline_window\n",
        "        self.num_metrics = num_metrics\n",
        "        self.rate = rate\n",
        "\n",
        "        self.first_block = ContextualTransformerBlock(d_model, num_heads, dff, rate)\n",
        "        self.other_blocks = [SelfAttentionBlock(d_model, num_heads, dff, rate) for _ in range(num_layers - 1)]\n",
        "\n",
        "        self.prediction_head = tf.keras.Sequential([\n",
        "            Dense(256, activation='relu', name=\"pred_head_dense1\"),\n",
        "            Dropout(rate),\n",
        "            Dense(128, activation='relu', name=\"pred_head_dense2\"),\n",
        "            Dropout(rate),\n",
        "            Dense(baseline_window * num_metrics, name=\"final_prediction\", dtype='float32'),\n",
        "            Reshape((baseline_window, num_metrics))\n",
        "        ], name=\"BASELINE_PREDICTION_HEAD\")\n",
        "\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        encoded_features, past_info, last_lco = inputs\n",
        "        current_info_length = 2 * DAY_MINUTES\n",
        "        current_info = encoded_features[:, -current_info_length:, :]\n",
        "        x = self.first_block((past_info, current_info), training=training)\n",
        "        for block in self.other_blocks:\n",
        "            x = block(x, training=training)\n",
        "        pooled_vector = tf.reduce_mean(x, axis=1)\n",
        "        last_vector = x[:, -1, :]\n",
        "        combined_final_vector = tf.concat([pooled_vector, last_vector, last_lco], axis=-1)\n",
        "        predictions = self.prediction_head(combined_final_vector)\n",
        "        return predictions\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"num_layers\": self.num_layers,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff,\n",
        "            \"baseline_window\": self.baseline_window,\n",
        "            \"num_metrics\": self.num_metrics,\n",
        "            \"rate\": self.rate\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class IntegratedModel(Model):\n",
        "    \"\"\"\n",
        "    [수정된 버전]\n",
        "    - __init__ 생성자에서 하위 모델과 설정을 명시적으로 저장합니다.\n",
        "    - `return_trajectory` 플래그를 추가하여 LCO 궤도 시각화를 지원합니다.\n",
        "    - `call` 메소드에서 `return_trajectory` 플래그에 따라 분기하여 궤도 정보를 반환합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self, lco_feature_extractor, other_feature_extractor,\n",
        "                 fourier_correction_model, main_predictor, config, **kwargs):\n",
        "        super(IntegratedModel, self).__init__(**kwargs)\n",
        "        self.lco_feature_extractor = lco_feature_extractor\n",
        "        self.other_feature_extractor = other_feature_extractor\n",
        "        self.fourier_correction_model = fourier_correction_model\n",
        "        self.main_predictor = main_predictor\n",
        "        self.config = config\n",
        "\n",
        "        self.fourier_layer = FourierTrajectoryLayer(config['num_harmonics'])\n",
        "        self.pos_encoding_layer = PositionalEncoding(\n",
        "            position=config['input_seq_len'],\n",
        "            d_model=config['d_model']\n",
        "        )\n",
        "\n",
        "        self.lambda_reg = config['lambda_reg']\n",
        "        self.lambda_cont = config['lambda_cont']\n",
        "        self.lambda_anchor = config['lambda_anchor']\n",
        "\n",
        "        # LCO 궤적 시각화를 위한 플래그\n",
        "        self.return_trajectory = False\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        time_offset = inputs['time_offset']\n",
        "        batch_size = tf.shape(time_offset)[0]\n",
        "        num_main_seq_days = self.config['input_seq_len'] // DAY_MINUTES\n",
        "        lookback_minutes = self.config['lookback_days'] * DAY_MINUTES\n",
        "\n",
        "        # --- 1. 일별 보정 궤적(Delta) 생성 ---\n",
        "        correction_input_keys = ['corr_input_lux', 'corr_input_sleep', 'corr_input_body1',\n",
        "                               'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3']\n",
        "        correction_inputs_list = [inputs[key] for key in correction_input_keys]\n",
        "\n",
        "        padding_minutes = (self.config['lookback_days'] - 1) * DAY_MINUTES\n",
        "        padded_correction_inputs = []\n",
        "        for data_tensor in correction_inputs_list:\n",
        "            num_features = tf.shape(data_tensor)[-1]\n",
        "            paddings = tf.zeros([batch_size, padding_minutes, num_features], dtype=data_tensor.dtype)\n",
        "            padded_tensor = Concatenate(axis=1)([paddings, data_tensor])\n",
        "            padded_correction_inputs.append(padded_tensor)\n",
        "\n",
        "        framed_inputs = [tf.signal.frame(data, frame_length=lookback_minutes, frame_step=DAY_MINUTES, axis=1)\n",
        "                        for data in padded_correction_inputs]\n",
        "        transposed_framed_inputs = [tf.transpose(f, [1, 0, 2, 3]) for f in framed_inputs]\n",
        "\n",
        "        def parallel_correction_fn(elems):\n",
        "            correction_params = self.fourier_correction_model(list(elems), training=training)\n",
        "            return self.fourier_layer(correction_params)\n",
        "\n",
        "        all_daily_curves = tf.map_fn(\n",
        "            fn=parallel_correction_fn,\n",
        "            elems=transposed_framed_inputs,\n",
        "            fn_output_signature=tf.TensorSpec(shape=[None, DAY_MINUTES, 2], dtype=self.compute_dtype)\n",
        "        )\n",
        "        all_daily_curves_swapped = tf.transpose(all_daily_curves, [1, 0, 2, 3])\n",
        "        full_correction_block = tf.reshape(all_daily_curves_swapped, [batch_size, -1, 2])\n",
        "\n",
        "        # --- [시각화용] 전체 보정 궤적 계산 ---\n",
        "        full_baseline_trajectory = inputs['baseline_inputs']\n",
        "        full_correction_block_casted = tf.cast(full_correction_block, full_baseline_trajectory.dtype)\n",
        "        full_corrected_trajectory = full_baseline_trajectory + full_correction_block_casted\n",
        "\n",
        "        # --- 2. 주 예측 모델을 위한 입력 생성 ---\n",
        "        input_seq_len = self.config['input_seq_len']\n",
        "        offsets = tf.cast(time_offset, dtype=tf.int32)\n",
        "        sequence_indices = tf.range(input_seq_len, dtype=tf.int32)[tf.newaxis, :]\n",
        "        time_indices = offsets + sequence_indices\n",
        "\n",
        "        final_delta_trajectory = tf.gather(full_correction_block, time_indices, batch_dims=1)\n",
        "        aligned_baseline = tf.gather(inputs['baseline_inputs'], time_indices, batch_dims=1)\n",
        "\n",
        "        other_input_keys = ['input_lux', 'input_sleep', 'input_body1', 'input_body2',\n",
        "                           'input_zeit1', 'input_zeit2', 'input_zeit3']\n",
        "        main_cnn_inputs_other_list = [tf.gather(inputs[key], time_indices, batch_dims=1)\n",
        "                                     for key in other_input_keys]\n",
        "\n",
        "        corrected_lco_trajectory = tf.cast(aligned_baseline + final_delta_trajectory, self.compute_dtype)\n",
        "\n",
        "        # --- 3. 주 예측 모델 실행 ---\n",
        "        lco_features = self.lco_feature_extractor(corrected_lco_trajectory, training=training)\n",
        "        other_features = self.other_feature_extractor(main_cnn_inputs_other_list, training=training)\n",
        "        combined_features = Concatenate(axis=-1, dtype='float32')([lco_features, other_features])\n",
        "        combined_features = tf.cast(combined_features, self.compute_dtype)\n",
        "        encoded_features = self.pos_encoding_layer(combined_features)\n",
        "\n",
        "        past_info_indices = inputs['past_info_indices']\n",
        "        batch_indices = tf.tile(tf.range(batch_size, dtype=tf.int64)[:, tf.newaxis],\n",
        "                               [1, self.config['num_markers_to_keep'] * 3])\n",
        "        gather_indices = tf.stack([tf.reshape(batch_indices, [-1]),\n",
        "                                  tf.reshape(tf.maximum(past_info_indices, 0), [-1])], axis=1)\n",
        "        past_info_flat = tf.gather_nd(encoded_features, gather_indices)\n",
        "        past_info = tf.reshape(past_info_flat, [batch_size, self.config['num_markers_to_keep'] * 3,\n",
        "                                               self.config['d_model']])\n",
        "        past_info *= tf.cast(tf.not_equal(past_info_indices, -1), self.compute_dtype)[:, :, tf.newaxis]\n",
        "\n",
        "        last_lco_corrected = corrected_lco_trajectory[:, -1, :]\n",
        "\n",
        "        y_pred_delta = self.main_predictor((encoded_features, past_info, last_lco_corrected), training=training)\n",
        "\n",
        "        # --- [수정] 시각화를 위한 궤도 반환 로직 ---\n",
        "        if self.return_trajectory:\n",
        "            # 예측값, 보정된 전체 궤도, 기준 전체 궤도를 반환\n",
        "            return y_pred_delta, full_corrected_trajectory, full_baseline_trajectory\n",
        "\n",
        "        # --- 4. 학습 시 손실 계산 ---\n",
        "        if training:\n",
        "            all_daily_curves_f32 = tf.cast(all_daily_curves, tf.float32)\n",
        "            reg_loss = tf.reduce_mean(tf.square(all_daily_curves_f32)) * self.lambda_reg\n",
        "            self.add_loss(reg_loss)\n",
        "\n",
        "            num_total_days = self.config['lookback_days'] + num_main_seq_days\n",
        "            full_corrected_reshaped = tf.reshape(full_corrected_trajectory,\n",
        "                                               [batch_size, num_total_days, DAY_MINUTES, 2])\n",
        "            full_corrected_reshaped_f32 = tf.cast(full_corrected_reshaped, tf.float32)\n",
        "            continuity_gaps = full_corrected_reshaped_f32[:, :-1, -1, :] - full_corrected_reshaped_f32[:, 1:, 0, :]\n",
        "            cont_loss = tf.reduce_mean(tf.square(continuity_gaps)) * self.lambda_cont\n",
        "            self.add_loss(cont_loss)\n",
        "\n",
        "            corrected_lco_trajectory_f32 = tf.cast(corrected_lco_trajectory, tf.float32)\n",
        "            last_day_corrected_x = tf.reshape(corrected_lco_trajectory_f32,\n",
        "                                             [batch_size, num_main_seq_days, DAY_MINUTES, 2])[:, -1, :, 0]\n",
        "            predicted_cbt_nadir_idx = tf.cast(tf.argmin(last_day_corrected_x, axis=1), tf.float32)\n",
        "\n",
        "            full_inputs_for_anchor = tf.cast(inputs['full_inputs_for_anchor'], tf.float32)\n",
        "            hr_series, sleep_series = full_inputs_for_anchor[:, :, 0], full_inputs_for_anchor[:, :, 1]\n",
        "            sleep_mask = tf.cast(sleep_series > 0.5, tf.float32)\n",
        "            has_sleep = tf.reduce_any(tf.cast(sleep_mask, tf.bool), axis=1)\n",
        "            hr_in_sleep = hr_series * sleep_mask + (1.0 - sleep_mask) * 1e9\n",
        "            nadir_in_sleep_indices = tf.cast(tf.argmin(hr_in_sleep, axis=1), tf.float32)\n",
        "            nadir_overall_indices = tf.cast(tf.argmin(hr_series, axis=1), tf.float32)\n",
        "            actual_hr_nadir_idx = tf.where(has_sleep, nadir_in_sleep_indices, nadir_overall_indices)\n",
        "            actual_cbt_nadir_idx = (actual_hr_nadir_idx + 120) % DAY_MINUTES\n",
        "            anchor_loss_val = tf.reduce_mean(tf.square(predicted_cbt_nadir_idx - actual_cbt_nadir_idx)) / (DAY_MINUTES**2)\n",
        "            anchor_loss = anchor_loss_val * self.lambda_anchor\n",
        "            self.add_loss(anchor_loss)\n",
        "\n",
        "        if not training and 'reference_baseline' in inputs:\n",
        "            reference_baseline = inputs['reference_baseline']\n",
        "            y_pred_delta_casted = tf.cast(y_pred_delta, reference_baseline.dtype)\n",
        "            y_pred_final = reference_baseline + y_pred_delta_casted\n",
        "            return y_pred_final\n",
        "        else:\n",
        "            return y_pred_delta\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"config\": self.config}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config_data, custom_objects=None):\n",
        "        return cls(**config_data)\n",
        "\n",
        "# =============================================================================\n",
        "# 4. 기준선 예측을 위한 TFRecord 생성 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "def find_wake_up_time(day_data):\n",
        "    \"\"\"하루 데이터에서 기상 시점 찾기\"\"\"\n",
        "    is_sleeping = day_data['is_sleeping'].values\n",
        "    for i in range(1, len(is_sleeping)):\n",
        "        if is_sleeping[i-1] == 1 and is_sleeping[i] == 0:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def serialize_example(inputs_dict):\n",
        "    feature = {key: _bytes_feature(tf.io.serialize_tensor(value).numpy())\n",
        "              for key, value in inputs_dict.items()}\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "\n",
        "def moving_average_np(a, n=3):\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "def precompute_all_markers_for_person(person_df):\n",
        "    \"\"\"개인별 마커 계산\"\"\"\n",
        "    calculation_df = person_df.assign(\n",
        "        corrected_skin_temp=person_df['skin_temp'] - person_df['ambient_temp']\n",
        "    )\n",
        "\n",
        "    heart_rate = calculation_df['heart_rate'].values\n",
        "    is_sleeping = calculation_df['is_sleeping'].values\n",
        "    temp_series = calculation_df['corrected_skin_temp'].values\n",
        "\n",
        "    hr_in_sleep = np.where(is_sleeping > 0.5, heart_rate, np.inf)\n",
        "    nadir_indices_abs = [\n",
        "        i + np.argmin(hr_in_sleep[i:i+60])\n",
        "        for i in range(0, len(calculation_df) - 60, 60)\n",
        "        if np.any(np.isfinite(hr_in_sleep[i:i+60]))\n",
        "    ]\n",
        "\n",
        "    if len(temp_series) > 40:\n",
        "        temp_deriv_smoothed = moving_average_np(np.gradient(moving_average_np(temp_series, 30)), 10)\n",
        "        offset_to_align = len(temp_series) - len(temp_deriv_smoothed)\n",
        "        onset_indices_abs = list(np.where(temp_deriv_smoothed > 0.001)[0] + offset_to_align)\n",
        "        offset_indices_abs = list(np.where(temp_deriv_smoothed < -0.001)[0] + offset_to_align)\n",
        "    else:\n",
        "        onset_indices_abs, offset_indices_abs = [], []\n",
        "\n",
        "    return {\n",
        "        \"onset\": sorted(list(set(onset_indices_abs))),\n",
        "        \"nadir\": sorted(list(set(nadir_indices_abs))),\n",
        "        \"offset\": sorted(list(set(offset_indices_abs)))\n",
        "    }\n",
        "\n",
        "def generate_past_info_indices_optimized(precomputed_markers, selection_start_abs, selection_end_abs,\n",
        "                                        main_seq_start_in_full_df, config):\n",
        "    \"\"\"기존 함수 유지\"\"\"\n",
        "    num_markers_to_keep = config['num_markers_to_keep']\n",
        "    padding_value = -1\n",
        "    final_indices_relative = []\n",
        "\n",
        "    for marker_type in [\"onset\", \"nadir\", \"offset\"]:\n",
        "        marker_candidates_abs = precomputed_markers[marker_type]\n",
        "        start_idx = bisect.bisect_left(marker_candidates_abs, selection_start_abs)\n",
        "        end_idx = bisect.bisect_right(marker_candidates_abs, selection_end_abs)\n",
        "        valid_candidates = marker_candidates_abs[start_idx:end_idx]\n",
        "\n",
        "        if len(valid_candidates) > 0:\n",
        "            top_indices_abs = valid_candidates[-num_markers_to_keep:]\n",
        "            top_indices_relative = [idx - main_seq_start_in_full_df for idx in top_indices_abs]\n",
        "            final_indices_relative.extend(reversed(top_indices_relative))\n",
        "            if len(top_indices_relative) < num_markers_to_keep:\n",
        "                final_indices_relative.extend([padding_value] * (num_markers_to_keep - len(top_indices_relative)))\n",
        "        else:\n",
        "            final_indices_relative.extend([padding_value] * num_markers_to_keep)\n",
        "\n",
        "    return np.array(final_indices_relative, dtype=np.int64)\n",
        "\n",
        "def create_baseline_tfrecords(df_all, df_all_scaled, baseline_lookup, indices, config,\n",
        "                             file_path, person_markers_dict):\n",
        "    \"\"\"기준선 예측을 위한 TFRecord 생성\"\"\"\n",
        "    print(f\"--- 기준선 TFRecord 파일 생성 시작: {file_path} ---\")\n",
        "\n",
        "    main_cnn_other_cols = {\n",
        "        'input_lux': ['lux'], 'input_sleep': ['is_sleeping'],\n",
        "        'input_body1': ['heart_rate', 'hrv', 'respiration_rate'],\n",
        "        'input_body2': ['skin_temp'], 'input_zeit1': ['meal_event'],\n",
        "        'input_zeit2': ['exercise_event'], 'input_zeit3': ['ambient_temp'],\n",
        "    }\n",
        "    fourier_corr_cols = {\n",
        "        'corr_input_lux': ['lux'], 'corr_input_sleep': ['is_sleeping'],\n",
        "        'corr_input_body1': ['heart_rate', 'hrv', 'respiration_rate'],\n",
        "        'corr_input_body2': ['skin_temp'], 'corr_input_zeit1': ['meal_event'],\n",
        "        'corr_input_zeit2': ['exercise_event'], 'corr_input_zeit3': ['ambient_temp'],\n",
        "    }\n",
        "    anchor_loss_cols = ['heart_rate', 'is_sleeping']\n",
        "\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "\n",
        "    df_unscaled_np = df_all.to_numpy()\n",
        "    df_scaled_np = df_all_scaled.to_numpy()\n",
        "    unscaled_cols_map = {name: i for i, name in enumerate(df_all.columns)}\n",
        "    scaled_cols_map = {name: i for i, name in enumerate(df_all_scaled.columns)}\n",
        "\n",
        "    def get_indices(cols, col_map):\n",
        "        return [col_map[c] for c in cols]\n",
        "\n",
        "    fourier_corr_indices = {k: get_indices(v, scaled_cols_map) for k, v in fourier_corr_cols.items()}\n",
        "    main_cnn_other_indices = {k: get_indices(v, scaled_cols_map) for k, v in main_cnn_other_cols.items()}\n",
        "    anchor_loss_indices = get_indices(anchor_loss_cols, unscaled_cols_map)\n",
        "    baseline_indices = get_indices(['x_base', 'xc_base'], scaled_cols_map)\n",
        "\n",
        "    with tf.io.TFRecordWriter(file_path) as writer:\n",
        "        for sample_info in tqdm(indices, desc=f\"{os.path.basename(file_path)} 생성 중\"):\n",
        "            person_id = sample_info['person_id']\n",
        "            wake_up_idx = sample_info['wake_up_idx']\n",
        "            y_delta = sample_info['y_delta']\n",
        "            y_yesterday = sample_info['y_yesterday']\n",
        "\n",
        "            main_seq_end_idx = wake_up_idx\n",
        "            main_seq_start_idx = wake_up_idx - config['input_seq_len']\n",
        "\n",
        "            if main_seq_start_idx < 0: continue\n",
        "\n",
        "            time_offset = main_seq_start_idx % DAY_MINUTES\n",
        "            slice_start_midnight = main_seq_start_idx - time_offset\n",
        "            total_days_needed = config['lookback_days'] + num_main_seq_days\n",
        "            slice_end_midnight = slice_start_midnight + total_days_needed * DAY_MINUTES\n",
        "\n",
        "            if slice_end_midnight > len(df_unscaled_np): continue\n",
        "\n",
        "            example_dict = {}\n",
        "\n",
        "            for key, col_idxs in fourier_corr_indices.items():\n",
        "                example_dict[key] = tf.constant(df_scaled_np[slice_start_midnight:slice_end_midnight, col_idxs], dtype=tf.float32)\n",
        "            for key, col_idxs in main_cnn_other_indices.items():\n",
        "                example_dict[key] = tf.constant(df_scaled_np[slice_start_midnight:slice_end_midnight, col_idxs], dtype=tf.float32)\n",
        "\n",
        "            example_dict['baseline_inputs'] = tf.constant(df_scaled_np[slice_start_midnight:slice_end_midnight, baseline_indices], dtype=tf.float32)\n",
        "            example_dict['time_offset'] = tf.constant([time_offset], dtype=tf.int32)\n",
        "\n",
        "            anchor_start = main_seq_start_idx + config['input_seq_len'] - DAY_MINUTES\n",
        "            anchor_end = main_seq_start_idx + config['input_seq_len']\n",
        "            example_dict['full_inputs_for_anchor'] = tf.constant(df_unscaled_np[anchor_start:anchor_end, anchor_loss_indices], dtype=tf.float32)\n",
        "\n",
        "            selection_start_abs = main_seq_start_idx\n",
        "            selection_end_abs = main_seq_start_idx + (config['input_seq_len'] - 2 * DAY_MINUTES)\n",
        "            example_dict['past_info_indices'] = generate_past_info_indices_optimized(\n",
        "                person_markers_dict[person_id], selection_start_abs, selection_end_abs,\n",
        "                main_seq_start_idx, config\n",
        "            )\n",
        "\n",
        "            example_dict['reference_baseline'] = tf.constant(y_yesterday, dtype=tf.float32)\n",
        "            example_dict['label'] = tf.constant(y_delta, dtype=tf.float32)\n",
        "            writer.write(serialize_example(example_dict))\n",
        "\n",
        "    print(f\"--- 기준선 TFRecord 파일 생성 완료: {file_path} ---\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5. 데이터 전처리 및 학습 파이프라인 (train_and_evaluate_baseline 수정됨)\n",
        "# =============================================================================\n",
        "def get_stable_limit_cycle(params, output_dir, force_recalculate=False):\n",
        "    \"\"\"기존 함수 유지\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    cache_path = os.path.join(output_dir, \"stable_limit_cycle.npy\")\n",
        "    if os.path.exists(cache_path) and not force_recalculate:\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    print(\"안정 궤도를 새로 계산합니다 (최초 1회 실행)...\")\n",
        "    standard_sleep, standard_light = np.zeros(DAY_MINUTES), np.zeros(DAY_MINUTES)\n",
        "    standard_sleep[0:8*60] = 1\n",
        "    standard_light[8*60:10*60] = 150; standard_light[10*60:18*60] = 400\n",
        "    standard_light[18*60:22*60] = 150; standard_light[22*60:24*60] = 50\n",
        "\n",
        "    light_func_spin_up = lambda t: standard_light[int((t % 24) * 60)]\n",
        "    sleep_func_spin_up = lambda t: standard_sleep[int((t % 24) * 60)]\n",
        "\n",
        "    sol_spin_up = solve_ivp(\n",
        "        fun=lco_model_ode, t_span=[0, 10 * 24], y0=[1.0, 0.0, 0.5],\n",
        "        method='BDF', args=(params, light_func_spin_up, sleep_func_spin_up),\n",
        "        dense_output=True, rtol=1e-6, atol=1e-9\n",
        "    )\n",
        "\n",
        "    if not sol_spin_up.success:\n",
        "        raise RuntimeError(f\"안정 궤도 계산 실패: {sol_spin_up.message}\")\n",
        "\n",
        "    limit_cycle_map = sol_spin_up.sol(np.arange(9 * 24, 10 * 24, 1.0/60.0)).T\n",
        "    np.save(cache_path, limit_cycle_map)\n",
        "    print(f\"새로운 안정 궤도를 저장했습니다: {cache_path}\")\n",
        "    return limit_cycle_map\n",
        "\n",
        "def generate_daily_baseline_trajectories_for_person(person_df, params, output_dir):\n",
        "    \"\"\"개인별 기준 궤도 생성\"\"\"\n",
        "    total_minutes = len(person_df)\n",
        "    num_days = len(person_df) // DAY_MINUTES\n",
        "    limit_cycle_map = get_stable_limit_cycle(params, output_dir)\n",
        "    map_phases = np.arctan2(limit_cycle_map[:, 1], limit_cycle_map[:, 0])\n",
        "    final_trajectory = np.zeros((total_minutes, 3))\n",
        "\n",
        "    theoretical_anchor_minute = 4 * 60\n",
        "    last_known_anchor_minute = -1\n",
        "    t_eval_day_hours = np.arange(DAY_MINUTES) / 60.0\n",
        "\n",
        "    for day in tqdm(range(num_days), desc=\"  - 일일 궤도 생성 중\", leave=False):\n",
        "        day_start_idx = day * DAY_MINUTES\n",
        "        day_end_idx = (day + 1) * DAY_MINUTES\n",
        "        day_df = person_df.iloc[day_start_idx:day_end_idx]\n",
        "\n",
        "        day_hr, day_sleep = day_df['heart_rate'].values, day_df['is_sleeping'].values\n",
        "        sleep_indices = np.where(day_sleep > 0.5)[0]\n",
        "\n",
        "        current_anchor_minute = sleep_indices[np.argmin(day_hr[sleep_indices])] if len(sleep_indices) > 0 else -1\n",
        "        if current_anchor_minute != -1: last_known_anchor_minute = current_anchor_minute\n",
        "\n",
        "        anchor_to_use = last_known_anchor_minute if last_known_anchor_minute != -1 else theoretical_anchor_minute\n",
        "        cbt_nadir_minute_in_day = (anchor_to_use + 120) % DAY_MINUTES\n",
        "        initial_phase_at_midnight = (-170.7 * np.pi / 180.0) - (cbt_nadir_minute_in_day * (2 * np.pi) / DAY_MINUTES)\n",
        "        initial_phase_at_midnight = (initial_phase_at_midnight + np.pi) % (2 * np.pi) - np.pi\n",
        "\n",
        "        y0 = limit_cycle_map[np.argmin(np.abs((map_phases - initial_phase_at_midnight + np.pi) % (2 * np.pi) - np.pi))]\n",
        "\n",
        "        day_df_smoothed = pd.DataFrame(index=day_df.index)\n",
        "        day_df_smoothed['lux_smoothed'] = day_df['integrated_lux'].rolling(window=15, min_periods=1, center=True).mean()\n",
        "        day_df_smoothed['sleep_smoothed'] = day_df['is_sleeping'].rolling(window=15, min_periods=1, center=True).mean()\n",
        "\n",
        "        light_func = interp1d(t_eval_day_hours, day_df_smoothed['lux_smoothed'].values, kind='linear', fill_value=\"extrapolate\")\n",
        "        sleep_func = interp1d(t_eval_day_hours, day_df_smoothed['sleep_smoothed'].values, kind='linear', fill_value=\"extrapolate\")\n",
        "\n",
        "        sol_day = solve_ivp(\n",
        "            fun=lco_model_ode, t_span=[0, 24], y0=y0, method='BDF',\n",
        "            jac=lco_model_jacobian, args=(params, light_func, sleep_func),\n",
        "            dense_output=True, t_eval=t_eval_day_hours, rtol=1e-5, atol=1e-8\n",
        "        )\n",
        "\n",
        "        if sol_day.success and sol_day.y.shape[1] == DAY_MINUTES:\n",
        "            final_trajectory[day_start_idx:day_end_idx, :] = sol_day.y.T\n",
        "        elif day > 0:\n",
        "            tqdm.write(f\"경고: Day {day+1} 시뮬레이션 실패. 이전 날 궤도를 복사합니다.\")\n",
        "            final_trajectory[day_start_idx:day_end_idx, :] = final_trajectory[(day-1)*DAY_MINUTES:day*DAY_MINUTES, :]\n",
        "\n",
        "    if total_minutes > num_days * DAY_MINUTES and num_days > 0:\n",
        "        remaining_start = num_days * DAY_MINUTES\n",
        "        remaining_len = total_minutes - num_days * DAY_MINUTES\n",
        "        final_trajectory[remaining_start:, :] = final_trajectory[(num_days-1)*DAY_MINUTES : (num_days-1)*DAY_MINUTES+remaining_len, :]\n",
        "\n",
        "    return final_trajectory\n",
        "\n",
        "def prepare_baseline_samples(df_all, baseline_lookup, person_ids, config):\n",
        "    \"\"\"기준선 예측을 위한 샘플 준비\"\"\"\n",
        "    print(\"--- 기준선 예측 샘플 준비 시작 ---\")\n",
        "    samples = []\n",
        "\n",
        "    for person_id in tqdm(person_ids, desc=\"샘플 생성 중\"):\n",
        "        person_data = df_all[df_all['person_id'] == person_id].copy()\n",
        "        person_data = person_data.sort_values('timestamp').reset_index(drop=True)\n",
        "        dates = person_data['timestamp'].dt.date.unique()\n",
        "\n",
        "        for i, date in enumerate(dates[1:], 1):\n",
        "            today_mask = person_data['timestamp'].dt.date == date\n",
        "            today_indices = person_data.index[today_mask]\n",
        "            if len(today_indices) == 0: continue\n",
        "\n",
        "            today_data = person_data.loc[today_indices]\n",
        "            wake_up_idx_in_day = find_wake_up_time(today_data)\n",
        "            if wake_up_idx_in_day is None: continue\n",
        "\n",
        "            wake_up_idx = today_indices[wake_up_idx_in_day]\n",
        "            if (person_id, str(date)) not in baseline_lookup: continue\n",
        "            y_today = baseline_lookup[(person_id, str(date))]\n",
        "\n",
        "            y_yesterday = None\n",
        "            for days_back in range(1, 8):\n",
        "                prev_date = date - pd.Timedelta(days=days_back)\n",
        "                if (person_id, str(prev_date)) in baseline_lookup:\n",
        "                    y_yesterday = baseline_lookup[(person_id, str(prev_date))]\n",
        "                    break\n",
        "            if y_yesterday is None: continue\n",
        "\n",
        "            y_delta = y_today - y_yesterday\n",
        "            if wake_up_idx < config['input_seq_len']: continue\n",
        "\n",
        "            samples.append({\n",
        "                'person_id': person_id, 'wake_up_idx': wake_up_idx,\n",
        "                'y_delta': y_delta, 'y_yesterday': y_yesterday, 'date': date\n",
        "            })\n",
        "\n",
        "    print(f\"--- 총 {len(samples)}개의 샘플 생성 완료 ---\")\n",
        "    return samples\n",
        "\n",
        "def train_and_evaluate_baseline(train_tfrecord, val_tfrecord, num_train_samples,\n",
        "                               num_val_samples, config):\n",
        "    \"\"\"\n",
        "    [수정된 함수]\n",
        "    기준선 예측 모델 학습 및 평가. 매 에포크 종료 시 LCO 궤도 시각화 기능 추가.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- 기준선 예측 모델 학습 및 검증 시작 ---\")\n",
        "\n",
        "    main_cnn_other_keys = ['input_lux', 'input_sleep', 'input_body1', 'input_body2',\n",
        "                          'input_zeit1', 'input_zeit2', 'input_zeit3']\n",
        "    fourier_corr_keys = ['corr_input_lux', 'corr_input_sleep', 'corr_input_body1',\n",
        "                        'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3']\n",
        "    other_keys = ['baseline_inputs', 'full_inputs_for_anchor', 'past_info_indices',\n",
        "                  'label', 'time_offset', 'reference_baseline']\n",
        "    feature_spec = {key: tf.io.FixedLenFeature([], tf.string)\n",
        "                   for key in main_cnn_other_keys + fourier_corr_keys + other_keys}\n",
        "\n",
        "    def _parse_function(example_proto):\n",
        "        parsed = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "        inputs = {}\n",
        "        for key in main_cnn_other_keys: inputs[key] = tf.io.parse_tensor(parsed[key], out_type=tf.float32)\n",
        "        for key in fourier_corr_keys: inputs[key] = tf.io.parse_tensor(parsed[key], out_type=tf.float32)\n",
        "        inputs['baseline_inputs'] = tf.io.parse_tensor(parsed['baseline_inputs'], out_type=tf.float32)\n",
        "        inputs['full_inputs_for_anchor'] = tf.io.parse_tensor(parsed['full_inputs_for_anchor'], out_type=tf.float32)\n",
        "        inputs['past_info_indices'] = tf.io.parse_tensor(parsed['past_info_indices'], out_type=tf.int64)\n",
        "        inputs['time_offset'] = tf.io.parse_tensor(parsed['time_offset'], out_type=tf.int32)\n",
        "        inputs['reference_baseline'] = tf.io.parse_tensor(parsed['reference_baseline'], out_type=tf.float32)\n",
        "        label = tf.io.parse_tensor(parsed['label'], out_type=tf.float32)\n",
        "        return inputs, label\n",
        "\n",
        "    def make_dataset(file_path):\n",
        "        return tf.data.TFRecordDataset(file_path, num_parallel_reads=tf.data.AUTOTUNE)\\\n",
        "            .map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "            .shuffle(256).batch(config['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    train_dataset = make_dataset(train_tfrecord)\n",
        "    val_dataset = make_dataset(val_tfrecord)\n",
        "\n",
        "    # LCO 시각화를 위한 검증 데이터 1배치 고정\n",
        "    vis_data_x, _ = next(iter(val_dataset))\n",
        "\n",
        "    # 모델 구축\n",
        "    lco_feature_extractor, other_feature_extractor = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "    fourier_correction_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "    main_predictor = MainPredictor(\n",
        "        num_layers=config['num_layers'], d_model=config['d_model'], num_heads=config['num_heads'], dff=config['dff'],\n",
        "        baseline_window=config['baseline_window'], num_metrics=config['num_metrics'], rate=config['rate']\n",
        "    )\n",
        "    integrated_model = IntegratedModel(lco_feature_extractor, other_feature_extractor, fourier_correction_model, main_predictor, config)\n",
        "\n",
        "    optimizer = mixed_precision.LossScaleOptimizer(Adam(learning_rate=config['learning_rate']))\n",
        "    mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, y_delta):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_delta = integrated_model(x, training=True)\n",
        "            main_loss = mse_loss_fn(y_delta, y_pred_delta)\n",
        "            total_loss = main_loss + sum(integrated_model.losses)\n",
        "        gradients = tape.gradient(total_loss, integrated_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, integrated_model.trainable_variables))\n",
        "        return total_loss\n",
        "\n",
        "    @tf.function\n",
        "    def val_step(x, y_delta):\n",
        "        y_pred_delta = integrated_model(x, training=False)\n",
        "        return mse_loss_fn(y_delta, y_pred_delta)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    best_val_loss = float('inf')\n",
        "    model_save_path = os.path.join(OUTPUT_DIR, \"best_baseline_model.weights.h5\")\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "        progbar = tf.keras.utils.Progbar(num_train_samples // config['batch_size'], stateful_metrics=['train_loss'])\n",
        "        epoch_train_losses = []\n",
        "\n",
        "        for i, (x, y) in enumerate(train_dataset):\n",
        "            loss = train_step(x, y)\n",
        "            progbar.update(i + 1, values=[('train_loss', loss)])\n",
        "            epoch_train_losses.append(loss)\n",
        "\n",
        "        history['train_loss'].append(np.mean(epoch_train_losses))\n",
        "\n",
        "        val_losses = [val_step(x, y) for x, y in val_dataset]\n",
        "        val_loss = tf.reduce_mean(val_losses)\n",
        "        history['val_loss'].append(val_loss.numpy())\n",
        "        print(f\"\\nValidation Loss: {val_loss.numpy():.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            print(f\"Validation loss improved. Saving model weights to {model_save_path}\")\n",
        "            best_val_loss = val_loss\n",
        "            integrated_model.save_weights(model_save_path)\n",
        "\n",
        "        # --- [추가된 부분] 에포크 종료 시 LCO 궤적 시각화 ---\n",
        "        print(\"\\nEpoch 종료. LCO 보정 궤적 시각화 생성 중...\")\n",
        "        integrated_model.return_trajectory = True\n",
        "        _, corrected_traj, baseline_traj = integrated_model(vis_data_x, training=False)\n",
        "        integrated_model.return_trajectory = False # 플래그 리셋\n",
        "\n",
        "        # 시각화 함수 호출 (배치의 첫 번째 샘플 사용)\n",
        "        plot_lco_comparison(\n",
        "            epoch,\n",
        "            baseline_traj[0].numpy(),    # (7 * 1440, 2)\n",
        "            corrected_traj[0].numpy(),   # (7 * 1440, 2)\n",
        "            config,\n",
        "            OUTPUT_DIR\n",
        "        )\n",
        "\n",
        "    print(\"\\n--- 학습 및 검증 완료 ---\")\n",
        "    return integrated_model, val_dataset, history\n",
        "\n",
        "# =============================================================================\n",
        "# 6. 예측 및 시각화 (plot_lco_comparison 추가됨)\n",
        "# =============================================================================\n",
        "def plot_lco_comparison(epoch, baseline_traj, corrected_traj, config, output_dir):\n",
        "    \"\"\"\n",
        "    [새로 추가된 함수]\n",
        "    매 에포크 종료 시 LCO 기준 궤도와 모델이 보정한 궤도를 비교하여 시각화합니다.\n",
        "    \"\"\"\n",
        "    total_minutes = corrected_traj.shape[0]\n",
        "    total_days = total_minutes // DAY_MINUTES\n",
        "    days_per_chunk = 3 # 한 번에 3일치씩 그리기\n",
        "    num_chunks = math.ceil(total_days / days_per_chunk)\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_day = i * days_per_chunk\n",
        "        end_day = min((i + 1) * days_per_chunk, total_days)\n",
        "        if start_day >= end_day: continue\n",
        "\n",
        "        chunk_days = end_day - start_day\n",
        "        fig = plt.figure(figsize=(8 * chunk_days, 12))\n",
        "        gs = fig.add_gridspec(2, chunk_days, height_ratios=[2, 1])\n",
        "\n",
        "        # 상단: 시간 흐름에 따른 궤적 그래프\n",
        "        ax_ts = fig.add_subplot(gs[0, :])\n",
        "        start_idx, end_idx = start_day * DAY_MINUTES, end_day * DAY_MINUTES\n",
        "        time_axis = np.arange(start_idx, end_idx)\n",
        "\n",
        "        ax_ts.plot(time_axis, baseline_traj[start_idx:end_idx, 0], 'r--', alpha=0.7, label='Baseline x')\n",
        "        ax_ts.plot(time_axis, baseline_traj[start_idx:end_idx, 1], 'b--', alpha=0.7, label='Baseline xc')\n",
        "        ax_ts.plot(time_axis, corrected_traj[start_idx:end_idx, 0], 'r-', lw=2, label='Corrected x (Model Output)')\n",
        "        ax_ts.plot(time_axis, corrected_traj[start_idx:end_idx, 1], 'b-', lw=2, label='Corrected xc (Model Output)')\n",
        "\n",
        "        ax_ts.set_title(f'Epoch {epoch+1}: LCO Trajectory Comparison (Days {start_day+1}-{end_day})', fontsize=16)\n",
        "        ax_ts.set_xlabel('Time (minutes from start)'); ax_ts.set_ylabel('State Value'); ax_ts.legend(); ax_ts.grid(True)\n",
        "\n",
        "        # 하단: 일별 위상 공간 그래프\n",
        "        for d_idx in range(chunk_days):\n",
        "            ax_phase = fig.add_subplot(gs[1, d_idx])\n",
        "            day_num = start_day + d_idx\n",
        "            day_start_idx_local, day_end_idx_local = day_num * DAY_MINUTES, (day_num + 1) * DAY_MINUTES\n",
        "            x_vals, xc_vals = corrected_traj[day_start_idx_local:day_end_idx_local, 0], corrected_traj[day_start_idx_local:day_end_idx_local, 1]\n",
        "\n",
        "            ax_phase.plot(x_vals, xc_vals, color='purple', label=f'Day {day_num+1} Cycle')\n",
        "            ax_phase.scatter(x_vals[0], xc_vals[0], marker='o', color='g', s=100, zorder=5, label='Start')\n",
        "            ax_phase.scatter(x_vals[-1], xc_vals[-1], marker='X', color='r', s=100, zorder=5, label='End')\n",
        "            ax_phase.set_title(f'Day {day_num+1} Phase Cycle'); ax_phase.set_xlabel('x'); ax_phase.set_ylabel('xc')\n",
        "            ax_phase.grid(True); ax_phase.set_aspect('equal', adjustable='box'); ax_phase.legend()\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        save_path = os.path.join(output_dir, f'lco_comparison_epoch_{epoch+1}_days_{start_day+1}-{end_day}.png')\n",
        "        plt.savefig(save_path); plt.close(fig)\n",
        "    print(f\"LCO 궤도 비교 그래프가 {output_dir}에 저장되었습니다.\")\n",
        "\n",
        "def predict_baseline_for_today(model, person_data, wake_up_idx, yesterday_baseline,\n",
        "                              person_markers, config):\n",
        "    \"\"\"오늘의 기준선 예측\"\"\"\n",
        "    print(\"\\n--- 오늘의 기준선 예측 시작 ---\")\n",
        "\n",
        "    input_end_idx = wake_up_idx\n",
        "    input_start_idx = wake_up_idx - config['input_seq_len']\n",
        "    if input_start_idx < 0: raise ValueError(\"입력 시퀀스를 위한 충분한 과거 데이터가 없습니다.\")\n",
        "\n",
        "    time_offset = input_start_idx % DAY_MINUTES\n",
        "    slice_start_midnight = input_start_idx - time_offset\n",
        "    num_main_seq_days = config['input_seq_len'] // DAY_MINUTES\n",
        "    total_days_needed = config['lookback_days'] + num_main_seq_days\n",
        "    slice_end_midnight = slice_start_midnight + total_days_needed * DAY_MINUTES\n",
        "    if slice_end_midnight > len(person_data): raise ValueError(\"전체 슬라이스를 위한 충분한 데이터가 없습니다.\")\n",
        "\n",
        "    full_slice = person_data.iloc[slice_start_midnight:slice_end_midnight]\n",
        "\n",
        "    feature_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "    feature_cols = [c for c in full_slice.columns if c not in ['x', 'xc', 'person_id', 'timestamp']]\n",
        "    full_slice_scaled = full_slice.copy()\n",
        "    full_slice_scaled[feature_cols] = feature_scaler.transform(full_slice[feature_cols])\n",
        "\n",
        "    model_input = {}\n",
        "    main_cnn_other_cols = {\n",
        "        'input_lux': ['lux'], 'input_sleep': ['is_sleeping'],\n",
        "        'input_body1': ['heart_rate', 'hrv', 'respiration_rate'],\n",
        "        'input_body2': ['skin_temp'], 'input_zeit1': ['meal_event'],\n",
        "        'input_zeit2': ['exercise_event'], 'input_zeit3': ['ambient_temp'],\n",
        "    }\n",
        "    fourier_corr_cols = {\n",
        "        'corr_input_lux': ['lux'], 'corr_input_sleep': ['is_sleeping'],\n",
        "        'corr_input_body1': ['heart_rate', 'hrv', 'respiration_rate'],\n",
        "        'corr_input_body2': ['skin_temp'], 'corr_input_zeit1': ['meal_event'],\n",
        "        'corr_input_zeit2': ['exercise_event'], 'corr_input_zeit3': ['ambient_temp'],\n",
        "    }\n",
        "\n",
        "    for key, cols in main_cnn_other_cols.items():\n",
        "        model_input[key] = tf.constant(full_slice_scaled[cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    for key, cols in fourier_corr_cols.items():\n",
        "        model_input[key] = tf.constant(full_slice_scaled[cols].values[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    model_input['baseline_inputs'] = tf.constant(full_slice_scaled[['x_base', 'xc_base']].values[np.newaxis, ...], dtype=tf.float32)\n",
        "    model_input['time_offset'] = tf.constant([[time_offset]], dtype=tf.int32)\n",
        "\n",
        "    anchor_start = input_start_idx + config['input_seq_len'] - DAY_MINUTES\n",
        "    anchor_end = input_start_idx + config['input_seq_len']\n",
        "    anchor_data = person_data.iloc[anchor_start:anchor_end]\n",
        "    model_input['full_inputs_for_anchor'] = tf.constant(anchor_data[['heart_rate', 'is_sleeping']].values[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    selection_start_abs = input_start_idx\n",
        "    selection_end_abs = input_start_idx + (config['input_seq_len'] - 2 * DAY_MINUTES)\n",
        "    past_info_indices = generate_past_info_indices_optimized(person_markers, selection_start_abs, selection_end_abs, input_start_idx, config)\n",
        "    model_input['past_info_indices'] = tf.constant(past_info_indices[np.newaxis, ...], dtype=tf.int64)\n",
        "\n",
        "    model_input['reference_baseline'] = tf.constant(yesterday_baseline[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "    predicted_baseline = model.predict(model_input)\n",
        "    print(\"--- 예측 완료 ---\")\n",
        "    return predicted_baseline[0]\n",
        "\n",
        "def plot_baseline_prediction(actual_baseline, predicted_baseline, output_dir):\n",
        "    \"\"\"기준선 예측 결과 시각화\"\"\"\n",
        "    metrics = ['Heart Rate', 'HRV', 'Respiration Rate', 'Skin Temperature']\n",
        "    time_axis = np.arange(30, 90)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (ax, metric) in enumerate(zip(axes, metrics)):\n",
        "        if actual_baseline is not None:\n",
        "            ax.plot(time_axis, actual_baseline[:, i], 'b-', label='Actual', linewidth=2)\n",
        "        ax.plot(time_axis, predicted_baseline[:, i], 'r--', label='Predicted', linewidth=2)\n",
        "        ax.set_title(f'{metric} Baseline Prediction'); ax.set_xlabel('Minutes after wake-up'); ax.set_ylabel(metric)\n",
        "        ax.legend(); ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"baseline_prediction_comparison.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    if actual_baseline is not None:\n",
        "        mae_per_metric = np.mean(np.abs(actual_baseline - predicted_baseline), axis=0)\n",
        "        rmse_per_metric = np.sqrt(np.mean((actual_baseline - predicted_baseline)**2, axis=0))\n",
        "        print(\"\\n=== 예측 성능 ===\")\n",
        "        for i, metric in enumerate(metrics):\n",
        "            print(f\"{metric}: MAE={mae_per_metric[i]:.2f}, RMSE={rmse_per_metric[i]:.2f}\")\n",
        "\n",
        "def plot_learning_curve(history, output_dir):\n",
        "    \"\"\"학습 곡선 시각화\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Learning Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss (MSE)')\n",
        "    plt.legend(); plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"learning_curve.png\"))\n",
        "    plt.close()\n",
        "    print(f\"학습 곡선이 {os.path.join(output_dir, 'learning_curve.png')}에 저장되었습니다.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 7. 메인 실행 함수 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "def run_training_pipeline(config):\n",
        "    \"\"\"전체 학습 파이프라인 실행\"\"\"\n",
        "    print(\"--- 기준선 예측 AI 학습 파이프라인 시작 ---\")\n",
        "\n",
        "    # 1. 데이터 로딩\n",
        "    df_all, person_ids = load_all_biometric_data()\n",
        "    baseline_lookup = load_baseline_lookup()\n",
        "\n",
        "    # 2. 각 개인별 데이터 처리\n",
        "    print(\"\\n--- 개인별 데이터 처리 시작 ---\")\n",
        "    processed_data = []\n",
        "    person_markers_dict = {}\n",
        "\n",
        "    for person_id in tqdm(person_ids, desc=\"개인별 처리 중\"):\n",
        "        person_df = df_all[df_all['person_id'] == person_id].copy()\n",
        "        person_df = person_df.sort_values('timestamp').reset_index(drop=True)\n",
        "        person_df.set_index('timestamp', inplace=True)\n",
        "        person_df['integrated_lux'] = person_df['lux'].rolling(window=30, min_periods=1).mean()\n",
        "        baseline_trajectory = generate_daily_baseline_trajectories_for_person(person_df.reset_index(), PARAMS, OUTPUT_DIR)\n",
        "        person_df[['x_base', 'xc_base', 'n_base']] = baseline_trajectory\n",
        "        person_df['x'] = 0.0; person_df['xc'] = 0.0\n",
        "\n",
        "        for col in person_df.columns:\n",
        "            if person_df[col].dtype == 'object':\n",
        "                person_df[col] = pd.to_numeric(person_df[col], errors='coerce').fillna(0)\n",
        "            if pd.api.types.is_integer_dtype(person_df[col]):\n",
        "                person_df[col] = person_df[col].astype(float)\n",
        "\n",
        "        person_df['person_id'] = person_id\n",
        "        person_df = person_df.reset_index()\n",
        "        processed_data.append(person_df)\n",
        "        person_markers_dict[person_id] = precompute_all_markers_for_person(person_df)\n",
        "\n",
        "    df_all_processed = pd.concat(processed_data, ignore_index=True)\n",
        "\n",
        "    # 3. 샘플 준비 및 분할\n",
        "    samples = prepare_baseline_samples(df_all_processed, baseline_lookup, person_ids, config)\n",
        "    np.random.shuffle(samples)\n",
        "    train_end = int(len(samples) * TRAIN_RATIO)\n",
        "    val_end = int(len(samples) * (TRAIN_RATIO + VALIDATION_RATIO))\n",
        "    train_samples, val_samples = samples[:train_end], samples[train_end:val_end]\n",
        "    print(f\"\\n학습 샘플: {len(train_samples)}, 검증 샘플: {len(val_samples)}\")\n",
        "\n",
        "    # 5. 스케일링\n",
        "    print(\"\\n--- 데이터 스케일링 ---\")\n",
        "    feature_cols = [c for c in df_all_processed.columns if c not in ['x', 'xc', 'person_id', 'timestamp']]\n",
        "    feature_scaler = StandardScaler()\n",
        "    train_person_ids = list(set([s['person_id'] for s in train_samples]))\n",
        "    train_data_for_scaling = df_all_processed[df_all_processed['person_id'].isin(train_person_ids)]\n",
        "    feature_scaler.fit(train_data_for_scaling[feature_cols])\n",
        "    df_all_scaled = df_all_processed.copy()\n",
        "    df_all_scaled[feature_cols] = feature_scaler.transform(df_all_processed[feature_cols])\n",
        "    joblib.dump(feature_scaler, os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "\n",
        "    # 6. TFRecord 생성\n",
        "    train_tfrecord_path = os.path.join(OUTPUT_DIR, \"train_baseline.tfrecord\")\n",
        "    val_tfrecord_path = os.path.join(OUTPUT_DIR, \"val_baseline.tfrecord\")\n",
        "    create_baseline_tfrecords(df_all_processed, df_all_scaled, baseline_lookup, train_samples, config, train_tfrecord_path, person_markers_dict)\n",
        "    create_baseline_tfrecords(df_all_processed, df_all_scaled, baseline_lookup, val_samples, config, val_tfrecord_path, person_markers_dict)\n",
        "\n",
        "    # 7. 모델 학습\n",
        "    model, val_dataset, history = train_and_evaluate_baseline(\n",
        "        train_tfrecord_path, val_tfrecord_path,\n",
        "        len(train_samples), len(val_samples), config\n",
        "    )\n",
        "\n",
        "    # 8. 결과 시각화\n",
        "    plot_learning_curve(history, OUTPUT_DIR)\n",
        "    print(\"\\n\\n\" + \"=\"*70 + \"\\n    기준선 예측 AI 학습 파이프라인 완료 (v0.21)\\n\" + \"=\"*70)\n",
        "    return model, df_all_processed, person_markers_dict, baseline_lookup\n",
        "\n",
        "def run_prediction_demo(config):\n",
        "    \"\"\"예측 데모 실행\"\"\"\n",
        "    print(\"\\n--- 기준선 예측 데모 시작 ---\")\n",
        "    model_weights_path = os.path.join(OUTPUT_DIR, \"best_baseline_model.weights.h5\")\n",
        "    if not os.path.exists(model_weights_path):\n",
        "        print(f\"오류: 모델 파일({model_weights_path})을 찾을 수 없습니다. 먼저 'train' 모드를 실행해주세요.\")\n",
        "        return\n",
        "\n",
        "    df_all, person_ids = load_all_biometric_data()\n",
        "    baseline_lookup = load_baseline_lookup()\n",
        "\n",
        "    lco_feat, other_feat = build_main_feature_extractors(config['input_seq_len'], config['d_model'])\n",
        "    fourier_model = build_fourier_correction_model(config['lookback_days'] * DAY_MINUTES, config['num_harmonics'], config['lstm_units'])\n",
        "    predictor = MainPredictor(\n",
        "        num_layers=config['num_layers'], d_model=config['d_model'], num_heads=config['num_heads'], dff=config['dff'],\n",
        "        baseline_window=config['baseline_window'], num_metrics=config['num_metrics'], rate=config['rate']\n",
        "    )\n",
        "    loaded_model = IntegratedModel(lco_feat, other_feat, fourier_model, predictor, config)\n",
        "\n",
        "    dummy_batch_size = 1\n",
        "    dummy_x = {}\n",
        "    total_minutes_needed = (config['lookback_days'] + config['input_seq_len'] // DAY_MINUTES) * DAY_MINUTES\n",
        "    main_cnn_keys = ['input_lux', 'input_sleep', 'input_body2', 'input_zeit1', 'input_zeit2', 'input_zeit3']\n",
        "    corr_keys = ['corr_input_lux', 'corr_input_sleep', 'corr_input_body2', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3']\n",
        "    for key in main_cnn_keys + corr_keys: dummy_x[key] = tf.zeros((dummy_batch_size, total_minutes_needed, 1))\n",
        "    for key in ['input_body1', 'corr_input_body1']: dummy_x[key] = tf.zeros((dummy_batch_size, total_minutes_needed, 3))\n",
        "    dummy_x['baseline_inputs'] = tf.zeros((dummy_batch_size, total_minutes_needed, 2))\n",
        "    dummy_x['past_info_indices'] = tf.zeros((dummy_batch_size, config['num_markers_to_keep'] * 3), dtype=tf.int64)\n",
        "    dummy_x['full_inputs_for_anchor'] = tf.zeros((dummy_batch_size, DAY_MINUTES, 2))\n",
        "    dummy_x['time_offset'] = tf.zeros((dummy_batch_size, 1), dtype=tf.int32)\n",
        "    dummy_x['reference_baseline'] = tf.zeros((dummy_batch_size, BASELINE_WINDOW, NUM_METRICS))\n",
        "\n",
        "    loaded_model(dummy_x, training=False)\n",
        "    loaded_model.load_weights(model_weights_path)\n",
        "    print(\"모델 로드 완료.\")\n",
        "\n",
        "    test_person_id = person_ids[-1]\n",
        "    person_data = df_all[df_all['person_id'] == test_person_id].copy()\n",
        "    person_data = person_data.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "    person_data.set_index('timestamp', inplace=True)\n",
        "    person_data['integrated_lux'] = person_data['lux'].rolling(window=30, min_periods=1).mean()\n",
        "    baseline_trajectory = generate_daily_baseline_trajectories_for_person(person_data.reset_index(), PARAMS, OUTPUT_DIR)\n",
        "    person_data[['x_base', 'xc_base', 'n_base']] = baseline_trajectory\n",
        "    person_data['x'] = 0.0; person_data['xc'] = 0.0\n",
        "\n",
        "    for col in person_data.columns:\n",
        "        if person_data[col].dtype == 'object':\n",
        "            person_data[col] = pd.to_numeric(person_data[col], errors='coerce').fillna(0)\n",
        "        if pd.api.types.is_integer_dtype(person_data[col]):\n",
        "            person_data[col] = person_data[col].astype(float)\n",
        "    person_data['person_id'] = test_person_id\n",
        "    person_data = person_data.reset_index()\n",
        "\n",
        "    person_markers = precompute_all_markers_for_person(person_data)\n",
        "\n",
        "    test_date, wake_up_idx = None, None\n",
        "    dates = person_data['timestamp'].dt.date.unique()\n",
        "    for date in reversed(dates[-7:]):\n",
        "        day_mask = person_data['timestamp'].dt.date == date\n",
        "        day_indices = person_data.index[day_mask]\n",
        "        if len(day_indices) == 0: continue\n",
        "        day_data = person_data.loc[day_indices]\n",
        "        wake_up_idx_in_day = find_wake_up_time(day_data)\n",
        "        if wake_up_idx_in_day is not None:\n",
        "            wake_up_idx = day_indices[wake_up_idx_in_day]\n",
        "            yesterday = date - pd.Timedelta(days=1)\n",
        "            if (test_person_id, str(yesterday)) in baseline_lookup:\n",
        "                test_date = date\n",
        "                break\n",
        "    if test_date is None:\n",
        "        print(\"예측 가능한 테스트 날짜를 찾을 수 없습니다.\"); return\n",
        "\n",
        "    print(f\"\\n테스트 정보:\\n- Person ID: {test_person_id}\\n- 날짜: {test_date}\\n- 기상 시각: {person_data.loc[wake_up_idx, 'timestamp']}\")\n",
        "\n",
        "    yesterday = test_date - pd.Timedelta(days=1)\n",
        "    yesterday_baseline = baseline_lookup[(test_person_id, str(yesterday))]\n",
        "    actual_baseline = baseline_lookup.get((test_person_id, str(test_date)))\n",
        "\n",
        "    try:\n",
        "        predicted_baseline = predict_baseline_for_today(loaded_model, person_data, wake_up_idx, yesterday_baseline, person_markers, config)\n",
        "        plot_baseline_prediction(actual_baseline, predicted_baseline, OUTPUT_DIR)\n",
        "        print(f\"\\n예측 결과가 {OUTPUT_DIR}/baseline_prediction_comparison.png에 저장되었습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"예측 중 오류 발생: {e}\")\n",
        "        import traceback; traceback.print_exc()\n",
        "\n",
        "# =============================================================================\n",
        "# 8. 메인 실행 블록 (기존 코드 유지)\n",
        "# =============================================================================\n",
        "if __name__ == '__main__':\n",
        "    EXECUTION_MODE = 'train' # 'train' 또는 'predict'\n",
        "    config = {\n",
        "        'batch_size': BATCH_SIZE, 'input_seq_len': INPUT_SEQUENCE_LENGTH,\n",
        "        'baseline_window': BASELINE_WINDOW, 'num_metrics': NUM_METRICS,\n",
        "        'lookback_days': PHASE_CORRECTION_LOOKBACK_DAYS, 'day_minutes': DAY_MINUTES,\n",
        "        'num_markers_to_keep': NUM_MARKERS_TO_KEEP, 'lambda_reg': LAMBDA_REG,\n",
        "        'lambda_cont': LAMBDA_CONT, 'lambda_anchor': LAMBDA_ANCHOR,\n",
        "        'num_layers': NUM_LAYERS, 'd_model': D_MODEL, 'num_heads': NUM_HEADS,\n",
        "        'dff': DFF, 'rate': DROPOUT_RATE, 'epochs': EPOCHS,\n",
        "        'learning_rate': LEARNING_RATE, 'num_harmonics': NUM_FOURIER_HARMONICS,\n",
        "        'lstm_units': LSTM_UNITS,\n",
        "    }\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    if EXECUTION_MODE == 'train':\n",
        "        print(\"\\n\" + \"=\"*70 + \"\\n  일주기 생체리듬 기준선 예측 AI v0.21.0\\n  Training Mode: 전체 학습 파이프라인 실행\\n\" + \"=\"*70 + \"\\n\")\n",
        "        try:\n",
        "            model, df_all_processed, person_markers_dict, baseline_lookup = run_training_pipeline(config)\n",
        "            print(\"\\n학습이 완료되었습니다. 예측 데모를 실행합니다...\")\n",
        "            run_prediction_demo(config)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n오류 발생: {e}\"); import traceback; traceback.print_exc()\n",
        "    elif EXECUTION_MODE == 'predict':\n",
        "        print(\"\\n\" + \"=\"*70 + \"\\n  일주기 생체리듬 기준선 예측 AI v0.21.0\\n  Prediction Mode: 저장된 모델로 예측 실행\\n\" + \"=\"*70 + \"\\n\")\n",
        "        try:\n",
        "            run_prediction_demo(config)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n오류 발생: {e}\"); import traceback; traceback.print_exc()\n",
        "    else:\n",
        "        print(f\"알 수 없는 모드입니다: {EXECUTION_MODE}. 'train' 또는 'predict' 중에서 선택해주세요.\")\n",
        "\n",
        "    print(\"\\n프로그램 실행이 완료되었습니다.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "  일주기 생체리듬 기준선 예측 AI v0.21.0\n",
            "  Training Mode: 전체 학습 파이프라인 실행\n",
            "======================================================================\n",
            "\n",
            "--- 기준선 예측 AI 학습 파이프라인 시작 ---\n",
            "--- 다중 사용자 데이터 로딩 시작: biometric_data ---\n",
            "  - Person 1: 72000 레코드 로드\n",
            "  - Person 2: 72000 레코드 로드\n",
            "--- 총 2명의 데이터 통합 완료: 144000 레코드 ---\n",
            "--- 기준선 데이터 로딩 시작 (수정된 방식) ---\n",
            "--- 총 100개의 기준선 데이터 로드 완료 ---\n",
            "\n",
            "--- 개인별 데이터 처리 시작 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r개인별 처리 중:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 개인별 일일 재조정 기준 궤도 생성 시작 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r개인별 처리 중:  50%|█████     | 1/2 [00:12<00:12, 12.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 개인별 일일 재조정 기준 궤도 생성 완료 ---\n",
            "--- 개인별 일일 재조정 기준 궤도 생성 시작 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "개인별 처리 중: 100%|██████████| 2/2 [00:25<00:00, 12.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 개인별 일일 재조정 기준 궤도 생성 완료 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 기준선 예측 샘플 준비 시작 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "샘플 생성 중: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 총 92개의 샘플 생성 완료 ---\n",
            "\n",
            "학습 샘플: 73, 검증 샘플: 9\n",
            "\n",
            "--- 데이터 스케일링 ---\n",
            "--- 기준선 TFRecord 파일 생성 시작: baseline_prediction_output_v0.21/train_baseline.tfrecord ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_baseline.tfrecord 생성 중: 100%|██████████| 73/73 [00:01<00:00, 41.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 기준선 TFRecord 파일 생성 완료: baseline_prediction_output_v0.21/train_baseline.tfrecord ---\n",
            "--- 기준선 TFRecord 파일 생성 시작: baseline_prediction_output_v0.21/val_baseline.tfrecord ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val_baseline.tfrecord 생성 중: 100%|██████████| 9/9 [00:00<00:00, 45.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 기준선 TFRecord 파일 생성 완료: baseline_prediction_output_v0.21/val_baseline.tfrecord ---\n",
            "\n",
            "--- 기준선 예측 모델 학습 및 검증 시작 ---\n",
            "\n",
            "Epoch 1/2\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - train_loss: 9.8405\n",
            "\u001b[1m10/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - train_loss: 7.6633\n",
            "\n",
            "Validation Loss: 3365.2031\n",
            "Validation loss improved. Saving model weights to baseline_prediction_output_v0.21/best_baseline_model.weights.h5\n",
            "\n",
            "Epoch 2/2\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - train_loss: 8.6085\n",
            "\u001b[1m10/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - train_loss: 6.4705\n",
            "\n",
            "Validation Loss: 2951.7666\n",
            "Validation loss improved. Saving model weights to baseline_prediction_output_v0.21/best_baseline_model.weights.h5\n",
            "\n",
            "--- 학습 및 검증 완료 ---\n",
            "학습 곡선이 baseline_prediction_output_v0.21/learning_curve.png에 저장되었습니다.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "    기준선 예측 AI 학습 파이프라인 완료 (v0.21)\n",
            "======================================================================\n",
            "\n",
            "학습이 완료되었습니다. 예측 데모를 실행합니다...\n",
            "\n",
            "--- 기준선 예측 데모 시작 ---\n",
            "--- 다중 사용자 데이터 로딩 시작: biometric_data ---\n",
            "  - Person 1: 72000 레코드 로드\n",
            "  - Person 2: 72000 레코드 로드\n",
            "--- 총 2명의 데이터 통합 완료: 144000 레코드 ---\n",
            "--- 기준선 데이터 로딩 시작 (수정된 방식) ---\n",
            "--- 총 100개의 기준선 데이터 로드 완료 ---\n",
            "모델 로드 완료.\n",
            "--- 개인별 일일 재조정 기준 궤도 생성 시작 ---\n",
            "--- 개인별 일일 재조정 기준 궤도 생성 완료 ---\n",
            "\n",
            "테스트 정보:\n",
            "- Person ID: 2\n",
            "- 날짜: 2025-08-19\n",
            "- 기상 시각: 2025-08-19 07:27:00\n",
            "\n",
            "--- 오늘의 기준선 예측 시작 ---\n",
            "예측 중 오류 발생: 전체 슬라이스를 위한 충분한 데이터가 없습니다.\n",
            "\n",
            "프로그램 실행이 완료되었습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1-2087095911.py\", line 1542, in run_prediction_demo\n",
            "    predicted_baseline = predict_baseline_for_today(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1-2087095911.py\", line 1188, in predict_baseline_for_today\n",
            "    raise ValueError(\"전체 슬라이스를 위한 충분한 데이터가 없습니다.\")\n",
            "ValueError: 전체 슬라이스를 위한 충분한 데이터가 없습니다.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "HVg_7TLh9CGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5cbd35-a0e1-454a-b5fe-e35deed6afc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 추가 유틸리티 함수들\n",
        "# =============================================================================\n",
        "def analyze_prediction_performance(df_all, baseline_lookup, model, config):\n",
        "    \"\"\"전체 데이터셋에 대한 예측 성능 분석\"\"\"\n",
        "    print(\"\\n--- 전체 예측 성능 분석 시작 ---\")\n",
        "\n",
        "    # 모든 샘플에 대해 예측 수행하고 성능 메트릭 계산\n",
        "    all_errors = []\n",
        "    metric_names = ['Heart Rate', 'HRV', 'Respiration Rate', 'Skin Temperature']\n",
        "\n",
        "    samples = prepare_baseline_samples(df_all, baseline_lookup,\n",
        "                                     df_all['person_id'].unique(), config)\n",
        "\n",
        "    for sample in tqdm(samples[:100], desc=\"성능 분석 중\"):  # 처음 100개 샘플만\n",
        "        try:\n",
        "            person_id = sample['person_id']\n",
        "            wake_up_idx = sample['wake_up_idx']\n",
        "            y_actual = sample['y_delta'] + sample['y_yesterday']  # 실제 기준선\n",
        "\n",
        "            # 예측 수행\n",
        "            person_data = df_all[df_all['person_id'] == person_id]\n",
        "            person_markers = precompute_all_markers_for_person(person_data)\n",
        "\n",
        "            predicted = predict_baseline_for_today(\n",
        "                model, person_data, wake_up_idx,\n",
        "                sample['y_yesterday'], person_markers, config\n",
        "            )\n",
        "\n",
        "            # 오차 계산\n",
        "            errors = np.abs(y_actual - predicted)\n",
        "            all_errors.append(errors)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if all_errors:\n",
        "        all_errors = np.array(all_errors)\n",
        "        mean_errors = np.mean(all_errors, axis=(0, 1))  # 각 지표별 평균 오차\n",
        "\n",
        "        print(\"\\n=== 전체 성능 분석 결과 ===\")\n",
        "        for i, metric in enumerate(metric_names):\n",
        "            print(f\"{metric}: 평균 절대 오차 = {mean_errors[i]:.2f}\")\n",
        "\n",
        "    return all_errors\n",
        "\n",
        "def export_model_for_deployment(model, config, output_dir):\n",
        "    \"\"\"배포를 위한 모델 내보내기\"\"\"\n",
        "    print(\"\\n--- 모델 내보내기 시작 ---\")\n",
        "\n",
        "    # 모델 아키텍처를 JSON으로 저장\n",
        "    model_json = model.to_json()\n",
        "    with open(os.path.join(output_dir, \"model_architecture.json\"), \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "    # 설정 저장\n",
        "    import json\n",
        "    with open(os.path.join(output_dir, \"model_config.json\"), \"w\") as json_file:\n",
        "        json.dump(config, json_file, indent=4)\n",
        "\n",
        "    # 모델 요약 정보 저장\n",
        "    with open(os.path.join(output_dir, \"model_summary.txt\"), \"w\") as f:\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    print(f\"모델이 {output_dir}에 내보내졌습니다.\")\n",
        "\n",
        "def create_real_time_prediction_interface(model, config):\n",
        "    \"\"\"실시간 예측을 위한 간단한 인터페이스\"\"\"\n",
        "    class BaselinePredictorInterface:\n",
        "        def __init__(self, model, config):\n",
        "            self.model = model\n",
        "            self.config = config\n",
        "            self.feature_scaler = joblib.load(os.path.join(OUTPUT_DIR, 'feature_scaler.gz'))\n",
        "\n",
        "        def predict_for_user(self, user_data_path, wake_up_time, yesterday_baseline):\n",
        "            \"\"\"\n",
        "            사용자 데이터와 기상 시간을 입력받아 오늘의 기준선을 예측\n",
        "\n",
        "            Args:\n",
        "                user_data_path: 사용자의 생체 데이터 CSV 경로\n",
        "                wake_up_time: 기상 시간 (datetime)\n",
        "                yesterday_baseline: 어제의 기준선 (60, 4) numpy array\n",
        "\n",
        "            Returns:\n",
        "                predicted_baseline: 예측된 오늘의 기준선 (60, 4) numpy array\n",
        "            \"\"\"\n",
        "            # 데이터 로드 및 전처리\n",
        "            user_data = pd.read_csv(user_data_path)\n",
        "            user_data['timestamp'] = pd.to_datetime(user_data['timestamp'])\n",
        "\n",
        "            # 기상 시점 인덱스 찾기\n",
        "            wake_up_idx = user_data[user_data['timestamp'] >= wake_up_time].index[0]\n",
        "\n",
        "            # 예측 수행\n",
        "            predicted = predict_baseline_for_today(\n",
        "                self.model, user_data, wake_up_idx,\n",
        "                yesterday_baseline, None, self.config\n",
        "            )\n",
        "\n",
        "            return predicted\n",
        "\n",
        "    return BaselinePredictorInterface(model, config)\n",
        "\n",
        "# =============================================================================\n",
        "# 데이터 생성 도우미 함수 (테스트용)\n",
        "# =============================================================================\n",
        "def create_sample_baseline_data(person_id, num_days=30, output_dir=\"biometric_data\"):\n",
        "    \"\"\"\n",
        "    테스트를 위한 샘플 기준선 데이터 생성\n",
        "    실제로는 데이터 생성기에서 생성되어야 함\n",
        "    \"\"\"\n",
        "    print(f\"Person {person_id}의 샘플 기준선 데이터 생성 중...\")\n",
        "\n",
        "    # 날짜 범위 생성\n",
        "    start_date = datetime.now() - timedelta(days=num_days)\n",
        "    dates = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(num_days)]\n",
        "\n",
        "    # 샘플 기준선 데이터 생성\n",
        "    baseline_data = []\n",
        "    for date in dates:\n",
        "        # 4개 지표 x 60분 = 240개 값\n",
        "        # 실제로는 기상 후 30-90분의 실제 측정값이어야 함\n",
        "        hr_baseline = np.random.normal(65, 5, BASELINE_WINDOW)  # 심박수\n",
        "        hrv_baseline = np.random.normal(50, 10, BASELINE_WINDOW)  # HRV\n",
        "        rr_baseline = np.random.normal(16, 2, BASELINE_WINDOW)  # 호흡수\n",
        "        temp_baseline = np.random.normal(36.5, 0.3, BASELINE_WINDOW)  # 체온\n",
        "\n",
        "        row = [date] + hr_baseline.tolist() + hrv_baseline.tolist() + \\\n",
        "               rr_baseline.tolist() + temp_baseline.tolist()\n",
        "        baseline_data.append(row)\n",
        "\n",
        "    # 컬럼명 생성\n",
        "    columns = ['date']\n",
        "    for metric in ['hr', 'hrv', 'rr', 'temp']:\n",
        "        for minute in range(30, 90):\n",
        "            columns.append(f'{metric}_min_{minute}')\n",
        "\n",
        "    # DataFrame 생성 및 저장\n",
        "    baseline_df = pd.DataFrame(baseline_data, columns=columns)\n",
        "\n",
        "    person_dir = os.path.join(output_dir, f\"person_{person_id}\")\n",
        "    if not os.path.exists(person_dir):\n",
        "        os.makedirs(person_dir)\n",
        "\n",
        "    baseline_path = os.path.join(person_dir, f\"biometric_data_person_{person_id}_baseline.csv\")\n",
        "    baseline_df.to_csv(baseline_path, index=False)\n",
        "\n",
        "    print(f\"기준선 데이터가 {baseline_path}에 저장되었습니다.\")\n",
        "    return baseline_df\n",
        "\n",
        "# =============================================================================\n",
        "# 모델 검증 함수\n",
        "# =============================================================================\n",
        "def validate_model_architecture(model):\n",
        "    \"\"\"모델 아키텍처 검증\"\"\"\n",
        "    print(\"\\n--- 모델 아키텍처 검증 ---\")\n",
        "\n",
        "    # 레이어 수 확인\n",
        "    print(f\"총 레이어 수: {len(model.layers)}\")\n",
        "\n",
        "    # 파라미터 수 확인\n",
        "    trainable_params = sum([np.prod(w.shape) for w in model.trainable_weights])\n",
        "    non_trainable_params = sum([np.prod(w.shape) for w in model.non_trainable_weights])\n",
        "    print(f\"학습 가능한 파라미터: {trainable_params:,}\")\n",
        "    print(f\"학습 불가능한 파라미터: {non_trainable_params:,}\")\n",
        "\n",
        "    # 출력 shape 확인\n",
        "    dummy_input = {}\n",
        "    batch_size = 2\n",
        "    total_minutes = (PHASE_CORRECTION_LOOKBACK_DAYS + INPUT_SEQUENCE_LENGTH // DAY_MINUTES) * DAY_MINUTES\n",
        "\n",
        "    # 더미 입력 생성\n",
        "    for key in ['input_lux', 'input_sleep', 'input_zeit1', 'input_zeit2', 'input_zeit3',\n",
        "                'corr_input_lux', 'corr_input_sleep', 'corr_input_zeit1', 'corr_input_zeit2', 'corr_input_zeit3']:\n",
        "        dummy_input[key] = tf.zeros((batch_size, total_minutes, 1))\n",
        "\n",
        "    for key in ['input_body1', 'corr_input_body1']:\n",
        "        dummy_input[key] = tf.zeros((batch_size, total_minutes, 3))\n",
        "\n",
        "    for key in ['input_body2', 'corr_input_body2']:\n",
        "        dummy_input[key] = tf.zeros((batch_size, total_minutes, 1))\n",
        "\n",
        "    dummy_input['baseline_inputs'] = tf.zeros((batch_size, total_minutes, 2))\n",
        "    dummy_input['past_info_indices'] = tf.zeros((batch_size, NUM_MARKERS_TO_KEEP * 3), dtype=tf.int64)\n",
        "    dummy_input['full_inputs_for_anchor'] = tf.zeros((batch_size, DAY_MINUTES, 2))\n",
        "    dummy_input['time_offset'] = tf.zeros((batch_size, 1), dtype=tf.int32)\n",
        "    dummy_input['reference_baseline'] = tf.zeros((batch_size, BASELINE_WINDOW, NUM_METRICS))\n",
        "\n",
        "    # 출력 확인\n",
        "    output = model(dummy_input, training=False)\n",
        "    print(f\"모델 출력 shape: {output.shape}\")\n",
        "    print(f\"예상 출력 shape: (batch_size={batch_size}, window={BASELINE_WINDOW}, metrics={NUM_METRICS})\")\n",
        "\n",
        "    if output.shape[1:] == (BASELINE_WINDOW, NUM_METRICS):\n",
        "        print(\"✓ 모델 아키텍처 검증 통과\")\n",
        "    else:\n",
        "        print(\"✗ 모델 출력 shape가 예상과 다릅니다!\")\n",
        "\n",
        "    return True\n",
        "\n",
        "# =============================================================================\n",
        "# 추가 시각화 함수\n",
        "# =============================================================================\n",
        "def visualize_baseline_patterns(baseline_lookup, output_dir):\n",
        "    \"\"\"전체 기준선 데이터의 패턴 시각화\"\"\"\n",
        "    print(\"\\n--- 기준선 패턴 시각화 ---\")\n",
        "\n",
        "    # 모든 기준선 데이터 수집\n",
        "    all_baselines = []\n",
        "    for key, baseline in baseline_lookup.items():\n",
        "        all_baselines.append(baseline)\n",
        "\n",
        "    if not all_baselines:\n",
        "        print(\"시각화할 기준선 데이터가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    all_baselines = np.array(all_baselines)\n",
        "    mean_baseline = np.mean(all_baselines, axis=0)\n",
        "    std_baseline = np.std(all_baselines, axis=0)\n",
        "\n",
        "    # 시각화\n",
        "    metrics = ['Heart Rate', 'HRV', 'Respiration Rate', 'Skin Temperature']\n",
        "    time_axis = np.arange(30, 90)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (ax, metric) in enumerate(zip(axes, metrics)):\n",
        "        # 평균과 표준편차\n",
        "        ax.plot(time_axis, mean_baseline[i, :], 'b-', label='Mean', linewidth=2)\n",
        "        ax.fill_between(time_axis,\n",
        "                       mean_baseline[i, :] - std_baseline[i, :],\n",
        "                       mean_baseline[i, :] + std_baseline[i, :],\n",
        "                       alpha=0.3, color='blue', label='±1 STD')\n",
        "\n",
        "        # 개별 샘플 (최대 20개)\n",
        "        for j in range(min(20, len(all_baselines))):\n",
        "            ax.plot(time_axis, all_baselines[j, i, :], 'gray', alpha=0.1, linewidth=0.5)\n",
        "\n",
        "        ax.set_title(f'{metric} Baseline Patterns')\n",
        "        ax.set_xlabel('Minutes after wake-up')\n",
        "        ax.set_ylabel(metric)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"baseline_patterns_analysis.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"기준선 패턴 분석이 {output_dir}/baseline_patterns_analysis.png에 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "weHVug1FJF6d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}